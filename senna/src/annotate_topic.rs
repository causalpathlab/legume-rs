use crate::deconv::{BiSusieDeconv, SusieDeconv};
use crate::embed_common::*;
use crate::interactive_markers::{
    auto_suggest_markers, augment_membership_matrix, find_candidate_markers,
    flexible_gene_match, print_augmentation_summary, read_suggestions_json,
    run_interactive_round, save_augmented_markers, write_candidates_json, MarkerDatabase,
};
use crate::pseudobulk_topic::{compute_pseudobulk_by_topic, TopicWeightStrategy};
use matrix_util::common_io::*;

use candle_nn::AdamW;
use candle_nn::Optimizer;
use fnv::FnvHashMap as HashMap;
use fnv::FnvHashSet as HashSet;
use indicatif::{ProgressBar, ProgressDrawTarget};
use rand::SeedableRng;

/// Model type for topic annotation
#[derive(Clone, Copy, Debug, Default, ValueEnum)]
pub enum AnnotationModel {
    /// Standard SuSiE: sparse in annotation dimension only
    Susie,
    /// Bi-directional SuSiE: sparse in both annotation and topic dimensions (softmax)
    #[default]
    BiSusie,
}

/// Design matrix normalization method
#[derive(Clone, Copy, Debug, Default, ValueEnum)]
pub enum DesignNorm {
    /// No normalization
    None,
    /// Column scaling (divide by column sum or std) - default
    #[default]
    Scale,
    /// L2 normalization (unit length columns)
    L2,
}

#[derive(Args, Debug)]
pub struct AnnotateTopicArgs {
    #[arg(
        short = 'g',
        long = "gene-dictionary",
        help = "A gene dictionary matrix file (generated by `topic`)",
        long_help = "A gene dictionary matrix file (generated by `topic`)\n\
		      The first column should correspond to features (genes).",
        required = true
    )]
    dict_file: Box<str>,

    #[arg(
        short = 'z',
        long = "latent-topic",
        help = "A latent topic proportion file (generated by `topic`)",
        long_help = "A latent topic proportion file (generated by `topic`)\n\
		      Values should be probabilities (on simplex).\n\
		      The first column should correspond to cell names.",
        required = true
    )]
    latent_file: Box<str>,

    #[arg(
        short = 'm',
        long = "marker-genes",
        help = "A marker feature file",
        long_help = "A marker feature file, e.g., mapping genes to cell types. \n\
		     Each line contains two names: \n\
		     (1) gene and \n\
		     (2) cell type \n\
		     (tab-separated or comma-separated). \n\n\
		     Cell type names: spaces will be replaced with '_' (e.g., 'T cell' -> 'T_cell').\n\n\
		     Gene name matching is flexible (case-insensitive, underscore-delimited):\n\
		     - 'CD8A' matches 'ENSG00000153563_CD8A' (suffix)\n\
		     - 'CD8A' matches 'CD8A_variant1' (prefix)\n\
		     - 'CD8A' matches 'chr1_CD8A_isoform2' (segment)\n\
		     You can use simple gene symbols even if your dictionary has Ensembl IDs.",
        required = true
    )]
    marker_file: Box<str>,

    #[arg(
        short = 'd',
        long = "data",
        help = "Expression data files (.h5 or .zarr)",
        long_help = "Optional expression data files for pseudobulk aggregation.\n\
		     When provided, computes pseudobulk by topic (weighted by topic proportions)\n\
		     instead of using the gene dictionary directly.\n\
		     Only marker genes are included in the pseudobulk."
    )]
    data_files: Option<Vec<Box<str>>>,

    #[arg(
        long,
        default_value_t = true,
        help = "Use hard assignment for pseudobulk (argmax instead of soft weights)",
        long_help = "When computing pseudobulk, use hard assignment (argmax) instead of soft weights.\n\
		     With soft weights, each cell's expression is spread across all topics proportionally.\n\
		     With hard assignment, each cell is assigned to its top topic only,\n\
		     which gives cleaner topic-specific expression patterns."
    )]
    pseudobulk_hard: bool,

    #[arg(
        long,
        short = 'i',
        default_value_t = 1000,
        help = "Number of training epochs after KL annealing (total = kl-anneal-epochs + epochs)."
    )]
    epochs: usize,

    #[arg(
        short = 'c',
        long,
        default_value_t = 1e4,
        help = "Scale factor for the observation matrix Y.",
        long_help = "Scale factor applied to the observation matrix Y (gene × topic data).\n\
		     The data is transformed as: Y = exp(log_data) * scale.\n\
		     Higher values increase the magnitude of observations.\n\
		     Does not affect the membership matrix (derived from marker genes)."
    )]
    column_sum_norm: f32,

    #[arg(
        short = 'l',
        long,
        default_value_t = 0.1,
        help = "Learning rate for optimization."
    )]
    learning_rate: f64,

    #[arg(
        long,
        short = 'L',
        default_value_t = 10,
        help = "Number of SuSiE components (L)."
    )]
    susie_components: usize,

    #[arg(
        long,
        short = 's',
        default_value_t = 30,
        help = "Number of Monte Carlo samples for SGVB."
    )]
    num_samples: usize,

    #[arg(
        long,
        default_value_t = false,
        help = "Use Poisson sampling for output Y during training.",
        long_help = "When enabled, samples Y[g,t] ~ Poisson(rate) where rate is derived from\n\
		     the data matrix scaled by column_sum_norm. This adds stochasticity to training.\n\
		     When disabled, uses the scaled data directly without sampling."
    )]
    poisson_sampling: bool,

    #[arg(
        long,
        value_delimiter = ',',
        default_values_t = vec![0.1, 1.0, 10.0],
        help = "Prior variance(s) for effect sizes (comma-separated for grid search).",
        long_help = "Prior variance(s) for effect sizes.\n\
		     If multiple values are provided (comma-separated), performs grid search\n\
		     and selects the best prior variance based on ELBO.\n\
		     Example: --prior-var 0.1,1.0,10.0"
    )]
    prior_var: Vec<f32>,

    #[arg(
        long,
        default_value_t = 0,
        help = "Number of epochs for KL annealing warm-up (0 to disable).",
        long_help = "Number of epochs over which to linearly anneal KL weight from 0 to 1.\n\
		     KL annealing (warm-up) helps prevent posterior collapse by letting the model\n\
		     focus on data fitting first, then gradually enforcing the prior.\n\
		     Set to 0 to disable annealing. A common choice is epochs/2."
    )]
    kl_anneal_epochs: usize,

    #[arg(
        long,
        default_value_t = 0.01,
        help = "Minimum max-PIP threshold for including a topic.",
        long_help = "Topics with max PIP below this threshold are excluded from cell annotation.\n\
		     This filters out topics that don't map to any known cell type.\n\
		     Set to 0 to include all topics."
    )]
    min_pip: f32,

    #[arg(
        long,
        short,
        required = true,
        help = "Output header",
        long_help = "Output prefix for generated files.\n\n\
		     Output files:\n\
		     - {out}.pip.parquet: PIP matrix (annotation × topic)\n\
		     - {out}.annotation.parquet: Cell annotations (cell × annotation probabilities)\n\
		     - {out}.argmax.tsv: Argmax cell type assignment per cell\n\
		     - {out}.augmented_markers.tsv: Augmented markers (if --interactive used)\n\
		     - {out}.auto_suggestions.json: Auto-suggestions (if --marker-db with --suggest-only)"
    )]
    out: Box<str>,

    #[arg(
        long,
        short,
        help = "Verbosity.",
        long_help = "Enable verbose output.\n\
		     Prints additional information during execution."
    )]
    verbose: bool,

    #[arg(
        long,
        short = 'M',
        default_value = "bi-susie",
        help = "Model type for annotation.",
        long_help = "Model type for annotation:\n\
             - susie: Standard SuSiE, sparse in annotation dimension only\n\
             - bi-susie: Bi-directional SuSiE, sparse in both annotation and topic dimensions"
    )]
    model: AnnotationModel,

    #[arg(
        long,
        default_value = "scale",
        help = "Design matrix normalization method.",
        long_help = "Normalization applied to design matrix X (gene × annotation membership) before fitting.\n\n\
             Methods:\n\
             - scale: Column scaling (divide by column statistics) - balances annotations with different marker counts\n\
             - l2: L2 normalization (unit length columns) - standard for regression\n\
             - none: No normalization - use raw 0/1 membership values\n\n\
             Default 'scale' helps prevent cell types with many markers from dominating."
    )]
    design_norm: DesignNorm,

    #[arg(
        long,
        short = 'I',
        help = "Enable interactive marker augmentation.",
        long_help = "Enable interactive mode for marker gene augmentation.\n\
             After initial fitting, shows candidate genes for each cell type and\n\
             asks if you want to add them as markers. This iterates until\n\
             you're satisfied with the marker coverage."
    )]
    interactive: bool,

    #[arg(
        long,
        help = "Output candidate markers to JSON for LLM review.",
        long_help = "Output candidate markers to JSON for LLM-assisted annotation.\n\n\
             The JSON contains cell types with candidate genes ranked by topic weight.\n\
             Share this with an LLM and ask it to:\n\
             1. Web search each gene to find its known cell type markers\n\
             2. Confirm or reject each gene-celltype association\n\
             3. Output validated pairs as: [{\"gene\": \"X\", \"celltype\": \"Y\"}, ...]\n\n\
             Example prompt for LLM:\n\
             \"For each candidate gene, search what cell types it marks.\n\
             Return only genes that are known markers for the proposed cell type.\""
    )]
    suggest_only: Option<Box<str>>,

    #[arg(
        long,
        help = "Apply marker suggestions from JSON file.",
        long_help = "Apply LLM-validated marker suggestions from JSON.\n\n\
             Expected format: [{\"gene\": \"ENSG..._SYM\", \"celltype\": \"Cell_Type\"}, ...]\n\
             Gene name matching is flexible (same as --marker-genes):\n\
             you can use symbols like 'CD8A' or full names like 'ENSG00000153563_CD8A'.\n\
             Cell type names must match marker file annotations.\n\n\
             Workflow: --suggest-only out.json -> LLM review -> --apply-suggestions validated.json"
    )]
    apply_suggestions: Option<Box<str>>,

    #[arg(
        long,
        help = "Reference marker database for auto-suggestions (e.g., PanglaoDB TSV).",
        long_help = "Path to a reference marker gene database (TSV with gene and cell_type columns).\n\
             When provided, candidates matching known markers are auto-accepted.\n\
             Unmatched candidates are shown for manual review (if --interactive) or skipped.\n\n\
             Matching behavior:\n\
             - Genes: flexible matching (same as --marker-genes)\n\
             - Cell types: fuzzy substring matching to handle name variations"
    )]
    marker_db: Option<Box<str>>,

    #[arg(
        long,
        default_value_t = 3,
        help = "Number of candidate genes to show per topic in interactive mode."
    )]
    top_candidates: usize,

    #[arg(
        long,
        default_value_t = 2,
        help = "Number of top topics to show per cell type in interactive mode."
    )]
    topics_per_celltype: usize,

    #[arg(
        long,
        default_value_t = 0.05,
        help = "Minimum PIP to show topic-celltype match in interactive mode."
    )]
    interactive_min_pip: f32,

    #[arg(
        long,
        default_value_t = 1.0,
        help = "Weight for augmented markers (vs 1.0 for original markers)."
    )]
    augment_weight: f32,
}

/// Training data bundle
struct TrainingData {
    /// Gene × topic data matrix (log dictionary or normalized pseudobulk)
    data: Mat,
    /// Gene × annotation membership matrix
    membership: Mat,
    /// Gene names (row names of data and membership)
    gene_names: Vec<Box<str>>,
}

pub fn annotate_topics(args: &AnnotateTopicArgs) -> anyhow::Result<()> {
    if args.verbose {
        std::env::set_var("RUST_LOG", "info");
    }
    env_logger::init();

    // 1. read dictionary (log-probabilities) and latent states (probabilities)
    let MatWithNames {
        rows: row_names,
        cols: _topics,
        mat: log_dict_dk,
    } = read_mat(&args.dict_file)?;

    info!(
        "Read dictionary {} x {}",
        log_dict_dk.nrows(),
        log_dict_dk.ncols()
    );

    let MatWithNames {
        rows: cell_names,
        cols: topic_names,
        mat: topic_nt_raw,
    } = read_mat(&args.latent_file)?;

    // Convert log-probabilities to probabilities if needed
    let topic_nt = if topic_nt_raw.max() <= 0.0 {
        info!("Detected log-probabilities in latent file, converting to probabilities");
        topic_nt_raw.map(|x| x.exp())
    } else {
        topic_nt_raw
    };

    // 2. read marker gene annotation
    let AnnotInfo {
        membership_ga,
        annot_names,
    } = build_annotation_matrix(&args.marker_file, &row_names)?;

    let nnz_features = (0..membership_ga.nrows())
        .filter(|&i| membership_ga.row(i).iter().any(|&x| x > 0.0))
        .count();

    info!(
        "Found {} cell types matched over {} features",
        annot_names.len(),
        nnz_features
    );

    // 3. Prepare training data: either from pseudobulk or gene dictionary
    let (training_data, use_pseudobulk) = if let Some(data_files) = &args.data_files {
        // Compute pseudobulk by topic weighted by topic proportions
        let weight_strategy = if args.pseudobulk_hard {
            info!("Computing pseudobulk with hard assignment (argmax)...");
            TopicWeightStrategy::Hard
        } else {
            info!("Computing pseudobulk with soft weights...");
            TopicWeightStrategy::Soft
        };

        // Collect marker genes for filtering
        let marker_genes: HashSet<Box<str>> = read_marker_gene_info(&args.marker_file)?
            .keys()
            .cloned()
            .collect();

        let (pseudobulk_row_names, pseudobulk_gt) = compute_pseudobulk_by_topic(
            data_files,
            &cell_names,
            &topic_nt,
            Some(&marker_genes),
            weight_strategy,
        )?;

        // Build membership matrix on the pseudobulk rows
        let AnnotInfo {
            membership_ga: pb_membership,
            annot_names: pb_annot_names,
        } = build_annotation_matrix(&args.marker_file, &pseudobulk_row_names)?;

        // Verify annotation names match (they should since we used the same marker file)
        if pb_annot_names != annot_names {
            info!(
                "Warning: annotation names differ between dictionary and pseudobulk.\n\
                 Using pseudobulk annotations."
            );
        }

        // Normalize pseudobulk:
        // 1. log1p to stabilize expression values
        // 2. sum_to_one_rows: normalize each gene across topics (removes gene-level bias)
        // 3. sum_to_one_columns + log: convert to log-probabilities per topic
        let log_pseudobulk = pseudobulk_gt.map(|x| (1.0 + x).ln());
        let row_norm = log_pseudobulk.sum_to_one_rows();
        let normalized = row_norm.sum_to_one_columns().map(|x| x.max(1e-10).ln());

        info!(
            "Pseudobulk data: {} genes × {} topics (row+col normalized to log-prob)",
            normalized.nrows(),
            normalized.ncols()
        );

        // Store the membership matrix for later use
        (
            TrainingData {
                data: normalized,
                membership: pb_membership,
                gene_names: pseudobulk_row_names,
            },
            true,
        )
    } else {
        // Use gene dictionary directly
        (
            TrainingData {
                data: log_dict_dk.clone(),
                membership: membership_ga.clone(),
                gene_names: row_names.clone(),
            },
            false,
        )
    };

    // 4. Build and train deconvolution model(s)
    let dev = candle_core::Device::Cpu;
    let data = &training_data.data;

    info!("Using {:?} model", args.model);

    // Closure: train model with given membership matrix, return PIP
    let train_model = |membership: &Mat| -> anyhow::Result<Mat> {
        // Apply design matrix normalization
        let normalized_membership = match args.design_norm {
            DesignNorm::None => membership.clone(),
            DesignNorm::Scale => {
                info!("Applying column scaling to design matrix");
                membership.scale_columns()
            }
            DesignNorm::L2 => {
                info!("Applying L2 normalization to design matrix");
                membership.normalize_columns()
            }
        };

        let x_ga = normalized_membership.to_tensor(&dev)?;

        if args.prior_var.len() == 1 {
            let prior_var = args.prior_var[0];
            let params = candle_nn::VarMap::new();
            let vb = candle_nn::VarBuilder::from_varmap(&params, candle_core::DType::F32, &dev);

            let pip = match args.model {
                AnnotationModel::Susie => {
                    let m = SusieDeconv::new(
                        x_ga,
                        data.ncols(),
                        args.susie_components,
                        prior_var,
                        args.num_samples,
                        vb,
                    )?;
                    run_training(&m, &params, args, data, &dev, use_pseudobulk)?.0
                }
                AnnotationModel::BiSusie => {
                    let m = BiSusieDeconv::new(
                        x_ga,
                        data.ncols(),
                        args.susie_components,
                        prior_var,
                        args.num_samples,
                        vb,
                    )?;
                    run_training(&m, &params, args, data, &dev, use_pseudobulk)?.0
                }
            };
            return Ok(pip);
        }

        // Model averaging over multiple prior_var
        let mut pips: Vec<Mat> = Vec::new();
        let mut elbos: Vec<f32> = Vec::new();

        for &prior_var in &args.prior_var {
            let params = candle_nn::VarMap::new();
            let vb = candle_nn::VarBuilder::from_varmap(&params, candle_core::DType::F32, &dev);

            let (pip, loss) = match args.model {
                AnnotationModel::Susie => {
                    let m = SusieDeconv::new(
                        x_ga.clone(),
                        data.ncols(),
                        args.susie_components,
                        prior_var,
                        args.num_samples,
                        vb,
                    )?;
                    run_training(&m, &params, args, data, &dev, use_pseudobulk)?
                }
                AnnotationModel::BiSusie => {
                    let m = BiSusieDeconv::new(
                        x_ga.clone(),
                        data.ncols(),
                        args.susie_components,
                        prior_var,
                        args.num_samples,
                        vb,
                    )?;
                    run_training(&m, &params, args, data, &dev, use_pseudobulk)?
                }
            };
            pips.push(pip);
            elbos.push(-loss);
        }

        // Softmax weights from ELBOs
        let max_e = elbos.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
        let exp_e: Vec<f32> = elbos.iter().map(|&e| (e - max_e).exp()).collect();
        let sum_e: f32 = exp_e.iter().sum();
        let weights: Vec<f32> = exp_e.iter().map(|&e| e / sum_e).collect();

        // Weighted average
        let (nr, nc) = (pips[0].nrows(), pips[0].ncols());
        let mut avg = Mat::zeros(nr, nc);
        for (pip, &w) in pips.iter().zip(&weights) {
            for i in 0..nr {
                for j in 0..nc {
                    avg[(i, j)] += w * pip[(i, j)];
                }
            }
        }
        Ok(avg)
    };

    // 5. Train with optional marker augmentation (interactive, suggest-only, or apply)
    let original_markers = read_marker_gene_info(&args.marker_file)?;
    let mut membership = training_data.membership.clone();
    let mut all_new_markers: Vec<(Box<str>, Box<str>)> = Vec::new();

    // Load reference marker database if provided (with vocab-aware parsing)
    let marker_db = if let Some(db_path) = &args.marker_db {
        info!("Loading marker database from {}...", db_path);
        let db = MarkerDatabase::load_with_vocab(db_path, &training_data.gene_names, &annot_names)?;
        info!("Loaded {} genes from marker database", db.len());
        Some(db)
    } else {
        None
    };

    // Handle apply-suggestions mode (non-interactive)
    if let Some(suggestions_path) = &args.apply_suggestions {
        info!("Applying suggestions from {}...", suggestions_path);
        let suggestions = read_suggestions_json(suggestions_path)?;
        info!("Read {} suggestions", suggestions.len());

        augment_membership_matrix(
            &mut membership,
            &training_data.gene_names,
            &annot_names,
            &suggestions,
            args.augment_weight,
        );
        all_new_markers.extend(suggestions);
    }

    // Handle suggest-only mode (non-interactive)
    let pip_mat = if let Some(output_path) = &args.suggest_only {
        info!("Training initial model...");
        let pip = train_model(&membership)?;

        let candidates = find_candidate_markers(
            &pip,
            data,
            &membership,
            &training_data.gene_names,
            &topic_names,
            &annot_names,
            args.interactive_min_pip,
            args.top_candidates,
            args.topics_per_celltype,
        );

        info!("Writing {} cell types with candidates to {}", candidates.len(), output_path);
        write_candidates_json(&candidates, output_path)?;

        // Also auto-suggest if marker_db provided
        if let Some(db) = &marker_db {
            let auto_suggestions = auto_suggest_markers(&candidates, db);
            if !auto_suggestions.is_empty() {
                let auto_file = format!("{}.auto_suggestions.json", args.out);
                eprintln!("Auto-suggested {} markers based on database:", auto_suggestions.len());
                for (gene, ct) in &auto_suggestions {
                    eprintln!("  {} -> {}", gene, ct);
                }

                // Write auto-suggestions as simple JSON
                let mut file = std::fs::File::create(&auto_file)?;
                use std::io::Write;
                writeln!(file, "[")?;
                for (i, (gene, ct)) in auto_suggestions.iter().enumerate() {
                    let comma = if i + 1 < auto_suggestions.len() { "," } else { "" };
                    writeln!(file, "  {{\"gene\": \"{}\", \"celltype\": \"{}\"}}{}",
                        gene, ct, comma)?;
                }
                writeln!(file, "]")?;
                info!("Wrote auto-suggestions to {}", auto_file);
            }
        }

        pip
    } else if args.interactive {
        let mut iteration = 1usize;
        loop {
            info!("Training iteration {}...", iteration);
            let pip = train_model(&membership)?;

            let candidates = find_candidate_markers(
                &pip,
                data,
                &membership,
                &training_data.gene_names,
                &topic_names,
                &annot_names,
                args.interactive_min_pip,
                args.top_candidates,
                args.topics_per_celltype,
            );

            // Auto-accept known markers if database provided
            let auto_accepted = if let Some(db) = &marker_db {
                let suggestions = auto_suggest_markers(&candidates, db);
                if !suggestions.is_empty() {
                    eprintln!();
                    eprintln!("Auto-accepted {} markers from database:", suggestions.len());
                    for (gene, ct) in &suggestions {
                        eprintln!("  + {} -> {}", gene, ct);
                    }
                    augment_membership_matrix(
                        &mut membership,
                        &training_data.gene_names,
                        &annot_names,
                        &suggestions,
                        args.augment_weight,
                    );
                    all_new_markers.extend(suggestions.clone());
                }
                suggestions
            } else {
                Vec::new()
            };

            // Filter out auto-accepted from interactive prompts
            // (for now, show all - user can skip already-added)
            let _ = auto_accepted;

            let result = run_interactive_round(&candidates, iteration)?;

            if !result.proceed && result.new_markers.is_empty() {
                // User cancelled with no pending markers
                return Ok(());
            }

            if result.new_markers.is_empty() {
                info!("No new markers. Finalizing...");
                break pip;
            }

            // Add new markers to membership
            augment_membership_matrix(
                &mut membership,
                &training_data.gene_names,
                &annot_names,
                &result.new_markers,
                args.augment_weight,
            );
            all_new_markers.extend(result.new_markers);

            if !result.proceed {
                // User added markers but doesn't want more rounds - do final fit
                info!("Final training with augmented markers...");
                break train_model(&membership)?;
            }

            iteration += 1;
        }
    } else {
        train_model(&membership)?
    };

    // Save augmented markers if any
    if !all_new_markers.is_empty() {
        let marker_file = format!("{}.augmented_markers.tsv", args.out);
        save_augmented_markers(&original_markers, &all_new_markers, &marker_file)?;
        print_augmentation_summary(&all_new_markers, all_new_markers.len());
        info!("Saved augmented markers to {}", marker_file);
    }

    // 5. Output results
    let pip_file = format!("{}.pip.parquet", args.out);
    let cell_annot_file = format!("{}.annotation.parquet", args.out);

    pip_mat.to_parquet(Some(&annot_names), Some(&topic_names), &pip_file)?;

    // Filter topics by max PIP threshold
    let n_topics = pip_mat.ncols();
    let mut topic_mask = vec![true; n_topics];
    let mut excluded_topics = Vec::new();

    for t in 0..n_topics {
        let max_pip = (0..pip_mat.nrows())
            .map(|a| pip_mat[(a, t)])
            .fold(0.0f32, f32::max);
        if max_pip < args.min_pip {
            topic_mask[t] = false;
            excluded_topics.push((t, max_pip));
        }
    }

    let n_included = topic_mask.iter().filter(|&&x| x).count();

    if !excluded_topics.is_empty() {
        info!(
            "Excluding {} topics with max PIP < {}: {:?}",
            excluded_topics.len(),
            args.min_pip,
            excluded_topics
                .iter()
                .map(|(t, p)| format!("{}({:.3})", topic_names[*t], p))
                .collect::<Vec<_>>()
        );
    }

    // Check if any topics remain
    if n_included == 0 {
        log::warn!(
            "All {} topics were excluded (max PIP < {}). \
             Try lowering --min-pip threshold or adjusting --prior-var.",
            n_topics,
            args.min_pip
        );
        // Use uniform annotation as fallback
        let n_annots = annot_names.len();
        let uniform_prob = 1.0 / n_annots as f32;
        let topic_annot = Mat::from_fn(cell_names.len(), n_annots, |_, _| uniform_prob);
        topic_annot.to_parquet(Some(&cell_names), Some(&annot_names), &cell_annot_file)?;

        // Output argmax assignments (will be arbitrary due to uniform probs)
        let argmax_file = format!("{}.argmax.tsv", args.out);
        write_argmax_assignments(&topic_annot, &cell_names, &annot_names, &argmax_file)?;

        display_annotation_histogram(&topic_annot, &annot_names);
        return Ok(());
    }

    // Apply mask to PIP matrix (zero out excluded topics)
    let mut pip_filtered = pip_mat.clone();
    for t in 0..n_topics {
        if !topic_mask[t] {
            for a in 0..pip_filtered.nrows() {
                pip_filtered[(a, t)] = 0.0;
            }
        }
    }

    // Cell annotation = filtered_PIP × topic_proportions, then normalize
    let topic_tn = topic_nt.transpose().sum_to_one_columns();
    let topic_annot = (&pip_filtered * topic_tn).sum_to_one_columns().transpose();
    topic_annot.to_parquet(Some(&cell_names), Some(&annot_names), &cell_annot_file)?;

    // Output argmax assignments
    let argmax_file = format!("{}.argmax.tsv", args.out);
    write_argmax_assignments(&topic_annot, &cell_names, &annot_names, &argmax_file)?;

    // Show annotation summary histogram
    display_annotation_histogram(&topic_annot, &annot_names);

    Ok(())
}

/// Write argmax cell type assignment for each cell
fn write_argmax_assignments(
    annot: &Mat,
    cell_names: &[Box<str>],
    annot_names: &[Box<str>],
    output_file: &str,
) -> anyhow::Result<()> {
    use std::io::Write;
    let mut file = std::fs::File::create(output_file)?;
    writeln!(file, "cell\tcell_type\tprobability")?;

    for i in 0..annot.nrows() {
        let row = annot.row(i);
        let (max_idx, max_val) = row
            .iter()
            .enumerate()
            .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
            .unwrap();
        writeln!(
            file,
            "{}\t{}\t{:.4}",
            cell_names[i], annot_names[max_idx], max_val
        )?;
    }
    info!("Wrote argmax assignments to {}", output_file);
    Ok(())
}

/// Display histogram of cell type assignments
fn display_annotation_histogram(annot: &Mat, annot_names: &[Box<str>]) {
    let n_cells = annot.nrows();
    let n_types = annot.ncols();

    // Compute max probability and argmax per cell
    let mut max_probs = Vec::with_capacity(n_cells);
    let mut assignments = Vec::with_capacity(n_cells);

    for i in 0..n_cells {
        let row = annot.row(i);
        let (max_idx, max_val) = row
            .iter()
            .enumerate()
            .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
            .unwrap();
        max_probs.push(*max_val);
        assignments.push(max_idx);
    }

    // Count per cell type
    let mut type_counts = vec![0usize; n_types];
    let mut type_prob_sum = vec![0.0f32; n_types];
    for (i, &ct) in assignments.iter().enumerate() {
        type_counts[ct] += 1;
        type_prob_sum[ct] += max_probs[i];
    }

    // Sort by count descending
    let mut sorted_types: Vec<usize> = (0..n_types).collect();
    sorted_types.sort_by(|&a, &b| type_counts[b].cmp(&type_counts[a]));

    let max_count = *type_counts.iter().max().unwrap_or(&1);
    const MAX_BAR: usize = 20;

    // Statistics
    let mean_prob: f32 = max_probs.iter().sum::<f32>() / n_cells as f32;
    let above_50 = max_probs.iter().filter(|&&x| x > 0.5).count();
    let above_70 = max_probs.iter().filter(|&&x| x > 0.7).count();

    eprintln!();
    eprintln!("Annotation Summary ({} cells)", n_cells);
    eprintln!(
        "  Mean max-prob: {:.3}  >0.5: {} ({:.1}%)  >0.7: {} ({:.1}%)",
        mean_prob,
        above_50,
        100.0 * above_50 as f32 / n_cells as f32,
        above_70,
        100.0 * above_70 as f32 / n_cells as f32
    );
    eprintln!();

    for &ct in &sorted_types {
        if type_counts[ct] == 0 {
            continue;
        }
        let name = &annot_names[ct];
        let count = type_counts[ct];
        let pct = 100.0 * count as f32 / n_cells as f32;
        let mean_p = type_prob_sum[ct] / count as f32;
        let bar_len = ((count as f64 / max_count as f64) * MAX_BAR as f64).round() as usize;
        let bar = "#".repeat(bar_len.max(1));

        eprintln!(
            "  {:20} {:>6} {:>5.1}%  {:.3}  {}",
            &name[..name.len().min(20)],
            count,
            pct,
            mean_p,
            bar
        );
    }
    eprintln!();
}

/// Trait for deconvolution models
trait DeconvModel {
    fn loss(&self, y_gt: &candle_core::Tensor) -> candle_core::Result<candle_core::Tensor>;
    fn loss_with_kl_weight(
        &self,
        y_gt: &candle_core::Tensor,
        kl_weight: f32,
    ) -> candle_core::Result<candle_core::Tensor>;
    fn pip(&self) -> candle_core::Result<candle_core::Tensor>;
}

impl DeconvModel for SusieDeconv {
    fn loss(&self, y_gt: &candle_core::Tensor) -> candle_core::Result<candle_core::Tensor> {
        SusieDeconv::loss(self, y_gt)
    }
    fn loss_with_kl_weight(
        &self,
        y_gt: &candle_core::Tensor,
        kl_weight: f32,
    ) -> candle_core::Result<candle_core::Tensor> {
        SusieDeconv::loss_with_kl_weight(self, y_gt, kl_weight)
    }
    fn pip(&self) -> candle_core::Result<candle_core::Tensor> {
        SusieDeconv::pip(self)
    }
}

impl DeconvModel for BiSusieDeconv {
    fn loss(&self, y_gt: &candle_core::Tensor) -> candle_core::Result<candle_core::Tensor> {
        BiSusieDeconv::loss(self, y_gt)
    }
    fn loss_with_kl_weight(
        &self,
        y_gt: &candle_core::Tensor,
        kl_weight: f32,
    ) -> candle_core::Result<candle_core::Tensor> {
        BiSusieDeconv::loss_with_kl_weight(self, y_gt, kl_weight)
    }
    fn pip(&self) -> candle_core::Result<candle_core::Tensor> {
        BiSusieDeconv::pip(self)
    }
}

/// Returns (PIP matrix, average loss after annealing)
fn run_training<M: DeconvModel>(
    model: &M,
    parameters: &candle_nn::VarMap,
    args: &AnnotateTopicArgs,
    data_gt: &Mat,
    dev: &candle_core::Device,
    _use_pseudobulk: bool,
) -> anyhow::Result<(Mat, f32)> {
    use rand_distr::{Distribution, Poisson};

    let mut adam = AdamW::new_lr(parameters.all_vars(), args.learning_rate)?;

    // KL annealing parameters
    let use_kl_annealing = args.kl_anneal_epochs > 0;
    let annealing_done_epoch = args.kl_anneal_epochs;

    // Total epochs = annealing epochs + training epochs
    let total_epochs = if use_kl_annealing {
        args.kl_anneal_epochs + args.epochs
    } else {
        args.epochs
    };

    let pb = ProgressBar::new(total_epochs as u64);

    if args.verbose {
        pb.set_draw_target(ProgressDrawTarget::hidden());
    }

    // Track losses after annealing for averaging
    let mut post_anneal_losses: Vec<f32> = Vec::new();

    // Precompute scaled data (used when not sampling)
    let scale = args.column_sum_norm;
    let scaled_data = data_gt.map(|x| (x.exp() * scale).max(1e-10));
    let y_gt_fixed = if !args.poisson_sampling {
        Some(scaled_data.to_tensor(dev)?)
    } else {
        None
    };

    // Training loop
    for epoch in 0..total_epochs {
        // Either sample Y[g,t] ~ Poisson(rate) or use scaled data directly
        let y_gt = if args.poisson_sampling {
            let mut rng = rand::rngs::StdRng::seed_from_u64(epoch as u64 + 1);
            scaled_data
                .map(|rate| Poisson::new(rate as f64).unwrap().sample(&mut rng) as f32)
                .to_tensor(dev)?
        } else {
            y_gt_fixed.as_ref().unwrap().clone()
        };

        // Compute loss with optional KL annealing (warm-up: 0 → 1)
        let loss = if use_kl_annealing {
            let kl_weight = (epoch as f32 / args.kl_anneal_epochs as f32).min(1.0);
            model.loss_with_kl_weight(&y_gt, kl_weight)?
        } else {
            model.loss(&y_gt)?
        };

        adam.backward_step(&loss)?;
        let loss_val: f32 = loss.to_scalar()?;
        pb.inc(1);

        // Accumulate losses after annealing is complete
        if epoch >= annealing_done_epoch {
            post_anneal_losses.push(loss_val);
        }

        if args.verbose {
            if use_kl_annealing && epoch < args.kl_anneal_epochs {
                let kl_weight = (epoch as f32 / args.kl_anneal_epochs as f32).min(1.0);
                info!(
                    "[{}] loss={:.2} kl_weight={:.3}",
                    epoch, loss_val, kl_weight
                );
            } else {
                info!("[{}] loss={:.2}", epoch, loss_val);
            }
        }
    }

    pb.finish_and_clear();

    // Compute average loss after annealing (or all losses if no annealing)
    let avg_loss = if post_anneal_losses.is_empty() {
        0.0
    } else {
        post_anneal_losses.iter().sum::<f32>() / post_anneal_losses.len() as f32
    };

    // Get PIP (Posterior Inclusion Probabilities) - annotation x topic
    let pip = model.pip()?;
    Ok((Mat::from_tensor(&pip)?, avg_loss))
}

fn read_mat(file_path: &str) -> anyhow::Result<MatWithNames<Mat>> {
    Ok(match file_ext(file_path)?.as_ref() {
        "parquet" => Mat::from_parquet(file_path)?,
        _ => Mat::read_data(file_path, &['\t', ','], None, Some(0), None, None)?,
    })
}

fn read_marker_gene_info(file_path: &str) -> anyhow::Result<HashMap<Box<str>, Box<str>>> {
    let ReadLinesOut { lines, header: _ } =
        read_lines_of_words_delim(&file_path, &['\t', ','], -1)?;

    Ok(lines
        .into_iter()
        .filter_map(|words| {
            if words.len() < 2 {
                None
            } else {
                Some((words[0].clone(), words[1].clone()))
            }
        })
        .collect())
}

fn build_annotation_matrix(
    marker_gene_path: &str,
    row_names: &[Box<str>],
) -> anyhow::Result<AnnotInfo> {
    let gene_to_type = read_marker_gene_info(marker_gene_path)?;

    if gene_to_type.is_empty() {
        return Err(anyhow::anyhow!("empty/invalid marker gene information"));
    }

    let mut row_to_type_vec = vec![];

    for (ri, rn) in row_names.iter().enumerate() {
        // Check each marker gene using flexible matching
        for (marker_gene, cell_type) in gene_to_type.iter() {
            if flexible_gene_match(marker_gene, rn) {
                row_to_type_vec.push((ri, cell_type));
            }
        }
    }

    row_to_type_vec.sort();
    row_to_type_vec.dedup();

    let mut id_to_celltypes: Vec<Box<str>> = row_to_type_vec
        .iter()
        .map(|&(_, t)| t.clone())
        .collect::<HashSet<_>>()
        .into_iter()
        .collect();

    id_to_celltypes.sort();

    let celltype_to_id: HashMap<Box<str>, usize> = id_to_celltypes
        .iter()
        .enumerate()
        .map(|(i, x)| (x.clone(), i))
        .collect();

    let triplets = row_to_type_vec
        .iter()
        .filter_map(|&(r, ct)| {
            if let Some(t) = celltype_to_id.get(ct) {
                Some((r, *t, 1.0))
            } else {
                None
            }
        })
        .collect::<Vec<_>>();

    let nrow = row_names.len();
    let ncol = id_to_celltypes.len();

    // gene x annotation group
    let membership_ga = Mat::from_nonzero_triplets(nrow, ncol, &triplets)?;

    // remove potential ' '
    let annot_names = id_to_celltypes
        .into_iter()
        .map(|x| x.replace(" ", "_").into_boxed_str())
        .collect();

    Ok(AnnotInfo {
        membership_ga,
        annot_names,
    })
}

struct AnnotInfo {
    membership_ga: Mat,
    annot_names: Vec<Box<str>>,
}
