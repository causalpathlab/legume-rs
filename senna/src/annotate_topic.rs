use crate::deconv::{VmfBiSusieDeconv, VmfSusieDeconv};
use crate::embed_common::*;
use crate::interactive_markers::{
    auto_suggest_markers, augment_membership_matrix, find_candidate_markers,
    flexible_gene_match, print_augmentation_summary, read_suggestions_json,
    run_interactive_round, save_augmented_markers, write_candidates_json, MarkerDatabase,
};
use matrix_util::common_io::*;

use candle_nn::AdamW;
use candle_nn::Optimizer;
use fnv::FnvHashMap as HashMap;
use fnv::FnvHashSet as HashSet;
use indicatif::{ProgressBar, ProgressDrawTarget};

/// Model type for topic annotation
#[derive(Clone, Copy, Debug, Default, ValueEnum)]
pub enum AnnotationModel {
    /// Standard SuSiE: sparse in annotation dimension only
    Susie,
    /// Bi-directional SuSiE: sparse in both annotation and topic dimensions (softmax)
    #[default]
    BiSusie,
}

/// Design matrix normalization method
#[derive(Clone, Copy, Debug, Default, ValueEnum)]
pub enum DesignNorm {
    /// No normalization
    None,
    /// Column scaling (divide by column sum or std)
    Scale,
    /// L2 normalization (unit length columns)
    L2,
    /// TF-IDF: down-weight genes marking many cell types, up-weight specific markers
    #[default]
    TfIdf,
}

#[derive(Args, Debug)]
pub struct AnnotateTopicArgs {
    #[arg(
        short = 'g',
        long = "gene-dictionary",
        help = "A gene dictionary matrix file (generated by `topic`)",
        long_help = "A gene dictionary matrix file (generated by `topic`)\n\
		      The first column should correspond to features (genes).",
        required = true
    )]
    dict_file: Box<str>,

    #[arg(
        short = 'z',
        long = "latent-topic",
        help = "A latent topic proportion file (generated by `topic`)",
        long_help = "A latent topic proportion file (generated by `topic`)\n\
		      Values should be probabilities (on simplex).\n\
		      The first column should correspond to cell names.",
        required = true
    )]
    latent_file: Box<str>,

    #[arg(
        short = 'm',
        long = "marker-genes",
        help = "A marker feature file",
        long_help = "A marker feature file, e.g., mapping genes to cell types. \n\
		     Each line contains two names: \n\
		     (1) gene and \n\
		     (2) cell type \n\
		     (tab-separated or comma-separated). \n\n\
		     Cell type names: spaces will be replaced with '_' (e.g., 'T cell' -> 'T_cell').\n\n\
		     Gene name matching is flexible (case-insensitive, underscore-delimited):\n\
		     - 'CD8A' matches 'ENSG00000153563_CD8A' (suffix)\n\
		     - 'CD8A' matches 'CD8A_variant1' (prefix)\n\
		     - 'CD8A' matches 'chr1_CD8A_isoform2' (segment)\n\
		     You can use simple gene symbols even if your dictionary has Ensembl IDs.",
        required = true
    )]
    marker_file: Box<str>,


    #[arg(
        long,
        short = 'i',
        default_value_t = 1000,
        help = "Number of training epochs after KL annealing (total = kl-anneal-epochs + epochs)."
    )]
    epochs: usize,


    #[arg(
        short = 'l',
        long,
        default_value_t = 0.1,
        help = "Learning rate for optimization."
    )]
    learning_rate: f64,

    #[arg(
        long,
        short = 'L',
        default_value_t = 10,
        help = "Number of SuSiE components (L)."
    )]
    susie_components: usize,

    #[arg(
        long,
        short = 's',
        default_value_t = 30,
        help = "Number of Monte Carlo samples for SGVB."
    )]
    num_samples: usize,


    #[arg(
        long,
        value_delimiter = ',',
        default_values_t = vec![0.1, 1.0, 10.0],
        help = "Prior variance(s) for effect sizes (comma-separated for grid search).",
        long_help = "Prior variance(s) for effect sizes.\n\
		     If multiple values are provided (comma-separated), performs grid search\n\
		     and selects the best prior variance based on ELBO.\n\
		     Example: --prior-var 0.1,1.0,10.0"
    )]
    prior_var: Vec<f32>,

    #[arg(
        long,
        default_value_t = 200,
        help = "Number of epochs for KL annealing warm-up (0 to disable).",
        long_help = "Number of epochs over which to linearly anneal KL weight from 0 to 1.\n\
		     KL annealing (warm-up) helps prevent posterior collapse by letting the model\n\
		     focus on data fitting first, then gradually enforcing the prior.\n\
		     Set to 0 to disable annealing. Default 200 works well with vMF likelihood."
    )]
    kl_anneal_epochs: usize,

    #[arg(
        long,
        default_value_t = 0.01,
        help = "Minimum max-PIP threshold for including a topic.",
        long_help = "Topics with max PIP below this threshold are excluded from cell annotation.\n\
		     This filters out topics that don't map to any known cell type.\n\
		     Set to 0 to include all topics."
    )]
    min_pip: f32,

    #[arg(
        long,
        short,
        required = true,
        help = "Output header",
        long_help = "Output prefix for generated files.\n\n\
		     Output files:\n\
		     - {out}.pip.parquet: PIP matrix (annotation × topic)\n\
		     - {out}.annotation.parquet: Cell annotations (cell × annotation probabilities)\n\
		     - {out}.argmax.tsv: Argmax cell type assignment per cell\n\
		     - {out}.augmented_markers.tsv: Augmented markers (if --interactive used)\n\
		     - {out}.auto_suggestions.json: Auto-suggestions (if --marker-db with --suggest-only)"
    )]
    out: Box<str>,

    #[arg(
        long,
        short,
        help = "Verbosity.",
        long_help = "Enable verbose output.\n\
		     Prints additional information during execution."
    )]
    verbose: bool,

    #[arg(
        long,
        short = 'M',
        default_value = "bi-susie",
        help = "Model type for annotation.",
        long_help = "Model type for annotation:\n\
             - susie: Standard SuSiE, sparse in annotation dimension only\n\
             - bi-susie: Bi-directional SuSiE, sparse in both annotation and topic dimensions"
    )]
    model: AnnotationModel,

    #[arg(
        long,
        default_value = "tf-idf",
        help = "Design matrix normalization method.",
        long_help = "Normalization applied to design matrix X (gene × annotation membership) before fitting.\n\n\
             Methods:\n\
             - tf-idf: TF-IDF weighting (down-weights common markers, up-weights specific ones) (default)\n\
             - l2: L2 normalization (unit length columns)\n\
             - scale: Column scaling (divide by column statistics)\n\
             - none: No normalization - use raw 0/1 membership values"
    )]
    design_norm: DesignNorm,

    #[arg(
        long,
        short = 'I',
        help = "Enable interactive marker augmentation.",
        long_help = "Enable interactive mode for marker gene augmentation.\n\
             After initial fitting, shows candidate genes for each cell type and\n\
             asks if you want to add them as markers. This iterates until\n\
             you're satisfied with the marker coverage."
    )]
    interactive: bool,

    #[arg(
        long,
        help = "Output candidate markers to JSON for LLM review.",
        long_help = "Output candidate markers to JSON for LLM-assisted annotation.\n\n\
             The JSON contains cell types with candidate genes ranked by topic weight.\n\
             Share this with an LLM and ask it to:\n\
             1. Web search each gene to find its known cell type markers\n\
             2. Confirm or reject each gene-celltype association\n\
             3. Output validated pairs as: [{\"gene\": \"X\", \"celltype\": \"Y\"}, ...]\n\n\
             Example prompt for LLM:\n\
             \"For each candidate gene, search what cell types it marks.\n\
             Return only genes that are known markers for the proposed cell type.\""
    )]
    suggest_only: Option<Box<str>>,

    #[arg(
        long,
        help = "Apply marker suggestions from JSON file.",
        long_help = "Apply LLM-validated marker suggestions from JSON.\n\n\
             Expected format: [{\"gene\": \"ENSG..._SYM\", \"celltype\": \"Cell_Type\"}, ...]\n\
             Gene name matching is flexible (same as --marker-genes):\n\
             you can use symbols like 'CD8A' or full names like 'ENSG00000153563_CD8A'.\n\
             Cell type names must match marker file annotations.\n\n\
             Workflow: --suggest-only out.json -> LLM review -> --apply-suggestions validated.json"
    )]
    apply_suggestions: Option<Box<str>>,

    #[arg(
        long,
        help = "Reference marker database for auto-suggestions (e.g., PanglaoDB TSV).",
        long_help = "Path to a reference marker gene database (TSV with gene and cell_type columns).\n\
             When provided, candidates matching known markers are auto-accepted.\n\
             Unmatched candidates are shown for manual review (if --interactive) or skipped.\n\n\
             Matching behavior:\n\
             - Genes: flexible matching (same as --marker-genes)\n\
             - Cell types: fuzzy substring matching to handle name variations"
    )]
    marker_db: Option<Box<str>>,

    #[arg(
        long,
        default_value_t = 3,
        help = "Number of candidate genes to show per topic in interactive mode."
    )]
    top_candidates: usize,

    #[arg(
        long,
        default_value_t = 2,
        help = "Number of top topics to show per cell type in interactive mode."
    )]
    topics_per_celltype: usize,

    #[arg(
        long,
        default_value_t = 0.05,
        help = "Minimum PIP to show topic-celltype match in interactive mode."
    )]
    interactive_min_pip: f32,

    #[arg(
        long,
        default_value_t = 1.0,
        help = "Weight for augmented markers (vs 1.0 for original markers)."
    )]
    augment_weight: f32,

}

/// Training data bundle
struct TrainingData {
    /// Gene × topic data matrix (log dictionary or normalized pseudobulk)
    data: Mat,
    /// Gene × annotation membership matrix
    membership: Mat,
    /// Gene names (row names of data and membership)
    gene_names: Vec<Box<str>>,
}

pub fn annotate_topics(args: &AnnotateTopicArgs) -> anyhow::Result<()> {
    if args.verbose {
        std::env::set_var("RUST_LOG", "info");
    }
    env_logger::init();

    // 1. read dictionary (log-probabilities) and latent states (probabilities)
    let MatWithNames {
        rows: row_names,
        cols: _topics,
        mat: log_dict_dk,
    } = read_mat(&args.dict_file)?;

    info!(
        "Read dictionary {} x {}",
        log_dict_dk.nrows(),
        log_dict_dk.ncols()
    );

    let MatWithNames {
        rows: cell_names,
        cols: topic_names,
        mat: topic_nt_raw,
    } = read_mat(&args.latent_file)?;

    // Convert log-probabilities to probabilities if needed
    let topic_nt = if topic_nt_raw.max() <= 0.0 {
        info!("Detected log-probabilities in latent file, converting to probabilities");
        topic_nt_raw.map(|x| x.exp())
    } else {
        topic_nt_raw
    };

    // 2. read marker gene annotation
    let AnnotInfo {
        membership_ga,
        annot_names,
    } = build_annotation_matrix(&args.marker_file, &row_names)?;

    let nnz_features = (0..membership_ga.nrows())
        .filter(|&i| membership_ga.row(i).iter().any(|&x| x > 0.0))
        .count();

    info!(
        "Found {} cell types matched over {} features",
        annot_names.len(),
        nnz_features
    );

    // 3. Prepare training data from gene dictionary
    let training_data = TrainingData {
        data: log_dict_dk.clone(),
        membership: membership_ga.clone(),
        gene_names: row_names.clone(),
    };

    // 4. Build and train deconvolution model(s)
    let dev = candle_core::Device::Cpu;
    let full_data = &training_data.data;

    info!("Using {:?} model", args.model);

    // Closure: train model with given membership matrix, return PIP
    let train_model = |membership: &Mat| -> anyhow::Result<Mat> {
        // Find marker genes (annotated - non-zero rows in membership)
        let marker_gene_indices: Vec<usize> = (0..membership.nrows())
            .filter(|&i| membership.row(i).iter().any(|&x| x > 0.0))
            .collect();

        // Find control genes: not annotated, with flat expression profiles (low variance)
        // These are non-informative genes that help the model learn "no cell-type specificity"
        let mut unannotated_with_scores: Vec<(usize, f32)> = (0..membership.nrows())
            .filter(|&i| membership.row(i).iter().all(|&x| x == 0.0))
            .filter_map(|i| {
                let row = full_data.row(i);
                let n = row.len() as f32;
                let mean = row.iter().sum::<f32>() / n;
                // Filter out lowly expressed genes (noise-dominated)
                // Threshold: mean log-prob > -10 (i.e., prob > exp(-10) ≈ 4.5e-5)
                if mean < -10.0 {
                    return None;
                }
                // Compute variance: lower variance = flatter profile = better control
                let variance = row.iter().map(|&x| (x - mean).powi(2)).sum::<f32>() / n;
                Some((i, variance))
            })
            .collect();

        // Sort by variance ascending (lowest variance = flattest profile first)
        unannotated_with_scores.sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap());

        // Take same number of controls as markers (50-50 split)
        let n_controls = marker_gene_indices.len();
        let control_gene_indices: Vec<usize> = unannotated_with_scores
            .iter()
            .take(n_controls)
            .map(|(i, _)| *i)
            .collect();

        // Combine marker and control genes
        let mut selected_indices = marker_gene_indices.clone();
        selected_indices.extend(&control_gene_indices);
        selected_indices.sort();

        let (data, membership_filtered) = if selected_indices.len() < full_data.nrows() {
            info!(
                "Selected {} genes: {} markers + {} controls (from {} total)",
                selected_indices.len(),
                marker_gene_indices.len(),
                control_gene_indices.len(),
                full_data.nrows()
            );
            (
                full_data.select_rows(&selected_indices),
                membership.select_rows(&selected_indices),
            )
        } else {
            (full_data.clone(), membership.clone())
        };

        // Apply design matrix normalization
        let normalized_membership = match args.design_norm {
            DesignNorm::None => membership_filtered,
            DesignNorm::Scale => {
                info!("Applying column scaling to design matrix");
                membership_filtered.scale_columns()
            }
            DesignNorm::L2 => {
                info!("Applying L2 normalization to design matrix");
                membership_filtered.normalize_columns()
            }
            DesignNorm::TfIdf => {
                info!("Applying TF-IDF transformation to design matrix");
                membership_filtered.tfidf_normalize_columns()
            }
        };

        let x_ga = normalized_membership.to_tensor(&dev)?;

        // Prepare normalized y for vMF: exp(log_prob) then L2 normalize
        info!("Using von Mises-Fisher likelihood (angle matching)");
        let probs = data.map(|x| x.exp());
        let y_normalized = probs.normalize_columns().to_tensor(&dev)?;

        if args.prior_var.len() == 1 {
            let prior_var = args.prior_var[0];
            let params = candle_nn::VarMap::new();
            let vb = candle_nn::VarBuilder::from_varmap(&params, candle_core::DType::F32, &dev);

            let pip = match args.model {
                AnnotationModel::Susie => {
                    let m = VmfSusieDeconv::new(
                        x_ga,
                        y_normalized,
                        data.ncols(),
                        args.susie_components,
                        prior_var,
                        args.num_samples,
                        vb,
                    )?;
                    run_training_vmf(&m, &params, args)?.0
                }
                AnnotationModel::BiSusie => {
                    let m = VmfBiSusieDeconv::new(
                        x_ga,
                        y_normalized,
                        data.ncols(),
                        args.susie_components,
                        prior_var,
                        args.num_samples,
                        vb,
                    )?;
                    run_training_vmf(&m, &params, args)?.0
                }
            };
            return Ok(pip);
        }

        // Model averaging over multiple prior_var
        let mut pips: Vec<Mat> = Vec::new();
        let mut elbos: Vec<f32> = Vec::new();
        let n_topics = data.ncols();

        for &prior_var in &args.prior_var {
            let params = candle_nn::VarMap::new();
            let vb = candle_nn::VarBuilder::from_varmap(&params, candle_core::DType::F32, &dev);

            let (pip, loss) = match args.model {
                AnnotationModel::Susie => {
                    let m = VmfSusieDeconv::new(
                        x_ga.clone(),
                        y_normalized.clone(),
                        n_topics,
                        args.susie_components,
                        prior_var,
                        args.num_samples,
                        vb,
                    )?;
                    run_training_vmf(&m, &params, args)?
                }
                AnnotationModel::BiSusie => {
                    let m = VmfBiSusieDeconv::new(
                        x_ga.clone(),
                        y_normalized.clone(),
                        n_topics,
                        args.susie_components,
                        prior_var,
                        args.num_samples,
                        vb,
                    )?;
                    run_training_vmf(&m, &params, args)?
                }
            };
            pips.push(pip);
            elbos.push(-loss);
        }

        // Softmax weights from ELBOs
        let max_e = elbos.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
        let exp_e: Vec<f32> = elbos.iter().map(|&e| (e - max_e).exp()).collect();
        let sum_e: f32 = exp_e.iter().sum();
        let weights: Vec<f32> = exp_e.iter().map(|&e| e / sum_e).collect();

        // Weighted average
        let (nr, nc) = (pips[0].nrows(), pips[0].ncols());
        let mut avg = Mat::zeros(nr, nc);
        for (pip, &w) in pips.iter().zip(&weights) {
            for i in 0..nr {
                for j in 0..nc {
                    avg[(i, j)] += w * pip[(i, j)];
                }
            }
        }
        Ok(avg)
    };

    // 5. Train with optional marker augmentation (interactive, suggest-only, or apply)
    let original_markers = read_marker_gene_info(&args.marker_file)?;
    let mut membership = training_data.membership.clone();
    let mut all_new_markers: Vec<(Box<str>, Box<str>)> = Vec::new();

    // Load reference marker database if provided (with vocab-aware parsing)
    let marker_db = if let Some(db_path) = &args.marker_db {
        info!("Loading marker database from {}...", db_path);
        let db = MarkerDatabase::load_with_vocab(db_path, &training_data.gene_names, &annot_names)?;
        info!("Loaded {} genes from marker database", db.len());
        Some(db)
    } else {
        None
    };

    // Handle apply-suggestions mode (non-interactive)
    if let Some(suggestions_path) = &args.apply_suggestions {
        info!("Applying suggestions from {}...", suggestions_path);
        let suggestions = read_suggestions_json(suggestions_path)?;
        info!("Read {} suggestions", suggestions.len());

        augment_membership_matrix(
            &mut membership,
            &training_data.gene_names,
            &annot_names,
            &suggestions,
            args.augment_weight,
        );
        all_new_markers.extend(suggestions);
    }

    // Handle suggest-only mode (non-interactive)
    let pip_mat = if let Some(output_path) = &args.suggest_only {
        info!("Training initial model...");
        let pip = train_model(&membership)?;

        let candidates = find_candidate_markers(
            &pip,
            full_data,
            &membership,
            &training_data.gene_names,
            &topic_names,
            &annot_names,
            args.interactive_min_pip,
            args.top_candidates,
            args.topics_per_celltype,
        );

        info!("Writing {} cell types with candidates to {}", candidates.len(), output_path);
        write_candidates_json(&candidates, output_path)?;

        // Also auto-suggest if marker_db provided
        if let Some(db) = &marker_db {
            let auto_suggestions = auto_suggest_markers(&candidates, db);
            if !auto_suggestions.is_empty() {
                let auto_file = format!("{}.auto_suggestions.json", args.out);
                eprintln!("Auto-suggested {} markers based on database:", auto_suggestions.len());
                for (gene, ct) in &auto_suggestions {
                    eprintln!("  {} -> {}", gene, ct);
                }

                // Write auto-suggestions as simple JSON
                let mut file = std::fs::File::create(&auto_file)?;
                use std::io::Write;
                writeln!(file, "[")?;
                for (i, (gene, ct)) in auto_suggestions.iter().enumerate() {
                    let comma = if i + 1 < auto_suggestions.len() { "," } else { "" };
                    writeln!(file, "  {{\"gene\": \"{}\", \"celltype\": \"{}\"}}{}",
                        gene, ct, comma)?;
                }
                writeln!(file, "]")?;
                info!("Wrote auto-suggestions to {}", auto_file);
            }
        }

        pip
    } else if args.interactive {
        let mut iteration = 1usize;
        loop {
            info!("Training iteration {}...", iteration);
            let pip = train_model(&membership)?;

            let candidates = find_candidate_markers(
                &pip,
                full_data,
                &membership,
                &training_data.gene_names,
                &topic_names,
                &annot_names,
                args.interactive_min_pip,
                args.top_candidates,
                args.topics_per_celltype,
            );

            // Auto-accept known markers if database provided
            let auto_accepted = if let Some(db) = &marker_db {
                let suggestions = auto_suggest_markers(&candidates, db);
                if !suggestions.is_empty() {
                    eprintln!();
                    eprintln!("Auto-accepted {} markers from database:", suggestions.len());
                    for (gene, ct) in &suggestions {
                        eprintln!("  + {} -> {}", gene, ct);
                    }
                    augment_membership_matrix(
                        &mut membership,
                        &training_data.gene_names,
                        &annot_names,
                        &suggestions,
                        args.augment_weight,
                    );
                    all_new_markers.extend(suggestions.clone());
                }
                suggestions
            } else {
                Vec::new()
            };

            // Filter out auto-accepted from interactive prompts
            // (for now, show all - user can skip already-added)
            let _ = auto_accepted;

            let result = run_interactive_round(&candidates, iteration)?;

            if !result.proceed && result.new_markers.is_empty() {
                // User cancelled with no pending markers
                return Ok(());
            }

            if result.new_markers.is_empty() {
                info!("No new markers. Finalizing...");
                break pip;
            }

            // Add new markers to membership
            augment_membership_matrix(
                &mut membership,
                &training_data.gene_names,
                &annot_names,
                &result.new_markers,
                args.augment_weight,
            );
            all_new_markers.extend(result.new_markers);

            if !result.proceed {
                // User added markers but doesn't want more rounds - do final fit
                info!("Final training with augmented markers...");
                break train_model(&membership)?;
            }

            iteration += 1;
        }
    } else {
        train_model(&membership)?
    };

    // Save augmented markers if any
    if !all_new_markers.is_empty() {
        let marker_file = format!("{}.augmented_markers.tsv", args.out);
        save_augmented_markers(&original_markers, &all_new_markers, &marker_file)?;
        print_augmentation_summary(&all_new_markers, all_new_markers.len());
        info!("Saved augmented markers to {}", marker_file);
    }

    // 5. Output results
    let pip_file = format!("{}.pip.parquet", args.out);
    let cell_annot_file = format!("{}.annotation.parquet", args.out);

    pip_mat.to_parquet(Some(&annot_names), Some(&topic_names), &pip_file)?;

    // Filter topics by max PIP threshold
    let n_topics = pip_mat.ncols();
    let mut topic_mask = vec![true; n_topics];
    let mut excluded_topics = Vec::new();

    for t in 0..n_topics {
        let max_pip = (0..pip_mat.nrows())
            .map(|a| pip_mat[(a, t)])
            .fold(0.0f32, f32::max);
        if max_pip < args.min_pip {
            topic_mask[t] = false;
            excluded_topics.push((t, max_pip));
        }
    }

    let n_included = topic_mask.iter().filter(|&&x| x).count();

    if !excluded_topics.is_empty() {
        info!(
            "Excluding {} topics with max PIP < {}: {:?}",
            excluded_topics.len(),
            args.min_pip,
            excluded_topics
                .iter()
                .map(|(t, p)| format!("{}({:.3})", topic_names[*t], p))
                .collect::<Vec<_>>()
        );
    }

    // Check if any topics remain
    if n_included == 0 {
        log::warn!(
            "All {} topics were excluded (max PIP < {}). \
             Try lowering --min-pip threshold or adjusting --prior-var.",
            n_topics,
            args.min_pip
        );
        // Use uniform annotation as fallback
        let n_annots = annot_names.len();
        let uniform_prob = 1.0 / n_annots as f32;
        let topic_annot = Mat::from_fn(cell_names.len(), n_annots, |_, _| uniform_prob);
        topic_annot.to_parquet(Some(&cell_names), Some(&annot_names), &cell_annot_file)?;

        // Output argmax assignments (will be arbitrary due to uniform probs)
        let argmax_file = format!("{}.argmax.tsv", args.out);
        write_argmax_assignments(&topic_annot, &cell_names, &annot_names, &argmax_file)?;

        display_annotation_histogram(&topic_annot, &annot_names);
        return Ok(());
    }

    // Apply mask to PIP matrix (zero out excluded topics)
    let mut pip_filtered = pip_mat.clone();
    for t in 0..n_topics {
        if !topic_mask[t] {
            for a in 0..pip_filtered.nrows() {
                pip_filtered[(a, t)] = 0.0;
            }
        }
    }

    // Cell annotation = filtered_PIP × topic_proportions, then normalize
    let topic_tn = topic_nt.transpose().sum_to_one_columns();
    let topic_annot = (&pip_filtered * topic_tn).sum_to_one_columns().transpose();
    topic_annot.to_parquet(Some(&cell_names), Some(&annot_names), &cell_annot_file)?;

    // Output argmax assignments
    let argmax_file = format!("{}.argmax.tsv", args.out);
    write_argmax_assignments(&topic_annot, &cell_names, &annot_names, &argmax_file)?;

    // Show annotation summary histogram
    display_annotation_histogram(&topic_annot, &annot_names);

    Ok(())
}

/// Write argmax cell type assignment for each cell
fn write_argmax_assignments(
    annot: &Mat,
    cell_names: &[Box<str>],
    annot_names: &[Box<str>],
    output_file: &str,
) -> anyhow::Result<()> {
    use std::io::Write;
    let mut file = std::fs::File::create(output_file)?;
    writeln!(file, "cell\tcell_type\tprobability")?;

    for i in 0..annot.nrows() {
        let row = annot.row(i);
        let (max_idx, max_val) = row
            .iter()
            .enumerate()
            .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
            .unwrap();
        writeln!(
            file,
            "{}\t{}\t{:.4}",
            cell_names[i], annot_names[max_idx], max_val
        )?;
    }
    info!("Wrote argmax assignments to {}", output_file);
    Ok(())
}

/// Display histogram of cell type assignments
fn display_annotation_histogram(annot: &Mat, annot_names: &[Box<str>]) {
    let n_cells = annot.nrows();
    let n_types = annot.ncols();

    // Compute max probability and argmax per cell
    let mut max_probs = Vec::with_capacity(n_cells);
    let mut assignments = Vec::with_capacity(n_cells);

    for i in 0..n_cells {
        let row = annot.row(i);
        let (max_idx, max_val) = row
            .iter()
            .enumerate()
            .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
            .unwrap();
        max_probs.push(*max_val);
        assignments.push(max_idx);
    }

    // Count per cell type
    let mut type_counts = vec![0usize; n_types];
    let mut type_prob_sum = vec![0.0f32; n_types];
    for (i, &ct) in assignments.iter().enumerate() {
        type_counts[ct] += 1;
        type_prob_sum[ct] += max_probs[i];
    }

    // Sort by count descending
    let mut sorted_types: Vec<usize> = (0..n_types).collect();
    sorted_types.sort_by(|&a, &b| type_counts[b].cmp(&type_counts[a]));

    let max_count = *type_counts.iter().max().unwrap_or(&1);
    const MAX_BAR: usize = 20;

    // Statistics
    let mean_prob: f32 = max_probs.iter().sum::<f32>() / n_cells as f32;
    let above_50 = max_probs.iter().filter(|&&x| x > 0.5).count();
    let above_70 = max_probs.iter().filter(|&&x| x > 0.7).count();

    eprintln!();
    eprintln!("Annotation Summary ({} cells)", n_cells);
    eprintln!(
        "  Mean max-prob: {:.3}  >0.5: {} ({:.1}%)  >0.7: {} ({:.1}%)",
        mean_prob,
        above_50,
        100.0 * above_50 as f32 / n_cells as f32,
        above_70,
        100.0 * above_70 as f32 / n_cells as f32
    );
    eprintln!();

    for &ct in &sorted_types {
        if type_counts[ct] == 0 {
            continue;
        }
        let name = &annot_names[ct];
        let count = type_counts[ct];
        let pct = 100.0 * count as f32 / n_cells as f32;
        let mean_p = type_prob_sum[ct] / count as f32;
        let bar_len = ((count as f64 / max_count as f64) * MAX_BAR as f64).round() as usize;
        let bar = "#".repeat(bar_len.max(1));

        eprintln!(
            "  {:20} {:>6} {:>5.1}%  {:.3}  {}",
            &name[..name.len().min(20)],
            count,
            pct,
            mean_p,
            bar
        );
    }
    eprintln!();
}

/// Trait for vMF deconvolution models (y is stored in the model, not passed to loss)
trait VmfDeconvModel {
    fn loss(&self) -> candle_core::Result<candle_core::Tensor>;
    fn loss_with_kl_weight(&self, kl_weight: f32) -> candle_core::Result<candle_core::Tensor>;
    fn pip(&self) -> candle_core::Result<candle_core::Tensor>;
}

impl VmfDeconvModel for VmfSusieDeconv {
    fn loss(&self) -> candle_core::Result<candle_core::Tensor> {
        VmfSusieDeconv::loss(self)
    }
    fn loss_with_kl_weight(&self, kl_weight: f32) -> candle_core::Result<candle_core::Tensor> {
        VmfSusieDeconv::loss_with_kl_weight(self, kl_weight)
    }
    fn pip(&self) -> candle_core::Result<candle_core::Tensor> {
        VmfSusieDeconv::pip(self)
    }
}

impl VmfDeconvModel for VmfBiSusieDeconv {
    fn loss(&self) -> candle_core::Result<candle_core::Tensor> {
        VmfBiSusieDeconv::loss(self)
    }
    fn loss_with_kl_weight(&self, kl_weight: f32) -> candle_core::Result<candle_core::Tensor> {
        VmfBiSusieDeconv::loss_with_kl_weight(self, kl_weight)
    }
    fn pip(&self) -> candle_core::Result<candle_core::Tensor> {
        VmfBiSusieDeconv::pip(self)
    }
}

/// Returns (PIP matrix, average loss after annealing)
fn run_training_vmf<M: VmfDeconvModel>(
    model: &M,
    parameters: &candle_nn::VarMap,
    args: &AnnotateTopicArgs,
) -> anyhow::Result<(Mat, f32)> {
    let mut adam = AdamW::new_lr(parameters.all_vars(), args.learning_rate)?;

    // KL annealing parameters
    let use_kl_annealing = args.kl_anneal_epochs > 0;
    let annealing_done_epoch = args.kl_anneal_epochs;

    // Total epochs = annealing epochs + training epochs
    let total_epochs = if use_kl_annealing {
        args.kl_anneal_epochs + args.epochs
    } else {
        args.epochs
    };

    let pb = ProgressBar::new(total_epochs as u64);

    if args.verbose {
        pb.set_draw_target(ProgressDrawTarget::hidden());
    }

    // Track losses after annealing for averaging
    let mut post_anneal_losses: Vec<f32> = Vec::new();

    // Training loop (vMF: y is already stored in model, no Poisson sampling)
    for epoch in 0..total_epochs {
        // Compute loss with optional KL annealing (warm-up: 0 → 1)
        let loss = if use_kl_annealing {
            let kl_weight = (epoch as f32 / args.kl_anneal_epochs as f32).min(1.0);
            model.loss_with_kl_weight(kl_weight)?
        } else {
            model.loss()?
        };

        adam.backward_step(&loss)?;
        let loss_val: f32 = loss.to_scalar()?;
        pb.inc(1);

        // Accumulate losses after annealing is complete
        if epoch >= annealing_done_epoch {
            post_anneal_losses.push(loss_val);
        }

        if args.verbose {
            if use_kl_annealing && epoch < args.kl_anneal_epochs {
                let kl_weight = (epoch as f32 / args.kl_anneal_epochs as f32).min(1.0);
                info!(
                    "[{}] loss={:.2} kl_weight={:.3}",
                    epoch, loss_val, kl_weight
                );
            } else {
                info!("[{}] loss={:.2}", epoch, loss_val);
            }
        }
    }

    pb.finish_and_clear();

    // Compute average loss after annealing (or all losses if no annealing)
    let avg_loss = if post_anneal_losses.is_empty() {
        0.0
    } else {
        post_anneal_losses.iter().sum::<f32>() / post_anneal_losses.len() as f32
    };

    // Get PIP (Posterior Inclusion Probabilities) - annotation x topic
    let pip = model.pip()?;
    Ok((Mat::from_tensor(&pip)?, avg_loss))
}

fn read_mat(file_path: &str) -> anyhow::Result<MatWithNames<Mat>> {
    Ok(match file_ext(file_path)?.as_ref() {
        "parquet" => Mat::from_parquet(file_path)?,
        _ => Mat::read_data(file_path, &['\t', ','], None, Some(0), None, None)?,
    })
}

fn read_marker_gene_info(file_path: &str) -> anyhow::Result<HashMap<Box<str>, Box<str>>> {
    let ReadLinesOut { lines, header: _ } =
        read_lines_of_words_delim(&file_path, &['\t', ','], -1)?;

    Ok(lines
        .into_iter()
        .filter_map(|words| {
            if words.len() < 2 {
                None
            } else {
                Some((words[0].clone(), words[1].clone()))
            }
        })
        .collect())
}

fn build_annotation_matrix(
    marker_gene_path: &str,
    row_names: &[Box<str>],
) -> anyhow::Result<AnnotInfo> {
    let gene_to_type = read_marker_gene_info(marker_gene_path)?;

    if gene_to_type.is_empty() {
        return Err(anyhow::anyhow!("empty/invalid marker gene information"));
    }

    let mut row_to_type_vec = vec![];

    for (ri, rn) in row_names.iter().enumerate() {
        // Check each marker gene using flexible matching
        for (marker_gene, cell_type) in gene_to_type.iter() {
            if flexible_gene_match(marker_gene, rn) {
                row_to_type_vec.push((ri, cell_type));
            }
        }
    }

    row_to_type_vec.sort();
    row_to_type_vec.dedup();

    let mut id_to_celltypes: Vec<Box<str>> = row_to_type_vec
        .iter()
        .map(|&(_, t)| t.clone())
        .collect::<HashSet<_>>()
        .into_iter()
        .collect();

    id_to_celltypes.sort();

    let celltype_to_id: HashMap<Box<str>, usize> = id_to_celltypes
        .iter()
        .enumerate()
        .map(|(i, x)| (x.clone(), i))
        .collect();

    let triplets = row_to_type_vec
        .iter()
        .filter_map(|&(r, ct)| {
            if let Some(t) = celltype_to_id.get(ct) {
                Some((r, *t, 1.0))
            } else {
                None
            }
        })
        .collect::<Vec<_>>();

    let nrow = row_names.len();
    let ncol = id_to_celltypes.len();

    // gene x annotation group
    let membership_ga = Mat::from_nonzero_triplets(nrow, ncol, &triplets)?;

    // remove potential ' '
    let annot_names = id_to_celltypes
        .into_iter()
        .map(|x| x.replace(" ", "_").into_boxed_str())
        .collect();

    Ok(AnnotInfo {
        membership_ga,
        annot_names,
    })
}

struct AnnotInfo {
    membership_ga: Mat,
    annot_names: Vec<Box<str>>,
}
