use crate::deconv::{BiSusieDeconv, SusieDeconv};
use crate::embed_common::*;
use crate::pseudobulk_topic::{compute_pseudobulk_by_topic, TopicWeightStrategy};
use matrix_util::common_io::*;

use candle_nn::AdamW;
use candle_nn::Optimizer;
use fnv::FnvHashMap as HashMap;
use fnv::FnvHashSet as HashSet;
use indicatif::{ProgressBar, ProgressDrawTarget};
use rand::SeedableRng;

/// Model type for topic annotation
#[derive(Clone, Copy, Debug, Default, ValueEnum)]
pub enum AnnotationModel {
    /// Standard SuSiE: sparse in annotation dimension only
    Susie,
    /// Bi-directional SuSiE: sparse in both annotation and topic dimensions (softmax)
    BiSusie,
    /// Bi-directional SuSiE with sparsemax for exact zeros (recommended)
    #[default]
    BiSusieSparsemax,
}

#[derive(Args, Debug)]
pub struct AnnotateTopicArgs {
    #[arg(
        short = 'g',
        long = "gene-dictionary",
        help = "A gene dictionary matrix file (generated by `topic`)",
        long_help = "A gene dictionary matrix file (generated by `topic`)\n\
		      The first column should correspond to features (genes).",
        required = true
    )]
    dict_file: Box<str>,

    #[arg(
        short = 'z',
        long = "latent-topic",
        help = "A latent topic proportion file (generated by `topic`)",
        long_help = "A latent topic proportion file (generated by `topic`)\n\
		      Values should be probabilities (on simplex).\n\
		      The first column should correspond to cell names.",
        required = true
    )]
    latent_file: Box<str>,

    #[arg(
        short = 'm',
        long = "marker-genes",
        help = "A marker feature file",
        long_help = "A marker feature file, e.g., mapping genes to cell types. \n\
		     Each line contains two names: \n\
		     (1) gene and \n\
		     (2) cell type \n\
		     (tab-separated or comma-separated). \n\
		     We allow space but will replace it with '_'.",
        required = true
    )]
    marker_file: Box<str>,

    #[arg(
        short = 'd',
        long = "data",
        help = "Expression data files (.h5 or .zarr)",
        long_help = "Optional expression data files for pseudobulk aggregation.\n\
		     When provided, computes pseudobulk by topic (weighted by topic proportions)\n\
		     instead of using the gene dictionary directly.\n\
		     Only marker genes are included in the pseudobulk."
    )]
    data_files: Option<Vec<Box<str>>>,

    #[arg(
        long,
        default_value_t = true,
        help = "Use hard assignment for pseudobulk (argmax instead of soft weights)",
        long_help = "When computing pseudobulk, use hard assignment (argmax) instead of soft weights.\n\
		     With soft weights, each cell's expression is spread across all topics proportionally.\n\
		     With hard assignment, each cell is assigned to its top topic only,\n\
		     which gives cleaner topic-specific expression patterns."
    )]
    pseudobulk_hard: bool,

    #[arg(
        long,
        short = 'i',
        default_value_t = 500,
        help = "Number of training epochs."
    )]
    epochs: usize,

    #[arg(
        short = 'c',
        long,
        default_value_t = 1e4,
        help = "Column sum normalization scale.",
        long_help = "Column sum normalization to scale the model parameters."
    )]
    column_sum_norm: f32,

    #[arg(
        short = 'l',
        long,
        default_value_t = 0.1,
        help = "Learning rate for optimization."
    )]
    learning_rate: f64,

    #[arg(
        long,
        short = 'L',
        default_value_t = 10,
        help = "Number of SuSiE components (L)."
    )]
    susie_components: usize,

    #[arg(
        long,
        short = 's',
        default_value_t = 30,
        help = "Number of Monte Carlo samples for SGVB."
    )]
    num_samples: usize,

    #[arg(
        long,
        value_delimiter = ',',
        default_values_t = vec![0.1, 1.0, 10.0],
        help = "Prior variance(s) for effect sizes (comma-separated for grid search).",
        long_help = "Prior variance(s) for effect sizes.\n\
		     If multiple values are provided (comma-separated), performs grid search\n\
		     and selects the best prior variance based on ELBO.\n\
		     Example: --prior-var 0.1,1.0,10.0"
    )]
    prior_var: Vec<f32>,

    #[arg(
        long,
        default_value_t = 10,
        help = "Number of epochs for KL annealing warm-up (0 to disable).",
        long_help = "Number of epochs over which to linearly anneal KL weight from 0 to 1.\n\
		     KL annealing (warm-up) helps prevent posterior collapse by letting the model\n\
		     focus on data fitting first, then gradually enforcing the prior.\n\
		     Set to 0 to disable annealing. A common choice is epochs/2."
    )]
    kl_anneal_epochs: usize,

    #[arg(
        long,
        default_value_t = 0.01,
        help = "Minimum max-PIP threshold for including a topic.",
        long_help = "Topics with max PIP below this threshold are excluded from cell annotation.\n\
		     This filters out topics that don't map to any known cell type.\n\
		     Set to 0 to include all topics."
    )]
    min_pip: f32,

    #[arg(
        long,
        short,
        required = true,
        help = "Output header",
        long_help = "Output header for results.\n\
		     Specify the output file or prefix for generated files."
    )]
    out: Box<str>,

    #[arg(
        long,
        short,
        help = "Verbosity.",
        long_help = "Enable verbose output.\n\
		     Prints additional information during execution."
    )]
    verbose: bool,

    #[arg(
        long,
        short = 'M',
        default_value = "bi-susie-sparsemax",
        help = "Model type for annotation.",
        long_help = "Model type for annotation:\n\
             - susie: Standard SuSiE, sparse in annotation dimension only\n\
             - bi-susie: Bi-directional SuSiE with softmax\n\
             - bi-susie-sparsemax: Bi-directional SuSiE with sparsemax (recommended)"
    )]
    model: AnnotationModel,
}

/// Training data bundle
struct TrainingData {
    /// Gene × topic data matrix (log dictionary or normalized pseudobulk)
    data: Mat,
    /// Gene × annotation membership matrix
    membership: Mat,
}

pub fn annotate_topics(args: &AnnotateTopicArgs) -> anyhow::Result<()> {
    if args.verbose {
        std::env::set_var("RUST_LOG", "info");
    }
    env_logger::init();

    // 1. read dictionary (log-probabilities) and latent states (probabilities)
    let MatWithNames {
        rows: row_names,
        cols: _topics,
        mat: log_dict_dk,
    } = read_mat(&args.dict_file)?;

    info!(
        "Read dictionary {} x {}",
        log_dict_dk.nrows(),
        log_dict_dk.ncols()
    );

    let MatWithNames {
        rows: cell_names,
        cols: topic_names,
        mat: topic_nt_raw,
    } = read_mat(&args.latent_file)?;

    // Convert log-probabilities to probabilities if needed
    let topic_nt = if topic_nt_raw.max() <= 0.0 {
        info!("Detected log-probabilities in latent file, converting to probabilities");
        topic_nt_raw.map(|x| x.exp())
    } else {
        topic_nt_raw
    };

    // 2. read marker gene annotation
    let AnnotInfo {
        membership_ga,
        annot_names,
    } = build_annotation_matrix(&args.marker_file, &row_names)?;

    let nnz_features = (0..membership_ga.nrows())
        .filter(|&i| membership_ga.row(i).iter().any(|&x| x > 0.0))
        .count();

    info!(
        "Found {} cell types matched over {} features",
        annot_names.len(),
        nnz_features
    );

    // 3. Prepare training data: either from pseudobulk or gene dictionary
    let (training_data, use_pseudobulk) = if let Some(data_files) = &args.data_files {
        // Compute pseudobulk by topic weighted by topic proportions
        let weight_strategy = if args.pseudobulk_hard {
            info!("Computing pseudobulk with hard assignment (argmax)...");
            TopicWeightStrategy::Hard
        } else {
            info!("Computing pseudobulk with soft weights...");
            TopicWeightStrategy::Soft
        };

        // Collect marker genes for filtering
        let marker_genes: HashSet<Box<str>> = read_marker_gene_info(&args.marker_file)?
            .keys()
            .cloned()
            .collect();

        let (pseudobulk_row_names, pseudobulk_gt) = compute_pseudobulk_by_topic(
            data_files,
            &cell_names,
            &topic_nt,
            Some(&marker_genes),
            weight_strategy,
        )?;

        // Build membership matrix on the pseudobulk rows
        let AnnotInfo {
            membership_ga: pb_membership,
            annot_names: pb_annot_names,
        } = build_annotation_matrix(&args.marker_file, &pseudobulk_row_names)?;

        // Verify annotation names match (they should since we used the same marker file)
        if pb_annot_names != annot_names {
            info!(
                "Warning: annotation names differ between dictionary and pseudobulk.\n\
                 Using pseudobulk annotations."
            );
        }

        // Normalize pseudobulk:
        // 1. log1p to stabilize expression values
        // 2. sum_to_one_rows: normalize each gene across topics (removes gene-level bias)
        // 3. sum_to_one_columns + log: convert to log-probabilities per topic
        let log_pseudobulk = pseudobulk_gt.map(|x| (1.0 + x).ln());
        let row_norm = log_pseudobulk.sum_to_one_rows();
        let normalized = row_norm.sum_to_one_columns().map(|x| x.max(1e-10).ln());

        info!(
            "Pseudobulk data: {} genes × {} topics (row+col normalized to log-prob)",
            normalized.nrows(),
            normalized.ncols()
        );

        // Store the membership matrix for later use
        (
            TrainingData {
                data: normalized,
                membership: pb_membership,
            },
            true,
        )
    } else {
        // Use gene dictionary directly
        (
            TrainingData {
                data: log_dict_dk.clone(),
                membership: membership_ga.clone(),
            },
            false,
        )
    };

    // 4. Build and train deconvolution model(s)
    // If multiple prior_var values provided, try each and select best by ELBO
    let dev = candle_core::Device::Cpu;
    let x_ga = training_data.membership.to_tensor(&dev)?;

    info!("Using {:?} model", args.model);

    let pip_mat = if args.prior_var.len() == 1 {
        // Single prior_var: train directly
        let prior_var = args.prior_var[0];
        let parameters = candle_nn::VarMap::new();
        let param_builder =
            candle_nn::VarBuilder::from_varmap(&parameters, candle_core::DType::F32, &dev);

        match args.model {
            AnnotationModel::Susie => {
                let model = SusieDeconv::new(
                    x_ga,
                    training_data.data.ncols(),
                    args.susie_components,
                    prior_var,
                    args.num_samples,
                    param_builder,
                )?;
                run_training(
                    &model,
                    &parameters,
                    args,
                    &training_data.data,
                    &dev,
                    use_pseudobulk,
                )?
                .0
            }
            AnnotationModel::BiSusie => {
                let model = BiSusieDeconv::new(
                    x_ga,
                    training_data.data.ncols(),
                    args.susie_components,
                    prior_var,
                    args.num_samples,
                    param_builder,
                )?;
                run_training(
                    &model,
                    &parameters,
                    args,
                    &training_data.data,
                    &dev,
                    use_pseudobulk,
                )?
                .0
            }
            AnnotationModel::BiSusieSparsemax => {
                let model = BiSusieDeconv::with_sparsemax(
                    x_ga,
                    training_data.data.ncols(),
                    args.susie_components,
                    prior_var,
                    args.num_samples,
                    param_builder,
                )?;
                run_training(
                    &model,
                    &parameters,
                    args,
                    &training_data.data,
                    &dev,
                    use_pseudobulk,
                )?
                .0
            }
        }
    } else {
        // Multiple prior_var values: model averaging with ELBO weights
        info!(
            "Model averaging over {} prior_var values: {:?}",
            args.prior_var.len(),
            args.prior_var
        );

        let mut pips: Vec<Mat> = Vec::new();
        let mut elbos: Vec<f32> = Vec::new();

        for &prior_var in &args.prior_var {
            info!("Training with prior_var = {}", prior_var);

            let parameters = candle_nn::VarMap::new();
            let param_builder =
                candle_nn::VarBuilder::from_varmap(&parameters, candle_core::DType::F32, &dev);

            let (pip, final_loss) = match args.model {
                AnnotationModel::Susie => {
                    let model = SusieDeconv::new(
                        x_ga.clone(),
                        training_data.data.ncols(),
                        args.susie_components,
                        prior_var,
                        args.num_samples,
                        param_builder,
                    )?;
                    run_training(
                        &model,
                        &parameters,
                        args,
                        &training_data.data,
                        &dev,
                        use_pseudobulk,
                    )?
                }
                AnnotationModel::BiSusie => {
                    let model = BiSusieDeconv::new(
                        x_ga.clone(),
                        training_data.data.ncols(),
                        args.susie_components,
                        prior_var,
                        args.num_samples,
                        param_builder,
                    )?;
                    run_training(
                        &model,
                        &parameters,
                        args,
                        &training_data.data,
                        &dev,
                        use_pseudobulk,
                    )?
                }
                AnnotationModel::BiSusieSparsemax => {
                    let model = BiSusieDeconv::with_sparsemax(
                        x_ga.clone(),
                        training_data.data.ncols(),
                        args.susie_components,
                        prior_var,
                        args.num_samples,
                        param_builder,
                    )?;
                    run_training(
                        &model,
                        &parameters,
                        args,
                        &training_data.data,
                        &dev,
                        use_pseudobulk,
                    )?
                }
            };

            let elbo = -final_loss; // loss is -ELBO
            info!("  prior_var = {} -> ELBO = {:.2}", prior_var, elbo);

            pips.push(pip);
            elbos.push(elbo);
        }

        // Compute softmax weights from ELBOs
        let max_elbo = elbos.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
        let exp_elbos: Vec<f32> = elbos.iter().map(|&e| (e - max_elbo).exp()).collect();
        let sum_exp: f32 = exp_elbos.iter().sum();
        let weights: Vec<f32> = exp_elbos.iter().map(|&e| e / sum_exp).collect();

        // Log weights
        for (&prior_var, &w) in args.prior_var.iter().zip(weights.iter()) {
            info!("  prior_var = {} -> weight = {:.3}", prior_var, w);
        }

        // Weighted average of PIPs
        let n_rows = pips[0].nrows();
        let n_cols = pips[0].ncols();
        let mut avg_pip = Mat::zeros(n_rows, n_cols);

        for (pip, &w) in pips.iter().zip(weights.iter()) {
            for i in 0..n_rows {
                for j in 0..n_cols {
                    avg_pip[(i, j)] += w * pip[(i, j)];
                }
            }
        }

        avg_pip
    };

    // 5. Output results
    let pip_file = format!("{}.pip.parquet", args.out);
    let cell_annot_file = format!("{}.annotation.parquet", args.out);

    pip_mat.to_parquet(Some(&annot_names), Some(&topic_names), &pip_file)?;

    // Filter topics by max PIP threshold
    let n_topics = pip_mat.ncols();
    let mut topic_mask = vec![true; n_topics];
    let mut excluded_topics = Vec::new();

    for t in 0..n_topics {
        let max_pip = (0..pip_mat.nrows())
            .map(|a| pip_mat[(a, t)])
            .fold(0.0f32, f32::max);
        if max_pip < args.min_pip {
            topic_mask[t] = false;
            excluded_topics.push((t, max_pip));
        }
    }

    let n_included = topic_mask.iter().filter(|&&x| x).count();

    if !excluded_topics.is_empty() {
        info!(
            "Excluding {} topics with max PIP < {}: {:?}",
            excluded_topics.len(),
            args.min_pip,
            excluded_topics
                .iter()
                .map(|(t, p)| format!("{}({:.3})", topic_names[*t], p))
                .collect::<Vec<_>>()
        );
    }

    // Check if any topics remain
    if n_included == 0 {
        log::warn!(
            "All {} topics were excluded (max PIP < {}). \
             Try lowering --min-pip threshold or adjusting --prior-var.",
            n_topics,
            args.min_pip
        );
        // Use uniform annotation as fallback
        let n_annots = annot_names.len();
        let uniform_prob = 1.0 / n_annots as f32;
        let topic_annot = Mat::from_fn(cell_names.len(), n_annots, |_, _| uniform_prob);
        topic_annot.to_parquet(Some(&cell_names), Some(&annot_names), &cell_annot_file)?;

        // Output argmax assignments (will be arbitrary due to uniform probs)
        let argmax_file = format!("{}.argmax.tsv", args.out);
        write_argmax_assignments(&topic_annot, &cell_names, &annot_names, &argmax_file)?;

        display_annotation_histogram(&topic_annot, &annot_names);
        return Ok(());
    }

    // Apply mask to PIP matrix (zero out excluded topics)
    let mut pip_filtered = pip_mat.clone();
    for t in 0..n_topics {
        if !topic_mask[t] {
            for a in 0..pip_filtered.nrows() {
                pip_filtered[(a, t)] = 0.0;
            }
        }
    }

    // Cell annotation = filtered_PIP × topic_proportions, then normalize
    let topic_tn = topic_nt.transpose().sum_to_one_columns();
    let topic_annot = (&pip_filtered * topic_tn).sum_to_one_columns().transpose();
    topic_annot.to_parquet(Some(&cell_names), Some(&annot_names), &cell_annot_file)?;

    // Output argmax assignments
    let argmax_file = format!("{}.argmax.tsv", args.out);
    write_argmax_assignments(&topic_annot, &cell_names, &annot_names, &argmax_file)?;

    // Show annotation summary histogram
    display_annotation_histogram(&topic_annot, &annot_names);

    Ok(())
}

/// Write argmax cell type assignment for each cell
fn write_argmax_assignments(
    annot: &Mat,
    cell_names: &[Box<str>],
    annot_names: &[Box<str>],
    output_file: &str,
) -> anyhow::Result<()> {
    use std::io::Write;
    let mut file = std::fs::File::create(output_file)?;
    writeln!(file, "cell\tcell_type\tprobability")?;

    for i in 0..annot.nrows() {
        let row = annot.row(i);
        let (max_idx, max_val) = row
            .iter()
            .enumerate()
            .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
            .unwrap();
        writeln!(
            file,
            "{}\t{}\t{:.4}",
            cell_names[i], annot_names[max_idx], max_val
        )?;
    }
    info!("Wrote argmax assignments to {}", output_file);
    Ok(())
}

/// Display histogram of cell type assignments
fn display_annotation_histogram(annot: &Mat, annot_names: &[Box<str>]) {
    let n_cells = annot.nrows();
    let n_types = annot.ncols();

    // Compute max probability and argmax per cell
    let mut max_probs = Vec::with_capacity(n_cells);
    let mut assignments = Vec::with_capacity(n_cells);

    for i in 0..n_cells {
        let row = annot.row(i);
        let (max_idx, max_val) = row
            .iter()
            .enumerate()
            .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
            .unwrap();
        max_probs.push(*max_val);
        assignments.push(max_idx);
    }

    // Count per cell type
    let mut type_counts = vec![0usize; n_types];
    let mut type_prob_sum = vec![0.0f32; n_types];
    for (i, &ct) in assignments.iter().enumerate() {
        type_counts[ct] += 1;
        type_prob_sum[ct] += max_probs[i];
    }

    // Sort by count descending
    let mut sorted_types: Vec<usize> = (0..n_types).collect();
    sorted_types.sort_by(|&a, &b| type_counts[b].cmp(&type_counts[a]));

    let max_count = *type_counts.iter().max().unwrap_or(&1);
    const MAX_BAR: usize = 20;

    // Statistics
    let mean_prob: f32 = max_probs.iter().sum::<f32>() / n_cells as f32;
    let above_50 = max_probs.iter().filter(|&&x| x > 0.5).count();
    let above_70 = max_probs.iter().filter(|&&x| x > 0.7).count();

    eprintln!();
    eprintln!("Annotation Summary ({} cells)", n_cells);
    eprintln!(
        "  Mean max-prob: {:.3}  >0.5: {} ({:.1}%)  >0.7: {} ({:.1}%)",
        mean_prob,
        above_50,
        100.0 * above_50 as f32 / n_cells as f32,
        above_70,
        100.0 * above_70 as f32 / n_cells as f32
    );
    eprintln!();

    for &ct in &sorted_types {
        if type_counts[ct] == 0 {
            continue;
        }
        let name = &annot_names[ct];
        let count = type_counts[ct];
        let pct = 100.0 * count as f32 / n_cells as f32;
        let mean_p = type_prob_sum[ct] / count as f32;
        let bar_len = ((count as f64 / max_count as f64) * MAX_BAR as f64).round() as usize;
        let bar = "#".repeat(bar_len.max(1));

        eprintln!(
            "  {:20} {:>6} {:>5.1}%  {:.3}  {}",
            &name[..name.len().min(20)],
            count,
            pct,
            mean_p,
            bar
        );
    }
    eprintln!();
}

/// Trait for deconvolution models
trait DeconvModel {
    fn loss(&self, y_gt: &candle_core::Tensor) -> candle_core::Result<candle_core::Tensor>;
    fn loss_with_kl_weight(
        &self,
        y_gt: &candle_core::Tensor,
        kl_weight: f32,
    ) -> candle_core::Result<candle_core::Tensor>;
    fn pip(&self) -> candle_core::Result<candle_core::Tensor>;
}

impl DeconvModel for SusieDeconv {
    fn loss(&self, y_gt: &candle_core::Tensor) -> candle_core::Result<candle_core::Tensor> {
        SusieDeconv::loss(self, y_gt)
    }
    fn loss_with_kl_weight(
        &self,
        y_gt: &candle_core::Tensor,
        kl_weight: f32,
    ) -> candle_core::Result<candle_core::Tensor> {
        SusieDeconv::loss_with_kl_weight(self, y_gt, kl_weight)
    }
    fn pip(&self) -> candle_core::Result<candle_core::Tensor> {
        SusieDeconv::pip(self)
    }
}

impl DeconvModel for BiSusieDeconv {
    fn loss(&self, y_gt: &candle_core::Tensor) -> candle_core::Result<candle_core::Tensor> {
        BiSusieDeconv::loss(self, y_gt)
    }
    fn loss_with_kl_weight(
        &self,
        y_gt: &candle_core::Tensor,
        kl_weight: f32,
    ) -> candle_core::Result<candle_core::Tensor> {
        BiSusieDeconv::loss_with_kl_weight(self, y_gt, kl_weight)
    }
    fn pip(&self) -> candle_core::Result<candle_core::Tensor> {
        BiSusieDeconv::pip(self)
    }
}

/// Returns (PIP matrix, average loss after annealing)
fn run_training<M: DeconvModel>(
    model: &M,
    parameters: &candle_nn::VarMap,
    args: &AnnotateTopicArgs,
    data_gt: &Mat,
    dev: &candle_core::Device,
    _use_pseudobulk: bool,
) -> anyhow::Result<(Mat, f32)> {
    use rand_distr::{Distribution, Poisson};

    let mut adam = AdamW::new_lr(parameters.all_vars(), args.learning_rate)?;

    let pb = ProgressBar::new(args.epochs as u64);

    if args.verbose {
        pb.set_draw_target(ProgressDrawTarget::hidden());
    }

    // KL annealing parameters
    let use_kl_annealing = args.kl_anneal_epochs > 0;
    let annealing_done_epoch = args.kl_anneal_epochs;

    // Track losses after annealing for averaging
    let mut post_anneal_losses: Vec<f32> = Vec::new();

    // Training loop
    for epoch in 0..args.epochs {
        // Sample Y[g,t] ~ Poisson(rate)
        let mut rng = rand::rngs::StdRng::seed_from_u64(epoch as u64 + 1);

        // Both pseudobulk and dictionary are now in log-probability space
        // Use the same scale for Poisson sampling
        let scale = args.column_sum_norm;

        let y_gt = data_gt
            .map(|x| {
                let rate = (x.exp() * scale).max(1e-10);
                Poisson::new(rate as f64).unwrap().sample(&mut rng) as f32
            })
            .to_tensor(dev)?;

        // Compute loss with optional KL annealing (warm-up: 0 → 1)
        let loss = if use_kl_annealing {
            let kl_weight = (epoch as f32 / args.kl_anneal_epochs as f32).min(1.0);
            model.loss_with_kl_weight(&y_gt, kl_weight)?
        } else {
            model.loss(&y_gt)?
        };

        adam.backward_step(&loss)?;
        let loss_val: f32 = loss.to_scalar()?;
        pb.inc(1);

        // Accumulate losses after annealing is complete
        if epoch >= annealing_done_epoch {
            post_anneal_losses.push(loss_val);
        }

        if args.verbose {
            if use_kl_annealing && epoch < args.kl_anneal_epochs {
                let kl_weight = (epoch as f32 / args.kl_anneal_epochs as f32).min(1.0);
                info!(
                    "[{}] loss={:.2} kl_weight={:.3}",
                    epoch, loss_val, kl_weight
                );
            } else {
                info!("[{}] loss={:.2}", epoch, loss_val);
            }
        }
    }

    pb.finish_and_clear();

    // Compute average loss after annealing (or all losses if no annealing)
    let avg_loss = if post_anneal_losses.is_empty() {
        0.0
    } else {
        post_anneal_losses.iter().sum::<f32>() / post_anneal_losses.len() as f32
    };

    // Get PIP (Posterior Inclusion Probabilities) - annotation x topic
    let pip = model.pip()?;
    Ok((Mat::from_tensor(&pip)?, avg_loss))
}

fn read_mat(file_path: &str) -> anyhow::Result<MatWithNames<Mat>> {
    Ok(match file_ext(file_path)?.as_ref() {
        "parquet" => Mat::from_parquet(file_path)?,
        _ => Mat::read_data(file_path, &['\t', ','], None, Some(0), None, None)?,
    })
}

fn read_marker_gene_info(file_path: &str) -> anyhow::Result<HashMap<Box<str>, Box<str>>> {
    let ReadLinesOut { lines, header: _ } =
        read_lines_of_words_delim(&file_path, &['\t', ','], -1)?;

    Ok(lines
        .into_iter()
        .filter_map(|words| {
            if words.len() < 2 {
                None
            } else {
                Some((words[0].clone(), words[1].clone()))
            }
        })
        .collect())
}

fn build_annotation_matrix(
    marker_gene_path: &str,
    row_names: &[Box<str>],
) -> anyhow::Result<AnnotInfo> {
    let gene_to_type = read_marker_gene_info(marker_gene_path)?;

    if gene_to_type.is_empty() {
        return Err(anyhow::anyhow!("empty/invalid marker gene information"));
    }

    let mut row_to_type_vec = vec![];

    for (ri, rn) in row_names.iter().enumerate() {
        let rn_lower = rn.to_lowercase();

        // Check each marker gene
        for (marker_gene, cell_type) in gene_to_type.iter() {
            let marker_lower = marker_gene.to_lowercase();

            // 1. Exact match
            if rn_lower == marker_lower {
                row_to_type_vec.push((ri, cell_type));
                continue;
            }

            // 2. Match at word boundary: row ends with "_MARKER" or "MARKER_"
            let suffix_match = rn_lower.ends_with(&format!("_{}", marker_lower));
            let prefix_match = rn_lower.starts_with(&format!("{}_", marker_lower));

            // 3. Match marker as a complete segment between underscores
            let segment_match = rn_lower.contains(&format!("_{}_", marker_lower));

            if suffix_match || prefix_match || segment_match {
                row_to_type_vec.push((ri, cell_type));
            }
        }
    }

    row_to_type_vec.sort();
    row_to_type_vec.dedup();

    let mut id_to_celltypes: Vec<Box<str>> = row_to_type_vec
        .iter()
        .map(|&(_, t)| t.clone())
        .collect::<HashSet<_>>()
        .into_iter()
        .collect();

    id_to_celltypes.sort();

    let celltype_to_id: HashMap<Box<str>, usize> = id_to_celltypes
        .iter()
        .enumerate()
        .map(|(i, x)| (x.clone(), i))
        .collect();

    let triplets = row_to_type_vec
        .iter()
        .filter_map(|&(r, ct)| {
            if let Some(t) = celltype_to_id.get(ct) {
                Some((r, *t, 1.0))
            } else {
                None
            }
        })
        .collect::<Vec<_>>();

    let nrow = row_names.len();
    let ncol = id_to_celltypes.len();

    // gene x annotation group
    let membership_ga = Mat::from_nonzero_triplets(nrow, ncol, &triplets)?;

    // remove potential ' '
    let annot_names = id_to_celltypes
        .into_iter()
        .map(|x| x.replace(" ", "_").into_boxed_str())
        .collect();

    Ok(AnnotInfo {
        membership_ga,
        annot_names,
    })
}

struct AnnotInfo {
    membership_ga: Mat,
    annot_names: Vec<Box<str>>,
}
