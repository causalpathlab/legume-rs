mod misc;
mod qc;
mod simulate;
mod simulate_deconv;
mod sparse_data_visitors;
mod sparse_io;
mod sparse_io_vector;
mod sparse_matrix_hdf5;
mod sparse_matrix_zarr;
mod sparse_util;

use crate::misc::*;
use crate::qc::*;
use crate::sparse_io::*;
use crate::sparse_io_vector::*;
use crate::sparse_util::*;

use clap::{ArgAction, Args, Parser, Subcommand, ValueEnum};
use common_io::*;
use data_beans::sparse_data_visitors::create_jobs;
use indicatif::ParallelProgressIterator;
use indicatif::{ProgressBar, ProgressDrawTarget};
use log::info;
use matrix_util::traits::IoOps;
use matrix_util::*;
use rayon::prelude::*;
use simulate_deconv::generate_convoluted_data;
use simulate_deconv::SimConvArgs;
use tempfile::TempDir;

use fnv::FnvHashMap as HashMap;
use std::sync::Arc;

fn main() -> anyhow::Result<()> {
    env_logger::init();

    let cli = Cli::parse();

    match &cli.commands {
        Commands::FromMtx(args) => {
            run_build_from_mtx(args)?;
        }
        Commands::ListH5(args) => {
            list_h5(args)?;
        }
        Commands::ListZarr(args) => {
            list_zarr(args)?;
        }
        Commands::FromZarr(args) => {
            run_build_from_zarr_triplets(args)?;
        }
        Commands::FromH5ad(args) => {
            run_build_from_h5_triplets(args)?;
        }
        Commands::Simulate(args) => {
            run_simulate(args)?;
        }
        Commands::SimulateConv(args) => {
            generate_convoluted_data(args)?;
        }
        Commands::Info(args) => {
            show_info(args)?;
        }
        Commands::Statistics(args) => {
            run_stat(args)?;
        }
        Commands::Squeeze(args) => {
            run_squeeze(args)?;
        }
        Commands::Columns(args) => {
            take_columns(args)?;
        }
        Commands::Rows(args) => {
            take_rows(args)?;
        }
        Commands::ColumnNames(args) => {
            take_column_names(args)?;
        }
        Commands::RowNames(args) => {
            take_row_names(args)?;
        }
        Commands::SubsetColumns(args) => {
            subset_columns(args)?;
        }
        Commands::AlignData(args) => {
            align_backends(args)?;
        }
        Commands::ReorderRows(args) => {
            reorder_rows(args)?;
        }
        Commands::MergeMtx(args) => {
            run_merge_mtx(args)?;
        }
        Commands::MergeBackend(args) => {
            run_merge_backend(args)?;
        }
        Commands::Merge(args) => {
            run_merge_backend(args)?;
        }
    }

    Ok(())
}

#[derive(Parser, Debug)]
#[command(
    version,
    about = "Basic utility functions for processing a sparse matrix.",
    long_about = "Basic utility functions for processing a sparse matrix.

We assume non-negative sparse matrices were generated by single-cell omics (feature x cell).
This tool creates a data structure for faster access, organized as follows:

    (root)
        ├── nrow
        ├── ncell
        ├── by_column
        │   ├── data
        │   ├── indices (row indices)
        │   └── indptr (column pointers)
        └── by_row
            ├── data
            ├── indices (column indices)
            └── indptr (row pointers)

For more details on each command, use '--help' after the command name."
)]
struct Cli {
    #[command(subcommand)]
    commands: Commands,
}

#[derive(Subcommand, Debug)]
enum Commands {
    #[command(about = "Build backend from `mtx` file and associated `tsv` files")]
    FromMtx(FromMtxArgs),

    #[command(
        about = "Build backend from triplets in `h5` (AnnData)",
        visible_alias = "from-h5"
    )]
    FromH5ad(FromH5Args),

    #[command(
        about = "Build backend from triplets in 10X Xenium `zarr`",
        long_about = "Build a backend from triplets in `zarr` format.\n\
		      Supports conversion and indexing for fast access."
    )]
    FromZarr(FromZarrArgs),

    #[command(
        about = "List contents of `h5` file",
        long_about = "List what are included in the `h5` file.\n\
		      Shows datasets, groups, and metadata."
    )]
    ListH5(ListH5Args),

    #[command(
        about = "List contents of `zarr` file",
        long_about = "List what are included in the `zarr` file.\n\
		      Displays structure and available arrays."
    )]
    ListZarr(ListZarrArgs),

    #[command(
        about = "Sort rows by name order",
        long_about = "Sort rows according to the order of row names specified in a row name file.\n\
		      Useful for aligning datasets and ensuring consistent row order."
    )]
    ReorderRows(ReorderRowsArgs),

    #[command(
        about = "Take columns and output dense matrix",
        long_about = "Take columns from the sparse matrix and save them to an `output` file as a dense matrix for quick examination.\n\
		      Useful for extracting subsets for visualization or analysis."
    )]
    Columns(TakeColumnsArgs),

    #[command(
        about = "Take rows and output dense matrix (transposed)",
        long_about = "Take rows from the sparse matrix and save them to an `output` file as a dense matrix for quick examination.\n\
		      For convenience, it will output a transposed (`column x selected_row`) matrix."
    )]
    Rows(TakeRowsArgs),

    #[command(
        about = "List column names",
        long_about = "List all column names in the backend.\n\
		      Useful for inspecting available features."
    )]
    ColumnNames(TakeColumnNamesArgs),

    #[command(
        about = "List row names",
        long_about = "List all row names in the backend.\n\
		      Useful for inspecting available samples or observations."
    )]
    RowNames(TakeRowNamesArgs),

    #[command(
        about = "Subset columns and create new backend",
        long_about = "Take columns from the sparse matrix and create a new sparse matrix backend.\n\
		      Allows for focused analysis on selected features."
    )]
    SubsetColumns(SubsetColumnsArgs),

    #[command(
        about = "Align data backends",
        long_about = "To ensure that column names are aligned for multimodal analysis.\n\
		      We will only keep columns and rows matched across files.\n\
		      1st row: `D(1,1)-D(1,2)`, 2nd row: `D(2,1)-D(2,2)`, etc.",
        visible_alias = "align"
    )]
    AlignData(AlignDataArgs),

    #[command(
        about = "Merge multiple `.mtx` files",
        long_about = "Merge multiple 10x `.mtx` files into one fileset.\n\
		      Useful for combining datasets from different sources."
    )]
    MergeMtx(MergeMtxArgs),

    #[command(
        about = "Merge multiple backend files",
        long_about = "Merge multiple backend file(sets) into one.\n\
		      Supports various formats and options for merging."
    )]
    MergeBackend(MergeBackendArgs),

    #[command(
        about = "Alias for merge-backend",
        long_about = "An alias of `merge-backend`.\n\
		      Performs the same operation as `merge-backend`."
    )]
    Merge(MergeBackendArgs),

    #[command(
        about = "Squeeze out sparse rows/columns",
        long_about = "Squeeze out rows and columns with too few non-zeros.\n\
		      It will overwrite the original (be careful) and save the indices kept."
    )]
    Squeeze(RunSqueezeArgs),

    #[command(
        about = "Show basic matrix info",
        long_about = "Show basic information of a sparse matrix.\n\
		      If output header is provided, row and column names will be saved."
    )]
    Info(InfoArgs),

    #[command(
        about = "Take matrix statistics",
        long_about = "Take basic statistics from a sparse matrix.\n\
		      The output file will contain columns of \n\
		      (1) `nnz` - number of non-zero elements, \n\
		      (2) `tot` - total sum, \n\
		      (3) `mu` - average `μ`, \n\
		      (4) `sig` - standard deviation `σ`.",
        visible_alias = "stat"
    )]
    Statistics(RunStatArgs),

    #[command(
        about = "Simulate matrix with Gamma topic",
        long_about = "`Y(i,j) ~ δ(i,B(j)) Σ β(i,k) θ(j,k)` with β,θ ~ Gamma topic;\n\
		      B(j)`=batch; `ln δ`~N(0,1)"
    )]
    Simulate(RunSimulateArgs),

    #[command(about = "Simulate convoluted data matrix (experimental)")]
    SimulateConv(SimConvArgs),
}

#[derive(Args, Debug)]
pub struct ReorderRowsArgs {
    /// Data file -- either `.zarr` or `.h5`
    data_file: Box<str>,

    /// Row/feature name file (name per each line; `.tsv.gz` or `.tsv`)
    #[arg(short, long, required = true)]
    row_file: Box<str>,

    /// output header
    #[arg(short, long, required = true)]
    output: Box<str>,
}

#[derive(Args, Debug)]
pub struct TakeColumnsArgs {
    /// data file -- either `.zarr` or `.h5`
    data_file: Box<str>,

    /// column indices to take: e.g., `0,1,2,3`
    #[arg(short = 'i', long, value_delimiter = ',')]
    column_indices: Option<Vec<usize>>,

    /// column name file where each line is a column name
    #[arg(short = 'f', long)]
    name_file: Option<Box<str>>,

    /// column names to take: e.g., `col1,col2,col3` (supports substring matching)
    #[arg(short = 'n', long, value_delimiter = ',')]
    column_names: Option<Vec<Box<str>>>,

    /// output `parquet` file
    #[arg(short, long, default_value = "stdout")]
    output: Box<str>,
}

#[derive(Args, Debug)]
pub struct TakeRowsArgs {
    /// data file -- either `.zarr` or `.h5`
    data_file: Box<str>,

    /// row indices to take: e.g., `0,1,2,3`
    #[arg(short = 'i', long, value_delimiter = ',')]
    row_indices: Option<Vec<usize>>,

    /// row name file where each line is a row name
    #[arg(short = 'f', long)]
    name_file: Option<Box<str>>,

    /// row names to take: e.g., `gene1,gene2,gene3` (supports substring matching)
    #[arg(short = 'n', long, value_delimiter = ',')]
    row_names: Option<Vec<Box<str>>>,

    /// output `parquet` file
    #[arg(short, long, default_value = "stdout")]
    output: Box<str>,
}

#[derive(Args, Debug)]
pub struct TakeColumnNamesArgs {
    /// data file -- either `.zarr` or `.h5`
    data_file: Box<str>,

    /// output file
    #[arg(short, long, default_value = "stdout")]
    output: Box<str>,
}

#[derive(Args, Debug)]
pub struct TakeRowNamesArgs {
    /// data file -- either `.zarr` or `.h5`
    data_file: Box<str>,

    /// output file
    #[arg(short, long, default_value = "stdout")]
    output: Box<str>,
}

#[derive(Args, Debug)]
pub struct SubsetColumnsArgs {
    /// data file -- either `.zarr` or `.h5`
    data_file: Box<str>,

    /// column indices to take: e.g., `0,1,2,3`
    #[arg(short = 'i', long, value_delimiter = ',')]
    column_indices: Option<Vec<usize>>,

    /// column name file where each line is a column name
    #[arg(short = 'f', long)]
    name_file: Option<Box<str>>,

    /// squeeze
    #[arg(long, default_value_t = false)]
    do_squeeze: bool,

    /// minimum number of non-zero cutoff for rows
    #[arg(long, default_value_t = 1)]
    row_nnz_cutoff: usize,

    /// minimum number of non-zero cutoff for columns
    #[arg(long, default_value_t = 1)]
    column_nnz_cutoff: usize,

    /// output file
    #[arg(short, long, required = true)]
    output: Box<str>,
}

#[derive(Args, Debug)]
pub struct AlignDataArgs {
    /// data file -- either `.zarr` or `.h5`
    #[arg(required = true)]
    data_files: Vec<Box<str>>,

    /// Data types (treating them as different rows)
    #[arg(short = 'r', long, required = true)]
    num_data_types: usize,

    /// output directory
    #[arg(short, long, required = true)]
    output_directory: Box<str>,
}

#[derive(Args, Debug)]
pub struct FromMtxArgs {
    /// matrix market-formatted data file (`.mtx.gz` or `.mtx`)
    mtx: Box<str>,

    /// row/feature name file (name per each line; `.tsv.gz` or `.tsv`)
    #[arg(short, long)]
    row: Option<Box<str>>,

    /// column/cell/barcode file (name per each line; `.tsv.gz` or `.tsv`)
    #[arg(short, long)]
    col: Option<Box<str>>,

    /// backend for the output file
    #[arg(long, value_enum, default_value = "zarr")]
    backend: SparseIoBackend,

    /// output file header: {output}.{backend}
    #[arg(short, long)]
    output: Box<str>,

    /// squeeze
    #[arg(long, default_value_t = false)]
    do_squeeze: bool,

    /// minimum number of non-zero cutoff for rows
    #[arg(long, default_value_t = 1)]
    row_nnz_cutoff: usize,

    /// minimum number of non-zero cutoff for columns
    #[arg(long, default_value_t = 1)]
    column_nnz_cutoff: usize,

    /// verbose mode
    #[arg(short, long, action = ArgAction::Count)]
    verbose: u8,
}

#[derive(Args, Debug)]
pub struct ListH5Args {
    h5_file: Box<str>,
}

#[derive(Args, Debug)]
pub struct ListZarrArgs {
    zarr_file: Box<str>,
}

#[derive(Args, Debug)]
pub struct FromH5Args {
    #[arg(
        help = "Input HDF5 file containing sparse matrix triplets",
        long_help = "Specify the HDF5 file where triplets of sparse matrix data are stored. \n\
		     Supports 10X Genomics and H5AD formats."
    )]
    h5_file: Box<str>,

    #[arg(
        long,
        value_enum,
        default_value = "zarr",
        help = "Backend format for output",
        long_help = "Choose the backend format for the output file. \n\
		     Supported formats include 'zarr' and 'h5'"
    )]
    backend: SparseIoBackend,

    #[arg(
        short,
        long,
        help = "Output file header or name",
        long_help = "Specify the output file header. \n\
		     The output will be named as {output}.{backend}.\n\
		     Redundant {backend} names will be ignored."
    )]
    output: Box<str>,

    #[arg(
        short = 'x',
        long,
        default_value = "matrix",
        help = "Root group name for sparse data triplets",
        long_help = "Set the root group name under which sparse data triplets are stored in the HDF5 file. \n\
		     Use the 'list-h5' command to inspect available groups."
    )]
    root_group_name: Box<str>,

    #[arg(
        short = 'd',
        long,
        default_value = "data",
        help = "Data field name",
        long_help = "Name of the dataset containing triplet values X(i,j) under the root group."
    )]
    data_field: Box<str>,

    #[arg(
        short = 'i',
        long,
        default_value = "indices",
        help = "Indices field name",
        long_help = "Name of the dataset containing indices. \n\
		     Row indices for CSC, column indices for CSR, under the root group."
    )]
    indices_field: Box<str>,

    #[arg(
        short = 'p',
        long,
        default_value = "indptr",
        help = "Indptr field name",
        long_help = "Name of the dataset containing indptr. \n\
		     Column pointers for CSC, row pointers for CSR, under the root group."
    )]
    indptr_field: Box<str>,

    #[arg(
        short = 't',
        long,
        value_enum,
        default_value = "column",
        help = "Pointer type (row or column)",
        long_help = "Specify whether the pointers are for row (gene) or column (cell) indices."
    )]
    pointer_type: IndexPointerType,

    #[arg(
        short = 'r',
        long,
        default_value = "features/id",
        help = "Row ID field name",
        long_help = "Group or dataset name for row, gene, or feature IDs under the root group."
    )]
    row_id_field: Box<str>,

    #[arg(
        short = 'n',
        long,
        default_value = "features/name",
        help = "Row name field name",
        long_help = "Group or dataset name for row, gene, or feature names under the root group."
    )]
    row_name_field: Box<str>,

    #[arg(
        short = 'f',
        long,
        default_value = "features/feature_type",
        help = "Row type field name",
        long_help = "Group or dataset name for row, gene, or feature types under the root group."
    )]
    row_type_field: Box<str>,

    #[arg(
        long,
        default_value = "gene",
        help = "Select row type",
        long_help = "Select row type for inclusion. Rows are included if their type contains this value."
    )]
    select_row_type: Box<str>,

    #[arg(
        long,
        default_value = "aggregate",
        help = "Remove row type",
        long_help = "Remove rows if their type contains this value."
    )]
    remove_row_type: Box<str>,

    #[arg(
        short = 'c',
        long,
        default_value = "barcodes",
        help = "Column name field",
        long_help = "Group or dataset name for columns or cells under the root group."
    )]
    column_name_field: Box<str>,

    #[arg(
        long,
        default_value_t = false,
        help = "Squeeze sparse rows or columns",
        long_help = "Enable squeezing to remove rows and columns with too few non-zeros. \n\
		     This can help reduce file size and improve performance."
    )]
    do_squeeze: bool,

    #[arg(
        long,
        default_value_t = 1,
        help = "Row non-zero cutoff",
        long_help = "Minimum number of non-zero elements required for rows. \n\
		     Rows with fewer non-zeros will be removed if squeezing is enabled."
    )]
    row_nnz_cutoff: usize,

    #[arg(
        long,
        default_value_t = 1,
        help = "Column non-zero cutoff",
        long_help = "Minimum number of non-zero elements required for columns. \n\
		     Columns with fewer non-zeros will be removed if squeezing is enabled."
    )]
    column_nnz_cutoff: usize,

    #[arg(
        short,
        long,
        action = clap::ArgAction::Count,
        help = "Verbose mode",
        long_help = "Enable verbose mode for more detailed output. \n\
		     Use multiple times for increased verbosity."
    )]
    verbose: u8,
}

#[derive(clap::Args, Debug)]
pub struct FromZarrArgs {
    #[arg(
        help = "Input Zarr file containing sparse matrix triplets",
        long_help = "Specify the Zarr file where triplets of sparse matrix data are stored. \n\
		     For example, 10X Genomics Xenium's 'cell_feature_matrix.zarr.zip'."
    )]
    zarr_file: Box<str>,

    #[arg(
        long,
        value_enum,
        default_value = "zarr",
        help = "Backend format for output",
        long_help = "Choose the backend format for the output file. \n\
		     Supported formats include 'zarr' and 'h5'."
    )]
    backend: SparseIoBackend,

    #[arg(
        short,
        long,
        help = "Output file header or name",
        long_help = "Specify the output file header. \n\
		     The output will be named as {output}.{backend}.\n\
		     Redundant {backend} names will be ignored."
    )]
    output: Box<str>,

    #[arg(
        short = 'd',
        long,
        default_value = "/cell_features/data",
        help = "Data field path",
        long_help = "Path to the dataset containing triplet values. \n\
		     Use the 'list-zarr' subcommand to inspect available fields."
    )]
    data_field: Box<str>,

    #[arg(
        short = 'i',
        long,
        default_value = "/cell_features/indices",
        help = "Indices field path",
        long_help = "Path to the dataset containing indices. \n\
		     Row indices for CSC, column indices for CSR."
    )]
    indices_field: Box<str>,

    #[arg(
        short = 'p',
        long,
        default_value = "/cell_features/indptr",
        help = "Indptr field path",
        long_help = "Path to the dataset containing indptr. \n\
		     Column pointers for CSC, row pointers for CSR."
    )]
    indptr_field: Box<str>,

    #[arg(
        short = 't',
        long,
        value_enum,
        default_value = "row",
        help = "Pointer type (row or column)",
        long_help = "Specify whether the pointers keep track of row (gene) or column (cell) indices."
    )]
    pointer_type: IndexPointerType,

    #[arg(
        short = 'r',
        long,
        default_value = "/cell_features/feature_ids",
        help = "Row ID field path",
        long_help = "Path to the group or dataset for row, gene, or feature IDs."
    )]
    row_id_field: Box<str>,

    #[arg(
        short = 'n',
        long,
        default_value = "/cell_features/feature_keys",
        help = "Row name field path",
        long_help = "Path to the group or dataset for row, gene, or feature names."
    )]
    row_name_field: Box<str>,

    #[arg(
        short = 'f',
        long,
        default_value = "/cell_features/feature_types",
        help = "Row type field path",
        long_help = "Path to the group or dataset for row, gene, or feature types."
    )]
    row_type_field: Box<str>,

    #[arg(
        long,
        default_value = "gene",
        help = "Select row type",
        long_help = "Select row type for inclusion. Rows are included if their type contains this value."
    )]
    select_row_type: Box<str>,

    #[arg(
        long,
        default_value = "aggregate",
        help = "Remove row type",
        long_help = "Remove rows if their type contains this value."
    )]
    remove_row_type: Box<str>,

    #[arg(
        short = 'c',
        long,
        default_value = "/cell_features/cell_id",
        help = "Column name field path",
        long_help = "Path to the group or dataset for columns or cells. \n\
		     Will first attempt Xenium's Cell ID format mapping."
    )]
    column_name_field: Box<str>,

    #[arg(
        long,
        default_value_t = false,
        help = "Squeeze sparse rows or columns",
        long_help = "Enable squeezing to remove rows and columns with too few non-zeros. \n\
		     This can help reduce file size and improve performance."
    )]
    do_squeeze: bool,

    #[arg(
        long,
        default_value_t = 1,
        help = "Row non-zero cutoff",
        long_help = "Minimum number of non-zero elements required for rows. \n\
		     Rows with fewer non-zeros will be removed if squeezing is enabled."
    )]
    row_nnz_cutoff: usize,

    #[arg(
        long,
        default_value_t = 1,
        help = "Column non-zero cutoff",
        long_help = "Minimum number of non-zero elements required for columns. \n\
		     Columns with fewer non-zeros will be removed if squeezing is enabled."
    )]
    column_nnz_cutoff: usize,

    #[arg(
        short,
        long,
        action = clap::ArgAction::Count,
        help = "Verbose mode",
        long_help = "Enable verbose mode for more detailed output. \n\
		     Use multiple times for increased verbosity."
    )]
    verbose: u8,
}

#[derive(Args, Debug)]
pub struct MergeBackendArgs {
    #[arg(
        help = "Input data files",
        long_help = "Data files to be merged into a single backend. \n\
		     Provide one or more files in supported formats."
    )]
    data_files: Vec<Box<str>>,

    #[arg(
        long,
        value_enum,
        default_value = "zarr",
        help = "Backend format",
        long_help = "Specify the backend format to use for the merged data. \n\
		     Supported formats include 'zarr', 'h5', etc."
    )]
    backend: SparseIoBackend,

    #[arg(
        short,
        long,
        required = true,
        help = "Output file header",
        long_help = "Output file header: {output}.{backend} and {output}.batch.gz. \n\
		     The backend will contain everything. \n\
		     Batch assignment information will be saved in a separate file \n\
		     and is needed for embedding steps later."
    )]
    output: Box<str>,

    #[arg(
        long,
        default_value_t = false,
        help = "Squeeze sparse rows/columns",
        long_help = "Enable squeezing to remove rows and columns with too few non-zeros. \n\
		     This can help reduce file size and improve performance."
    )]
    do_squeeze: bool,

    #[arg(
        long,
        default_value_t = 1,
        help = "Row non-zero cutoff",
        long_help = "Minimum number of non-zero elements required for rows. \n\
		     Rows with fewer non-zeros will be removed if squeezing is enabled."
    )]
    row_nnz_cutoff: usize,

    #[arg(
        long,
        default_value_t = 1,
        help = "Column non-zero cutoff",
        long_help = "Minimum number of non-zero elements required for columns. \n\
		     Columns with fewer non-zeros will be removed if squeezing is enabled."
    )]
    column_nnz_cutoff: usize,

    #[arg(
        long,
        default_value = "100",
        help = "Block size for parallel processing",
        long_help = "Block size for parallel processing. \n\
		     Adjust this value to optimize performance for your hardware."
    )]
    block_size: usize,

    #[arg(
        short,
        long,
        action = clap::ArgAction::Count,
        help = "Verbose mode",
        long_help = "Enable verbose mode for more detailed output. \n\
		     Use multiple times for increased verbosity."
    )]
    verbose: u8,
}

#[derive(clap::Args, Debug)]
pub struct MergeMtxArgs {
    #[arg(
        value_delimiter = ',',
        required = true,
        help = "Input data directories",
        long_help = "Within each directory and its sub-directories, \n\
		     the program will search for files named as specified by \n\
                     (1) `mtx_file_name`, \n\
		     (2) `feature_file_name`, \n\
		     and (3) `barcode_file_name` \n\
		     to merge into one backend file."
    )]
    data_directories: Vec<Box<str>>,

    #[arg(
        long,
        value_enum,
        default_value = "zarr",
        help = "Backend format",
        long_help = "Specify the backend format for the merged data. \n\
                     Supported formats include 'zarr', 'h5', etc."
    )]
    backend: SparseIoBackend,

    #[arg(
        short,
        long,
        required = true,
        help = "Output file header",
        long_help = "Output file header: {output}.{backend} and {output}.batch.gz. \n\
                     The backend will contain all merged data. \n\
                     Batch assignment information will be saved in a separate file and \n\
		     is needed for embedding steps later."
    )]
    output: Box<str>,

    #[arg(
        short,
        long,
        default_value = "matrix.mtx",
        help = "Matrix file name",
        long_help = "Name of the matrix file to search for in each directory. \n\
                     The default for 10x data is 'matrix.mtx'."
    )]
    mtx_file_name: Box<str>,

    #[arg(
        short,
        long,
        default_value = "genes.tsv.gz",
        help = "Feature/row file name",
        long_help = "Name of the feature (row) file to search for in each directory. \n\
                     The default is 'genes.tsv.gz'."
    )]
    feature_file_name: Box<str>,

    #[arg(
        long,
        default_value_t = 2,
        help = "Number of words for feature names",
        long_help = "Number of words to use when parsing feature names from the feature file. \n\
                     Adjust this to match your data format."
    )]
    num_feature_name_words: usize,

    #[arg(
        short,
        long,
        default_value = "barcodes.tsv.gz",
        help = "Barcode/column file name",
        long_help = "Name of the barcode (column) file to search for in each directory. \n\
                     The default is 'barcodes.tsv.gz'."
    )]
    barcode_file_name: Box<str>,

    #[arg(
        long,
        default_value_t = 5,
        help = "Number of words for barcode names",
        long_help = "Number of words to use when parsing barcode names from the barcode file. \n\
                     Adjust this to match your data format."
    )]
    num_barcode_name_words: usize,

    #[arg(
        long,
        default_value_t = false,
        help = "Squeeze sparse rows/columns",
        long_help = "Enable squeezing to remove rows and columns with too few non-zeros. \n\
                     This can help reduce file size and improve performance."
    )]
    do_squeeze: bool,

    #[arg(
        long,
        default_value_t = 1,
        help = "Row non-zero cutoff",
        long_help = "Minimum number of non-zero elements required for rows. \n\
                     Rows with fewer non-zeros will be removed if squeezing is enabled."
    )]
    row_nnz_cutoff: usize,

    #[arg(
        long,
        default_value_t = 1,
        help = "Column non-zero cutoff",
        long_help = "Minimum number of non-zero elements required for columns. \n\
                     Columns with fewer non-zeros will be removed if squeezing is enabled."
    )]
    column_nnz_cutoff: usize,

    #[arg(
        short,
        long,
        action = clap::ArgAction::Count,
        help = "Verbose mode",
        long_help = "Enable verbose mode for more detailed output. \n\
                     Use multiple times for increased verbosity."
    )]
    verbose: u8,
}

#[derive(Args, Debug)]
#[command(about)]
pub struct RunSqueezeArgs {
    /// data file -- either `.zarr` or `.h5`
    data_file: Box<str>,

    /// number of non-zero cutoff for rows
    #[arg(short, long, default_value = "0")]
    row_nnz_cutoff: usize,

    /// number of non-zero cutoff for columns
    #[arg(short, long, default_value = "0")]
    column_nnz_cutoff: usize,

    /// block_size for parallel processing
    #[arg(long, default_value = "100")]
    block_size: usize,
}

#[derive(ValueEnum, Clone, Debug, PartialEq)]
#[clap(rename_all = "lowercase")]
enum StatDim {
    Row,
    Column,
}

#[derive(Args, Debug)]
pub struct RunStatArgs {
    #[arg(
        required = true,
        help = "Input data files in '.zarr' or '.h5' format",
        long_help = "Provide data files in either '.zarr' or '.h5' format. \n\
		     You can convert '.mtx' files to '.zarr' or '.h5' using\n\
		     the 'data-beans from-mtx' command."
    )]
    data_files: Vec<Box<str>>,

    #[arg(
        short,
        long,
        value_enum,
        help = "Statistics dimension (row or column)",
        long_help = "Choose whether to compute statistics over rows or columns."
    )]
    stat_dim: StatDim,

    #[arg(
        short,
        long,
        help = "Row name pattern for column statistics",
        long_help = "Specify a pattern to select row names \n\
		     when accumulating statistics over columns.\n\
		     Only rows matching this pattern will be included."
    )]
    row_name_pattern: Option<Box<str>>,

    #[arg(
        short = 'g',
        long,
        help = "Column group membership file for row statistics",
        long_help = "Provide a file that defines column group membership \n\
		     when accumulating statistics over rows. \n\
		     This provides statistics computed for group-wise analysis."
    )]
    column_group_file: Option<Box<str>>,

    #[arg(
        long,
        value_enum,
        default_value = "100",
        help = "Block size for processing",
        long_help = "Set the block size for processing data. \n\
		     Adjust this value to optimize performance for your hardware."
    )]
    block_size: usize,

    #[arg(
        short,
        long,
        default_value = "stdout",
        help = "Output statistics file",
        long_help = "Specify the output file for statistics. \n\
		     You can provide a '.parquet' file for efficient storage, \n\
		     or use 'stdout' to print results to the console."
    )]
    output: Box<str>,
}

/// A quick information of the underlying matrix of a backend file.
#[derive(Args, Debug)]
pub struct InfoArgs {
    /// data file -- .zarr or .h5 file
    data_file: Box<str>,

    /// file header for {output}.{rows.gz,columns.gz}
    #[arg(short, long, default_value = "")]
    output: Box<str>,
}

#[derive(clap::Args, Debug)]
pub struct RunSimulateArgs {
    #[arg(
        short,
        long,
        help = "Number of rows, genes, or features",
        long_help = "Set the number of rows, which typically corresponds to \n\
		     genes or features in the simulated matrix."
    )]
    rows: usize,

    #[arg(
        short,
        long,
        help = "Number of columns or cells",
        long_help = "Set the number of columns, which typically corresponds to \n\
		     cells in the simulated matrix."
    )]
    cols: usize,

    #[arg(
        short,
        long,
        default_value_t = 1000,
        help = "Depth per column (expected non-zero genes per cell)",
        long_help = "Specify the expected number of non-zero genes per cell.\n\
		     This controls the sparsity of the simulated data."
    )]
    depth: usize,

    #[arg(
        short,
        long,
        default_value_t = 1,
        help = "Number of factors (cell types, topics, states, etc.)",
        long_help = "Set the number of factors, which can represent \n\
		     cell types, topics, states, or other groupings in the simulation."
    )]
    factors: usize,

    #[arg(
        short,
        long,
        default_value_t = 1,
        help = "Number of batches",
        long_help = "Specify the number of batches in the simulation. \n\
		     Batch effects will be added if two or more batches are specified."
    )]
    batches: usize,

    #[arg(
        long,
        default_value_t = 1.0,
        help = "Proportion of variance explained by topic membership",
        long_help = "Set the proportion of variance explained by the topic effects.\n\
		     This controls how much topics influence the simulated data."
    )]
    pve_topic: f32,

    #[arg(
        long,
        default_value_t = 1.0,
        help = "Proportion of variance explained by batch effects",
        long_help = "Set the proportion of variance explained by batch effects. \n\
		     This parameter only takes effect when there are two or more batches."
    )]
    pve_batch: f32,

    #[arg(
        short,
        long,
        help = "Output file header",
        long_help = "Specify the output file header. The output will be named as {output}.{backend}."
    )]
    output: Box<str>,

    #[arg(
        long,
        default_value_t = 10.0,
        help = "Overdispersion φ parameter for β ~ Gamma(1/φ, K*φ)",
        long_help = "Set the overdispersion φ parameter for β ~ Gamma(1/φ, K*φ).\n\
		     The mean of β will be K. \n\
		     A high value (φ > 10) causes strong influences from βθ factorization, \n\
		     while a small value (φ < 5) weakens the factorization. \n\
		     φ = 10 is a reasonable default value."
    )]
    overdisp: f32,

    #[arg(
        long,
        default_value_t = 42,
        help = "Random seed",
        long_help = "Set the random seed for reproducibility of the simulation."
    )]
    rseed: u64,

    #[arg(
        long,
        default_value_t = false,
        help = "Save output in MTX format",
        long_help = "Enable this option to save the simulated matrix in MTX format \n\
		     in addition to the backend format."
    )]
    save_mtx: bool,

    #[arg(
        long,
        value_enum,
        default_value = "zarr",
        help = "Backend format for output",
        long_help = "Choose the backend format for the output file. \n\
		     Supported formats include 'zarr' and 'h5'."
    )]
    backend: SparseIoBackend,
}

/////////////////////
// implementations //
/////////////////////

fn read_row_names(row_file: Box<str>, max_row_name_idx: usize) -> anyhow::Result<Vec<Box<str>>> {
    let _names = read_lines_of_words(&row_file, -1)?.lines;
    Ok(_names
        .into_iter()
        .map(|x| {
            let s = (0..x.len().min(max_row_name_idx))
                .filter_map(|i| x.get(i))
                .map(|x| x.to_string())
                .collect::<Vec<_>>()
                .join(ROW_SEP)
                .parse::<String>()
                .expect("invalid row name");
            s.into_boxed_str()
        })
        .collect())
}

fn read_col_names(col_file: Box<str>, max_column_name_idx: usize) -> anyhow::Result<Vec<Box<str>>> {
    let _names = read_lines_of_words(&col_file, -1)?.lines;
    Ok(_names
        .into_iter()
        .map(|x| {
            let s = (0..x.len().min(max_column_name_idx))
                .filter_map(|i| x.get(i))
                .map(|x| x.to_string())
                .collect::<Vec<_>>()
                .join(COLUMN_SEP)
                .parse::<String>()
                .expect("invalid col name");
            s.into_boxed_str()
        })
        .collect())
}

fn reorder_rows(args: &ReorderRowsArgs) -> anyhow::Result<()> {
    let row_names_order: Vec<Box<str>> = read_row_names(args.row_file.clone(), MAX_ROW_NAME_IDX)?;

    let (backend, data_file) = resolve_backend_file(&args.data_file, None)?;
    let (_, output_file) = resolve_backend_file(&args.output, Some(backend.clone()))?;

    if let Some(out_dir) = dirname(&output_file) {
        mkdir(&out_dir)?;
    }

    recursive_copy(&data_file, &output_file)?;
    info!("copied the existing data file {}", data_file);

    let mut data = open_sparse_matrix(&output_file, &backend)?;
    data.reorder_rows(&row_names_order)?;

    info!("done");
    Ok(())
}

fn align_backends(args: &AlignDataArgs) -> anyhow::Result<()> {
    let n_data = args.data_files.len();
    let n_data_columns = n_data.div_ceil(args.num_data_types);
    let n_expected = n_data_columns * args.num_data_types;

    if n_expected != n_data {
        return Err(anyhow::anyhow!(format!(
            "Should be multiple of {}: actual data files {} < expected {}",
            args.num_data_types, n_data, n_expected
        )));
    }

    let n_data_rows = n_expected.div_ceil(n_data_columns);

    let mut full_data_vec = args
        .data_files
        .iter()
        .enumerate()
        .map(|(i, a_file)| -> anyhow::Result<_> {
            let ext = file_ext(a_file)?;
            let base = basename(a_file)?;

            let data_col_id = i % n_data_columns + 1;
            let data_row_id = i / n_data_columns + 1;
            let dst_path = format!(
                "{}/{}_{}_{}.{}",
                args.output_directory, data_row_id, data_col_id, base, ext
            );
            info!("renaming files for easier sorting: {}", dst_path);
            recursive_copy(&a_file, &dst_path)?;
            let (backend, a_copied_file) = resolve_backend_file(&dst_path, None)?;
            open_sparse_matrix(&a_copied_file, &backend)
        })
        .collect::<anyhow::Result<Vec<_>>>()?;

    // identify common rows
    let mut shared_rows = vec![];
    for r in 0..n_data_rows {
        let data_set_idx: Vec<usize> = (r * n_data_columns..(r + 1) * n_data_columns).collect();

        let fully_shared = data_set_idx.len();
        let mut shared: HashMap<Box<str>, usize> = HashMap::default();
        for &di in data_set_idx.iter() {
            for v in full_data_vec[di].row_names()? {
                let count = shared.entry(v).or_default();
                *count += 1;
            }
        }

        let mut shared: Vec<Box<str>> = shared
            .into_iter()
            .filter_map(|(v, n)| if n == fully_shared { Some(v) } else { None })
            .collect();

        info!(
            "found {} shared rows/features on the data type {}",
            shared.len(),
            r
        );

        if shared.is_empty() {
            return Err(anyhow::anyhow!("no features are shared"));
        }

        shared.par_sort();
        shared_rows.push(shared);
    }

    for data_col in 0..n_data_columns {
        info!("aligning on the data column {}", data_col);

        // 0. subset of data sets
        let data_set_idx: Vec<usize> = (data_col..n_data).step_by(n_data_columns).collect();

        let subset_data_vec = data_set_idx
            .iter()
            .map(|&di| &full_data_vec[di])
            .collect::<Vec<_>>();

        // 1. figure out shared names for each column
        let fully_shared = data_set_idx.len();
        let mut shared: HashMap<Box<str>, usize> = HashMap::default();
        for &d in subset_data_vec.iter() {
            for v in d.column_names()? {
                let count = shared.entry(v).or_default();
                *count += 1;
            }
        }

        let mut shared: Vec<Box<str>> = shared
            .into_iter()
            .filter_map(|(v, n)| if n == fully_shared { Some(v) } else { None })
            .collect();

        info!("found {} shared columns", shared.len());

        if shared.is_empty() {
            for &di in data_set_idx.iter() {
                let data = &mut full_data_vec[di];
                info!(
                    "remove this unaligned backend: {}",
                    data.get_backend_file_name()
                );
                data.remove_backend_file()?;
            }
            continue;
        }

        shared.par_sort();

        // 2. subset columns
        for (r, &di) in data_set_idx.iter().enumerate() {
            let data = &mut full_data_vec[di];

            let pos: HashMap<Box<str>, usize> = data
                .column_names()?
                .into_iter()
                .enumerate()
                .map(|(i, x)| (x, i))
                .collect();
            let columns: Vec<usize> = shared.iter().map(|x| pos[x]).collect();

            let pos: HashMap<Box<str>, usize> = data
                .row_names()?
                .into_iter()
                .enumerate()
                .map(|(i, x)| (x, i))
                .collect();
            let rows: Vec<usize> = shared_rows[r].iter().map(|x| pos[x]).collect();
            data.subset_columns_rows(Some(&columns), Some(&rows))?;
        }

        info!(
            "done on the data column {}/{}",
            data_col + 1,
            n_data_columns
        );
    }

    Ok(())
}

fn subset_columns(args: &SubsetColumnsArgs) -> anyhow::Result<()> {
    let columns_indices = args.column_indices.clone();
    let column_name_file = args.name_file.clone();

    let (backend, data_file) = resolve_backend_file(&args.data_file, None)?;

    let (_, output_file) = resolve_backend_file(&args.output, Some(backend.clone()))?;

    if let Some(out_dir) = dirname(&output_file) {
        mkdir(&out_dir)?;
    }

    recursive_copy(&data_file, &output_file)?;
    info!("copied the existing data file {}", data_file);

    let mut data = open_sparse_matrix(&output_file, &backend)?;

    let selected_columns = if let Some(idx) = columns_indices {
        idx
    } else if let Some(column_file) = column_name_file {
        let col_names = read_col_names(column_file, MAX_COLUMN_NAME_IDX)?;
        if col_names.is_empty() {
            return Err(anyhow::anyhow!("Empty column file"));
        }
        let col_names_map = data
            .column_names()
            .expect("column names not found in data file")
            .iter()
            .enumerate()
            .map(|(i, x)| (x.clone(), i))
            .collect::<HashMap<_, _>>();

        let col_names_order = col_names
            .into_iter()
            .filter_map(|x| col_names_map.get(&x))
            .collect::<Vec<_>>();

        let idx: Vec<usize> = col_names_order.iter().map(|&x| *x).collect();
        if idx.is_empty() {
            return Err(anyhow::anyhow!("Found empty columns"));
        }
        idx
    } else {
        return Err(anyhow::anyhow!(
            "either `column-indices` or `name-file` must be provided"
        ));
    };

    data.subset_columns_rows(Some(&selected_columns), None)?;

    if args.do_squeeze {
        info!("Squeeze the backend data {}", &output_file);
        let squeeze_args = RunSqueezeArgs {
            data_file: output_file.into(),
            row_nnz_cutoff: args.row_nnz_cutoff,
            column_nnz_cutoff: args.column_nnz_cutoff,
            block_size: 100,
        };
        run_squeeze(&squeeze_args)?;
    }

    info!("done");
    Ok(())
}

fn take_columns(args: &TakeColumnsArgs) -> anyhow::Result<()> {
    let columns = args.column_indices.clone();
    let column_name_file = args.name_file.clone();
    let column_names_arg = args.column_names.clone();

    let (backend, data_file) = resolve_backend_file(&args.data_file, None)?;

    let output = args.output.clone();

    let data = open_sparse_matrix(&data_file, &backend)?;
    let row_names = data.row_names()?;

    // Closure for substring matching
    let match_columns_by_substring = |queries: &[Box<str>]| -> anyhow::Result<(Vec<usize>, Vec<Box<str>>)> {
        let all_column_names = data.column_names()?;
        let mut matched_indices = Vec::new();

        for query in queries.iter() {
            for (idx, column_name) in all_column_names.iter().enumerate() {
                if column_name.contains(query.as_ref()) {
                    matched_indices.push(idx);
                }
            }
        }

        if matched_indices.is_empty() {
            return Err(anyhow::anyhow!(
                "No column names matched the provided queries"
            ));
        }

        let column_names: Vec<Box<str>> = matched_indices.iter().map(|&i| all_column_names[i].clone()).collect();
        Ok((matched_indices, column_names))
    };

    let (data, column_names) = if let Some(columns) = columns {
        let n_columns = data.num_columns().unwrap_or(0);
        let columns: Vec<usize> = columns.into_iter().filter(|&i| i < n_columns).collect();

        if columns.is_empty() {
            return Err(anyhow::anyhow!("invalid indexes"));
        }

        let _names = data.column_names()?;
        let column_names: Vec<Box<str>> = columns.iter().map(|&i| _names[i].clone()).collect();

        (data.read_columns_ndarray(columns)?, column_names)
    } else if let Some(column_file) = column_name_file {
        let col_names_to_match = read_col_names(column_file, MAX_COLUMN_NAME_IDX)?;
        let (matched_indices, column_names) = match_columns_by_substring(&col_names_to_match)?;
        (data.read_columns_ndarray(matched_indices)?, column_names)
    } else if let Some(col_names_to_match) = column_names_arg {
        let (matched_indices, column_names) = match_columns_by_substring(&col_names_to_match)?;
        (data.read_columns_ndarray(matched_indices)?, column_names)
    } else {
        return Err(anyhow::anyhow!(
            "either `column-indices`, `name-file`, or `column-names` must be provided"
        ));
    };

    if let Ok(ext) = file_ext(&output) {
        if ext.as_ref() == "parquet" {
            data.to_parquet(Some(&row_names), Some(&column_names), &output)?;
            return Ok(());
        }
    }

    data.to_tsv(&output)?;

    Ok(())
}

fn take_rows(args: &TakeRowsArgs) -> anyhow::Result<()> {
    let rows = args.row_indices.clone();
    let row_name_file = args.name_file.clone();
    let row_names_arg = args.row_names.clone();

    let (backend, data_file) = resolve_backend_file(&args.data_file, None)?;

    let output = args.output.clone();

    let data_backend = open_sparse_matrix(&data_file, &backend)?;

    // Closure for substring matching
    let match_rows_by_substring = |queries: &[Box<str>]| -> anyhow::Result<(Vec<usize>, Vec<Box<str>>)> {
        let all_row_names = data_backend.row_names()?;
        let mut matched_indices = Vec::new();

        for query in queries.iter() {
            for (idx, row_name) in all_row_names.iter().enumerate() {
                if row_name.contains(query.as_ref()) {
                    matched_indices.push(idx);
                }
            }
        }

        if matched_indices.is_empty() {
            return Err(anyhow::anyhow!(
                "No row names matched the provided queries"
            ));
        }

        let row_names: Vec<Box<str>> = matched_indices.iter().map(|&i| all_row_names[i].clone()).collect();
        Ok((matched_indices, row_names))
    };

    let (data, row_names) = if let Some(rows) = rows {
        let n_rows = data_backend.num_rows().unwrap_or(0);
        let rows: Vec<usize> = rows.into_iter().filter(|&i| i < n_rows).collect();

        if rows.is_empty() {
            return Err(anyhow::anyhow!("invalid indexes"));
        }

        let _names = data_backend.row_names()?;
        let row_names: Vec<Box<str>> = rows.iter().map(|&i| _names[i].clone()).collect();

        (data_backend.read_rows_ndarray(rows)?, row_names)
    } else if let Some(row_name_file) = row_name_file {
        let row_names_to_match = read_col_names(row_name_file, MAX_ROW_NAME_IDX)?;
        let (matched_indices, row_names) = match_rows_by_substring(&row_names_to_match)?;
        (data_backend.read_rows_ndarray(matched_indices)?, row_names)
    } else if let Some(row_names_to_match) = row_names_arg {
        let (matched_indices, row_names) = match_rows_by_substring(&row_names_to_match)?;
        (data_backend.read_rows_ndarray(matched_indices)?, row_names)
    } else {
        return Err(anyhow::anyhow!(
            "either `row-indices`, `name-file`, or `row-names` must be provided"
        ));
    };

    let data_t = data.t().to_owned();

    if let Ok(ext) = file_ext(&output) {
        if ext.as_ref() == "parquet" {
            let column_names = data_backend.column_names()?;
            data_t.to_parquet(Some(&column_names), Some(&row_names), &output)?;
            return Ok(());
        }
    }

    data_t.to_tsv(&output)?;
    Ok(())
}

fn run_merge_backend(args: &MergeBackendArgs) -> anyhow::Result<()> {
    if args.data_files.len() <= 1 {
        info!("no need to merge one file");
        return Ok(());
    }

    if args.verbose > 0 {
        std::env::set_var("RUST_LOG", "info");
    }

    let num_batches = args.data_files.len();
    info!("merging over {} batches ...", num_batches);

    let mut row_names = vec![];
    let mut column_names = vec![];
    let mut column_batch_names = vec![];
    let mut triplets = vec![];

    let mut ntot = 0;
    for data_file in args.data_files.iter() {
        info!("Importing data file: {}", data_file);

        let backend = match file_ext(data_file)?.to_string().as_str() {
            "h5" => SparseIoBackend::HDF5,
            "zarr" => SparseIoBackend::Zarr,
            _ => SparseIoBackend::Zarr,
        };

        let mut data = open_sparse_matrix(data_file, &backend)?;
        data.preload_columns()?;

        if row_names.is_empty() {
            row_names = data.row_names()?;
        } else {
            info!("checking if the row names are consistent");
            assert_eq!(row_names, data.row_names()?);
        }

        if let Some(ntot_curr) = data.num_columns() {
            // 1. read triplets
            let jobs = create_jobs(ntot_curr, Some(args.block_size));
            let triplets_curr = jobs
                .par_iter()
                .progress_count(jobs.len() as u64)
                .filter_map(|(lb, ub)| {
                    if let Ok((_, _, triplets)) =
                        data.read_triplets_by_columns((*lb..*ub).collect())
                    {
                        let offset = (*lb + ntot) as u64;
                        Some(
                            triplets
                                .iter()
                                .map(|&(i, j, x_ij)| (i, j + offset, x_ij))
                                .collect::<Vec<_>>(),
                        )
                    } else {
                        None
                    }
                })
                .flatten()
                .collect::<Vec<_>>();

            let _names = data.column_names()?;
            let batch_name = basename(data_file)?;
            column_names.extend(
                _names
                    .into_iter()
                    .map(|x| format!("{}{}{}", x, COLUMN_SEP, batch_name).into_boxed_str())
                    .collect::<Vec<_>>(),
            );

            triplets.extend(triplets_curr);
            column_batch_names.extend(vec![batch_name.clone(); ntot_curr]);
            ntot += ntot_curr;
        }
    }

    info!("Found {} columns/barcodes ...", column_names.len());

    let (backend, backend_file) = resolve_backend_file(&args.output, Some(args.backend.clone()))?;

    if std::path::Path::new(backend_file.as_ref()).exists() {
        info!(
            "This existing backend file '{}' will be deleted",
            &backend_file
        );
        remove_file(&backend_file)?;
    }

    let mut data = create_sparse_from_triplets(
        &triplets,
        (row_names.len(), column_names.len(), triplets.len()),
        Some(&backend_file),
        Some(&backend),
    )?;

    data.register_row_names_vec(&row_names);
    data.register_column_names_vec(&column_names);

    info!(
        "Successfully created a sparse backend file: {}",
        &backend_file
    );

    let batch_map = column_names
        .into_iter()
        .zip(column_batch_names)
        .collect::<HashMap<_, _>>();

    if args.do_squeeze {
        info!("Squeeze the backend data {}", &backend_file);
        let squeeze_args = RunSqueezeArgs {
            data_file: backend_file.clone(),
            row_nnz_cutoff: args.row_nnz_cutoff,
            column_nnz_cutoff: args.column_nnz_cutoff,
            block_size: args.block_size,
        };

        run_squeeze(&squeeze_args)?;
    }

    // do the batch mapping at the end
    let batch_memb_file = format!("{}.batch.gz", args.output);
    let data = open_sparse_matrix(&backend_file, &backend)?;
    let default_batch = basename(&args.output)?;
    let column_batch_names = data
        .column_names()?
        .iter()
        .map(|k| batch_map.get(k).unwrap_or(&default_batch).clone())
        .collect::<Vec<_>>();

    write_lines(&column_batch_names, &batch_memb_file)?;

    info!("done");
    Ok(())
}

fn run_merge_mtx(args: &MergeMtxArgs) -> anyhow::Result<()> {
    if args.verbose > 0 {
        std::env::set_var("RUST_LOG", "info");
    }

    let directories = args.data_directories.clone();

    let mut mtx_files = vec![];
    let mut row_files = vec![];
    let mut col_files = vec![];
    let mut batch_names = vec![];

    for dir in directories.iter() {
        let dir = dir.clone().into_string();

        if let Some(base) = std::path::Path::new(&dir).file_stem() {
            let base = base.to_str().expect("invalid base name").to_string();
            info!("Searching relevant files within: {}", &base);
            let batch_name = Some(base);

            if let Ok(this_dir) = std::fs::read_dir(&dir) {
                let mut mtx: Option<Box<str>> = None;
                let mut row: Option<Box<str>> = None;
                let mut col: Option<Box<str>> = None;

                for x in this_dir {
                    if let Some(_path) = x?.path().to_str() {
                        let _path = _path.to_string();

                        if _path.ends_with(args.mtx_file_name.as_ref()) {
                            mtx = Some(_path.into_boxed_str());
                        } else if _path.ends_with(args.feature_file_name.as_ref()) {
                            row = Some(_path.into_boxed_str());
                        } else if _path.ends_with(args.barcode_file_name.as_ref()) {
                            col = Some(_path.into_boxed_str());
                        }
                    }
                }

                if let (Some(m), Some(r), Some(c), Some(b)) = (mtx, row, col, batch_name) {
                    info!("Build {} from {}, {}, {} ", &b, &m, &r, &c);
                    mtx_files.push(m);
                    row_files.push(r);
                    col_files.push(c);
                    batch_names.push(b);
                }
            }
        }

        info!("Searching subdir within: {}", &dir);

        let mut sub_dir_vec = std::fs::read_dir(&dir)?
            .filter_map(Result::ok)
            .collect::<Vec<_>>();

        sub_dir_vec.sort_by_key(|entry| entry.file_name());

        for sub in sub_dir_vec {
            if let Some(sub_dir) = sub.path().to_str() {
                let mut mtx: Option<Box<str>> = None;
                let mut row: Option<Box<str>> = None;
                let mut col: Option<Box<str>> = None;

                if let Some(base) = std::path::Path::new(&sub_dir).file_stem() {
                    let base = base.to_str().expect("invalid base name").to_string();
                    info!("searching {} ...", &base);

                    let batch_name = Some(base);

                    if let Ok(sub_dir) = std::fs::read_dir(sub_dir) {
                        for x in sub_dir {
                            if let Some(_path) = x?.path().to_str() {
                                let _path = _path.to_string();

                                info!("Found: {}", &_path);

                                if _path.ends_with(args.mtx_file_name.as_ref()) {
                                    mtx = Some(_path.into_boxed_str());
                                } else if _path.ends_with(args.feature_file_name.as_ref()) {
                                    row = Some(_path.into_boxed_str());
                                } else if _path.ends_with(args.barcode_file_name.as_ref()) {
                                    col = Some(_path.into_boxed_str());
                                }
                            }
                        }
                    }

                    if let (Some(m), Some(r), Some(c), Some(b)) = (mtx, row, col, batch_name) {
                        info!("Build {} from {}, {}, {} ", &b, &m, &r, &c);
                        mtx_files.push(m);
                        row_files.push(r);
                        col_files.push(c);
                        batch_names.push(b);
                    }
                }
            }
        }
    }

    let num_batches = batch_names.len();

    info!("merging over {} batches ...", num_batches);

    debug_assert_eq!(num_batches, mtx_files.len());
    debug_assert_eq!(num_batches, row_files.len());
    debug_assert_eq!(num_batches, col_files.len());

    if num_batches == 0 {
        return Err(anyhow::anyhow!("No relevant files found"));
    }

    info!("Finding common rows/features ...");

    let mut row_hash: HashMap<Box<str>, usize> = HashMap::default();

    for row_file in row_files.iter() {
        let row_names = read_row_names(row_file.clone(), args.num_feature_name_words)?;
        for name in row_names.iter() {
            let n = row_hash.entry(name.clone()).or_insert(0);
            *n += 1;
        }
    }

    let mut common_rows: Vec<Box<str>> = row_hash
        .into_iter()
        .filter_map(|(k, v)| {
            if v == num_batches {
                Some(k.clone())
            } else {
                None
            }
        })
        .collect();

    common_rows.sort_by_key(|x| x.to_string());

    let row_pos: HashMap<Box<str>, usize> = common_rows
        .iter()
        .enumerate()
        .map(|(i, v)| (v.clone(), i))
        .collect();

    info!(
        "Found {} common row/feature names across {} file sets",
        row_pos.len(),
        num_batches
    );

    info!("Elongating column/barcode names ...");

    let mut column_names = vec![];
    let mut column_batch_names = vec![];

    for (col_file, batch_name) in col_files.iter().zip(batch_names.iter()) {
        let _names = read_col_names(col_file.clone(), args.num_barcode_name_words)?;
        let nn = _names.len();
        column_names.extend(
            _names
                .into_iter()
                .map(|x| format!("{}{}{}", x, COLUMN_SEP, batch_name).into_boxed_str())
                .collect::<Vec<_>>(),
        );

        column_batch_names.extend(vec![batch_name.clone().into_boxed_str(); nn]);
    }

    info!("Found {} columns/barcodes ...", column_names.len());

    info!("Renaming triplets...");

    let mut renamed_triplets = vec![];
    let mut offset = 0;
    let mut nnz_tot = 0;

    let pb = ProgressBar::new(num_batches as u64);

    if args.verbose > 0 {
        pb.set_draw_target(ProgressDrawTarget::hidden());
    }

    for b in 0..num_batches {
        let row_names = read_row_names(row_files[b].clone(), args.num_feature_name_words)?;

        let (triplets, shape) = mtx_io::read_mtx_triplets(&mtx_files[b].clone())?;

        if let Some((nrow, ncol, nnz)) = shape {
            info!(
                "{}: {} rows, {} columns, {} non-zeros",
                &mtx_files[b], nrow, ncol, nnz
            );

            let triplets = triplets
                .into_iter()
                .filter_map(|(batch_i, batch_j, x_ij)| {
                    if let Some(i) = row_pos.get(&row_names[batch_i as usize]) {
                        let i = *i as u64;
                        let j = batch_j + offset;
                        Some((i, j, x_ij))
                    } else {
                        None
                    }
                })
                .collect::<Vec<_>>();

            nnz_tot += triplets.len();
            renamed_triplets.extend(triplets);

            let batch_col_names =
                read_col_names(col_files[b].clone(), args.num_barcode_name_words)?;
            let batch_name = batch_names[b].clone();

            (0..ncol).for_each(|batch_j| {
                let loc = format!("{}{}{}", batch_col_names[batch_j], COLUMN_SEP, batch_name)
                    .into_boxed_str();

                let glob_j = offset as usize + batch_j;
                let glob = column_names[glob_j].clone();
                debug_assert_eq!(glob, loc);
            });

            offset += ncol as u64;
            pb.inc(1);
        }
    }
    pb.finish_and_clear();

    info!("Done with creating triplets from {} mtx files", num_batches);

    let backend = args.backend.clone();
    let output = args.output.clone();
    let batch_memb_file = (output.to_string() + ".batch.gz").into_boxed_str();

    let backend_file = match backend {
        SparseIoBackend::HDF5 => format!("{}.h5", &output),
        SparseIoBackend::Zarr => format!("{}.zarr", &output),
    };

    if std::path::Path::new(&backend_file).exists() {
        info!(
            "This existing backend file '{}' will be deleted",
            &backend_file
        );
        remove_file(&backend_file)?;
    }

    let mut data = create_sparse_from_triplets(
        &renamed_triplets,
        (row_pos.len(), offset as usize, nnz_tot),
        Some(&backend_file),
        Some(&backend),
    )?;

    data.register_row_names_vec(&common_rows);
    data.register_column_names_vec(&column_names);

    info!(
        "Successfully created a sparse backend file: {}",
        &backend_file
    );

    let batch_map = column_names
        .into_iter()
        .zip(column_batch_names)
        .collect::<HashMap<_, _>>();

    if args.do_squeeze {
        info!("Squeeze the backend data {}", &backend_file);
        let squeeze_args = RunSqueezeArgs {
            data_file: backend_file.clone().into_boxed_str(),
            row_nnz_cutoff: args.row_nnz_cutoff,
            column_nnz_cutoff: args.column_nnz_cutoff,
            block_size: 100,
        };

        run_squeeze(&squeeze_args)?;
    }

    // do the batch mapping at the end
    let data = open_sparse_matrix(&backend_file, &backend)?;
    let default_batch = basename(&args.output)?;
    let column_batch_names = data
        .column_names()?
        .iter()
        .map(|k| batch_map.get(k).unwrap_or(&default_batch).clone())
        .collect::<Vec<_>>();

    write_lines(&column_batch_names, &batch_memb_file)?;

    info!("done");
    Ok(())
}

fn run_build_from_mtx(args: &FromMtxArgs) -> anyhow::Result<()> {
    if args.verbose > 0 {
        std::env::set_var("RUST_LOG", "info");
    }

    let mtx_file = args.mtx.as_ref();
    let row_file = args.row.as_ref();
    let col_file = args.col.as_ref();

    let (backend, backend_file) = resolve_backend_file(&args.output, Some(args.backend.clone()))?;

    if std::path::Path::new(backend_file.as_ref()).exists() {
        info!(
            "This existing backend file '{}' will be deleted",
            &backend_file
        );
        remove_file(&backend_file)?;
    }

    let mut data = create_sparse_from_mtx_file(mtx_file, Some(&backend_file), Some(&backend))?;

    if let Some(row_file) = row_file {
        data.register_row_names_file(row_file);
    } else if let Some(nrow) = data.num_rows() {
        let row_names: Vec<Box<str>> = (1..(nrow + 1)).map(|i| format!("{}", i).into()).collect();
        data.register_row_names_vec(&row_names);
    }

    if let Some(col_file) = col_file {
        data.register_column_names_file(col_file);
    } else if let Some(ncol) = data.num_columns() {
        let col_names: Vec<Box<str>> = (1..(ncol + 1)).map(|i| format!("{}", i).into()).collect();
        data.register_column_names_vec(&col_names);
    }

    if args.do_squeeze {
        let squeeze_args = RunSqueezeArgs {
            data_file: backend_file.clone(),
            row_nnz_cutoff: args.row_nnz_cutoff,
            column_nnz_cutoff: args.column_nnz_cutoff,
            block_size: 100,
        };

        run_squeeze(&squeeze_args)?;
    }

    info!("done");
    Ok(())
}

fn list_zarr(cmd_args: &ListZarrArgs) -> anyhow::Result<()> {
    let ext = file_ext(&cmd_args.zarr_file)?;

    match ext.to_string().as_ref() {
        "zarr" => {
            info!(".zarr file: {}", cmd_args.zarr_file.as_ref());
            use zarrs::config::MetadataRetrieveVersion;
            use zarrs::filesystem::FilesystemStore;
            use zarrs::node::Node;

            let store = Arc::new(FilesystemStore::new(cmd_args.zarr_file.as_ref())?);
            let node = Node::open_opt(&store, "/", &MetadataRetrieveVersion::Default).unwrap();
            let tree = node.hierarchy_tree();
            println!("hierarchy_tree:\n{}", tree);
        }
        "zip" => {
            info!("zipped .zarr file: {}", cmd_args.zarr_file.as_ref());
            use zip::ZipArchive;

            let file = std::fs::File::open(cmd_args.zarr_file.as_ref())?;
            let reader = std::io::BufReader::new(file);

            let mut archive = ZipArchive::new(reader)?;

            for i in 0..archive.len() {
                let file = archive.by_index(i)?;
                println!("{}", file.name());
                // if file.is_dir() {
                //     println!("{}", file.name());
                // }
            }
        }
        _ => {
            info!("unknown extension '{}'", ext);
        }
    };

    Ok(())
}

fn list_h5(cmd_args: &ListH5Args) -> anyhow::Result<()> {
    let data_file = cmd_args.h5_file.clone();
    let file = hdf5::File::open(data_file.to_string())?;
    info!("Opened {}", data_file.clone());

    fn list_group(group: &hdf5::Group, indent: usize) -> hdf5::Result<()> {
        for member in group.member_names()? {
            println!("{:indent$}{}", "", member, indent = indent);

            if let Ok(obj) = group.group(&member) {
                if let Ok(subgroup) = obj.as_group() {
                    list_group(&subgroup, indent + 2)?;
                }
            }
        }
        Ok(())
    }

    list_group(&file, 0)?;

    Ok(())
}

fn run_build_from_zarr_triplets(args: &FromZarrArgs) -> anyhow::Result<()> {
    if args.verbose > 0 {
        std::env::set_var("RUST_LOG", "info");
    }

    let source_zarr_file_path = args.zarr_file.clone();

    let (backend, backend_file) = resolve_backend_file(&args.output, Some(args.backend.clone()))?;

    if std::path::Path::new(backend_file.as_ref()).exists() {
        info!(
            "This existing backend file '{}' will be deleted",
            &backend_file
        );
        remove_file(&backend_file)?;
    }

    let (_dir, base, ext) = dir_base_ext(&source_zarr_file_path)?;

    let temp_dir = TempDir::new()?; // should be kept outside

    let store = if ext.as_ref() == "zip" {
        let temp_path = std::path::PathBuf::from(temp_dir.path());
        let temp_zarr = format!("{}/{}", temp_path.to_str().unwrap(), base);
        // let temp_zarr = format!("{}/{}", dir, base);
        unzip_dir(&source_zarr_file_path, Some(temp_zarr.as_ref()))?;
        info!("Unzipped to {}", temp_zarr);
        Arc::new(zarrs::filesystem::FilesystemStore::new(temp_zarr)?)
    } else {
        info!("Store at {}", source_zarr_file_path);
        Arc::new(zarrs::filesystem::FilesystemStore::new(
            source_zarr_file_path.as_ref(),
        )?)
    };

    let indices: Vec<u64> = read_zarr_numerics(store.clone(), args.indices_field.as_ref())?;
    let indptr: Vec<u64> = read_zarr_numerics(store.clone(), args.indptr_field.as_ref())?;
    let values: Vec<f32> = read_zarr_numerics(store.clone(), args.data_field.as_ref())?;
    info!("Read the arrays");

    let CooTripletsShape { triplets, shape } = ValuesIndicesPointers {
        values: &values,
        indices: &indices,
        indptr: &indptr,
    }
    .into_coo(args.pointer_type.clone())?;

    let TripletsShape { nrows, ncols, nnz } = shape;
    info!("Read {} non-zero elements in {} x {}", nnz, nrows, ncols);

    let mut row_ids = read_zarr_attr::<Vec<Box<str>>>(store.clone(), &args.row_id_field)
        .or_else(|_| read_zarr_strings(store.clone(), args.row_id_field.as_ref()))
        .unwrap_or_else(|_| (0..nrows).map(|x| x.to_string().into_boxed_str()).collect());

    let mut row_names = read_zarr_attr::<Vec<Box<str>>>(store.clone(), &args.row_name_field)
        .or_else(|_| read_zarr_strings(store.clone(), args.row_id_field.as_ref()))
        .unwrap_or_else(|_| (0..nrows).map(|x| x.to_string().into_boxed_str()).collect());

    info!("Read {} row names", row_ids.len());
    if nrows < row_ids.len() {
        info!("data doesn't contain all the row IDs");
        row_ids.truncate(nrows);
    }

    if nrows < row_names.len() {
        info!("data doesn't contain all the row names");
        row_names.truncate(nrows);
    }

    assert_eq!(nrows, row_ids.len());
    assert_eq!(nrows, row_names.len());

    // have composite row names
    let row_ids: Vec<Box<str>> = row_ids
        .into_iter()
        .zip(row_names)
        .map(|(id, name)| {
            if name.len() > 0 {
                format!("{}_{}", id, name).into_boxed_str()
            } else {
                id
            }
        })
        .collect();

    let mut row_types = read_zarr_attr::<Vec<Box<str>>>(store.clone(), &args.row_type_field)
        .or_else(|_| read_zarr_strings(store.clone(), args.row_id_field.as_ref()))
        .unwrap_or_else(|_| vec![args.select_row_type.clone(); nrows]);
    if nrows < row_types.len() {
        info!("data doesn't contain all the rows");
        row_types.truncate(nrows);
    }
    assert_eq!(nrows, row_types.len());

    let select_pattern = args.select_row_type.to_lowercase();
    let remove_pattern = args.remove_row_type.to_lowercase();
    let select_rows = row_types
        .iter()
        .enumerate()
        .filter_map(|(i, x)| {
            if x.to_lowercase().contains(&select_pattern)
                && !x.to_lowercase().contains(&remove_pattern)
            {
                Some(i)
            } else {
                None
            }
        })
        .collect::<Vec<_>>();

    let mut column_names =
        parse_10x_cell_id(read_zarr_ndarray::<u32>(store.clone(), &args.column_name_field)?.view())
            .or_else(|_| read_zarr_attr::<Vec<Box<str>>>(store.clone(), &args.column_name_field))
            .or_else(|_| read_zarr_strings(store.clone(), args.column_name_field.as_ref()))
            .unwrap_or_else(|_| (0..ncols).map(|x| x.to_string().into_boxed_str()).collect());

    if ncols < column_names.len() {
        info!("data doesn't contain all the columns");
        column_names.truncate(ncols);
    }
    assert_eq!(ncols, column_names.len());

    let mut out = create_sparse_from_triplets(
        &triplets,
        (nrows, ncols, nnz),
        Some(&backend_file),
        Some(&backend),
    )?;
    info!("created sparse matrix: {}", backend_file);
    out.register_row_names_vec(&row_ids);
    out.register_column_names_vec(&column_names);

    if select_rows.len() < nrows {
        info!("filtering in features of `{}` type", args.select_row_type);
        out.subset_columns_rows(None, Some(&select_rows))?;
    }

    if args.do_squeeze {
        info!("Squeeze the backend data {}", &backend_file);
        let squeeze_args = RunSqueezeArgs {
            data_file: backend_file.clone(),
            row_nnz_cutoff: args.row_nnz_cutoff,
            column_nnz_cutoff: args.column_nnz_cutoff,
            block_size: 100,
        };

        run_squeeze(&squeeze_args)?;
    }

    info!("done");
    Ok(())
}

fn run_build_from_h5_triplets(args: &FromH5Args) -> anyhow::Result<()> {
    if args.verbose > 0 {
        std::env::set_var("RUST_LOG", "info");
    }

    let h5_source_file = args.h5_file.clone();

    let (backend, backend_file) = resolve_backend_file(&args.output, Some(args.backend.clone()))?;

    if std::path::Path::new(backend_file.as_ref()).exists() {
        info!(
            "This existing backend file '{}' will be deleted",
            &backend_file
        );
        remove_file(&backend_file)?;
    }

    let file = hdf5::File::open(h5_source_file.to_string())?;
    info!("Opened data file: {}", h5_source_file.clone());

    if let Ok(root) = file.group(args.root_group_name.as_ref()) {
        // Read triplets with the shape information
        let CooTripletsShape { triplets, shape } = if let (Ok(values), Ok(indices), Ok(indptr)) = (
            root.dataset(args.data_field.as_ref()),
            root.dataset(args.indices_field.as_ref()),
            root.dataset(args.indptr_field.as_ref()),
        ) {
            ValuesIndicesPointers {
                values: &values.read_1d::<f32>()?.to_vec(),
                indices: &indices.read_1d::<u64>()?.to_vec(),
                indptr: &indptr.read_1d::<u64>()?.to_vec(),
            }
            .into_coo(args.pointer_type.clone())?
        } else {
            return Err(anyhow::anyhow!("unable to read triplets"));
        };

        let TripletsShape { nrows, ncols, nnz } = shape;
        info!("Read {} non-zero elements in {} x {}", nnz, nrows, ncols);

        let mut row_ids: Vec<Box<str>> = match root.dataset(args.row_id_field.as_ref()) {
            Ok(rows) => read_hdf5_strings(rows)?,
            _ => {
                info!("row (feature) IDs not found");
                (0..nrows).map(|x| x.to_string().into_boxed_str()).collect()
            }
        };

        let mut row_names: Vec<Box<str>> = match root.dataset(args.row_name_field.as_ref()) {
            Ok(rows) => read_hdf5_strings(rows)?,
            _ => {
                info!("row (feature) names not found");
                vec![Box::from(""); nrows]
            }
        };

        info!("Read {} row names", row_ids.len());
        if nrows < row_ids.len() {
            info!("data doesn't contain all the row IDs");
            row_ids.truncate(nrows);
        }

        if nrows < row_names.len() {
            info!("data doesn't contain all the row names");
            row_names.truncate(nrows);
        }

        assert_eq!(nrows, row_ids.len());
        assert_eq!(nrows, row_names.len());

        // have composite row names
        let row_ids: Vec<Box<str>> = row_ids
            .into_iter()
            .zip(row_names)
            .map(|(id, name)| {
                if name.len() > 0 {
                    format!("{}_{}", id, name).into_boxed_str()
                } else {
                    id
                }
            })
            .collect();

        let mut row_types: Vec<Box<str>> = match root.dataset(args.row_type_field.as_ref()) {
            Ok(rows) => read_hdf5_strings(rows)?,
            _ => {
                info!("use all the types");
                vec![args.select_row_type.clone(); nrows]
            }
        };

        if nrows < row_types.len() {
            info!("data doesn't contain all the row types");
            row_types.truncate(nrows);
        }

        assert_eq!(nrows, row_types.len());

        let mut column_names: Vec<Box<str>> = match root.dataset(args.column_name_field.as_ref()) {
            Ok(columns) => read_hdf5_strings(columns)?,
            _ => {
                info!("column (cell) names not found");
                (0..ncols).map(|x| x.to_string().into_boxed_str()).collect()
            }
        };
        info!("Read {} column names", column_names.len());

        if ncols < column_names.len() {
            info!("data doesn't contain all the columns");
            column_names.truncate(ncols);
        }
        assert_eq!(ncols, column_names.len());

        let mut out = create_sparse_from_triplets(
            &triplets,
            (nrows, ncols, nnz),
            Some(&backend_file),
            Some(&backend),
        )?;
        info!("created sparse matrix: {}", backend_file);
        out.register_row_names_vec(&row_ids);
        out.register_column_names_vec(&column_names);

        let select_pattern = args.select_row_type.to_lowercase();
        let remove_pattern = args.remove_row_type.to_lowercase();
        let select_rows = row_types
            .iter()
            .enumerate()
            .filter_map(|(i, x)| {
                if x.to_lowercase().contains(&select_pattern)
                    && !x.to_lowercase().contains(&remove_pattern)
                {
                    Some(i)
                } else {
                    None
                }
            })
            .collect::<Vec<_>>();

        if select_rows.len() < nrows {
            info!("filtering in features of `{}` type", args.select_row_type);
            out.subset_columns_rows(None, Some(&select_rows))?;
        }
    } else {
        return Err(anyhow::anyhow!(
            "unable to identify data with the specified root group name: {}",
            args.root_group_name
        ));
    }

    if args.do_squeeze {
        info!("Squeeze the backend data {}", &backend_file);
        let squeeze_args = RunSqueezeArgs {
            data_file: backend_file.clone(),
            row_nnz_cutoff: args.row_nnz_cutoff,
            column_nnz_cutoff: args.column_nnz_cutoff,
            block_size: 100,
        };

        run_squeeze(&squeeze_args)?;
    }
    info!("done");
    Ok(())
}

fn take_column_names(cmd_args: &TakeColumnNamesArgs) -> anyhow::Result<()> {
    use write_lines;

    let output = cmd_args.output.clone();
    let (backend, input) = resolve_backend_file(&cmd_args.data_file, None)?;
    let data = open_sparse_matrix(&input, &backend)?;
    let col_names = data.column_names()?;
    write_lines(&col_names, &output)?;

    Ok(())
}

fn take_row_names(cmd_args: &TakeRowNamesArgs) -> anyhow::Result<()> {
    use write_lines;

    let output = cmd_args.output.clone();
    let (backend, input) = resolve_backend_file(&cmd_args.data_file, None)?;
    let data = open_sparse_matrix(&input, &backend)?;

    let row_names = data.row_names()?;
    write_lines(&row_names, &output)?;

    Ok(())
}

fn show_info(cmd_args: &InfoArgs) -> anyhow::Result<()> {
    let output = cmd_args.output.clone();

    let (backend, input) = resolve_backend_file(&cmd_args.data_file, None)?;

    let data = open_sparse_matrix(&input, &backend)?;

    if let (Some(nrow), Some(ncol), Some(nnz)) =
        (data.num_rows(), data.num_columns(), data.num_non_zeros())
    {
        println!("number_of_rows:\t{}", nrow);
        println!("number_of_columns:\t{}", ncol);
        println!("number_of_nonzeros:\t{}", nnz);
    }

    if !output.is_empty() {
        use {mkdir, write_lines};
        mkdir(&output)?;
        let row_names = data.row_names()?;
        let col_names = data.column_names()?;
        write_lines(&row_names, &(output.to_string() + ".rows.gz"))?;
        write_lines(&col_names, &(output.to_string() + ".columns.gz"))?;
    }

    Ok(())
}

fn run_stat(cmd_args: &RunStatArgs) -> anyhow::Result<()> {
    let output = cmd_args.output.clone();
    dirname(&output).as_deref().map(mkdir).transpose()?;

    // to avoid duplicate barcodes in the column names
    let attach_data_name = cmd_args.data_files.len() > 1;

    let mut data = SparseIoVec::new();
    for data_file in cmd_args.data_files.iter() {
        let (backend, data_file) = resolve_backend_file(&data_file, None)?;

        let this_data = open_sparse_matrix(&data_file, &backend)?;
        let data_name = attach_data_name.then(|| basename(&data_file)).transpose()?;
        data.push(Arc::from(this_data), data_name)?;
    }

    match cmd_args.stat_dim {
        StatDim::Row => {
            if let Some(column_group_file) = &cmd_args.column_group_file {
                let ReadLinesOut { lines, header: _ } =
                    read_lines_of_words(&column_group_file, -1)?;
                if lines.is_empty() {
                    return Err(anyhow::anyhow!("empty group membership file"));
                }

                let cols = data.column_names()?;

                let column_membership = if lines[0].len() == 1 && lines.len() == cols.len() {
                    cols.into_iter()
                        .zip(lines)
                        .map(|(c, v)| (c, v[0].clone()))
                        .collect::<HashMap<_, _>>()
                } else {
                    lines
                        .into_iter()
                        .map(|v| (v[0].clone(), v[1].clone()))
                        .collect::<HashMap<_, _>>()
                };

                let (group_names, group_stats) = collect_stratified_row_stat_across_vec(
                    &data,
                    &column_membership,
                    cmd_args.block_size,
                )?;

                for (g, row_stat) in group_names.into_iter().zip(group_stats) {
                    let (dir, base, ext) = dir_base_ext(&cmd_args.output)?;

                    if cmd_args.output.eq_ignore_ascii_case("stdout") {
                        let out = row_stat
                            .to_string_vec(&data.row_names()?, "\t")?
                            .into_iter()
                            .map(|s| format!("{}\t{}", g, s).into_boxed_str())
                            .collect();
                        write_lines(&out, &cmd_args.output)?;
                    } else {
                        let out_file = if dir.len() > 0 {
                            format!("{}/{}.{}.{}", dir, base, g, ext)
                        } else {
                            format!("{}.{}.{}", base, g, ext)
                        };

                        info!("writing out: {}", out_file);
                        row_stat.save(&out_file, &data.row_names()?, "\t")?;
                    }
                }
            }

            let row_stat = collect_row_stat_across_vec(&data, cmd_args.block_size)?;

            row_stat.save(&cmd_args.output, &data.row_names()?, "\t")?;
        }
        StatDim::Column => {
            let select_rows = cmd_args.row_name_pattern.as_ref().map(|x| {
                let select_pattern = x.to_lowercase();
                data.row_names()
                    .expect("couldn't get the row names")
                    .iter()
                    .enumerate()
                    .filter_map(|(i, x)| {
                        if x.to_lowercase().contains(&select_pattern) {
                            Some(i)
                        } else {
                            None
                        }
                    })
                    .collect::<Vec<_>>()
            });

            let col_stat =
                collect_column_stat_across_vec(&data, select_rows.as_deref(), cmd_args.block_size)?;

            col_stat.save(&cmd_args.output, &data.column_names()?, "\t")?;
        }
    };

    Ok(())
}

fn run_squeeze(cmd_args: &RunSqueezeArgs) -> anyhow::Result<()> {
    let (backend, data_file) = resolve_backend_file(&cmd_args.data_file, None)?;
    let row_nnz_cutoff = cmd_args.row_nnz_cutoff;
    let col_nnz_cutoff = cmd_args.column_nnz_cutoff;

    let data = open_sparse_matrix(&data_file, &backend)?;

    info!(
        "before squeeze -- data: {} rows x {} columns",
        data.num_rows().unwrap(),
        data.num_columns().unwrap()
    );

    squeeze_by_nnz(
        data.as_ref(),
        SqueezeCutoffs {
            row: row_nnz_cutoff,
            column: col_nnz_cutoff,
        },
        cmd_args.block_size,
    )?;

    let data = open_sparse_matrix(&data_file, &backend)?;

    info!(
        "after squeeze -- data: {} rows x {} columns",
        data.num_rows().unwrap(),
        data.num_columns().unwrap()
    );

    Ok(())
}

fn run_simulate(cmd_args: &RunSimulateArgs) -> anyhow::Result<()> {
    let output = cmd_args.output.clone();
    mkdir(&output)?;

    let backend = cmd_args.backend.clone();

    let backend_file = match backend {
        SparseIoBackend::HDF5 => output.to_string() + ".h5",
        SparseIoBackend::Zarr => output.to_string() + ".zarr",
    };

    let mtx_file = output.to_string() + ".mtx.gz";
    let row_file = output.to_string() + ".rows.gz";
    let col_file = output.to_string() + ".cols.gz";

    let dict_file = mtx_file.replace(".mtx.gz", ".dict.parquet");
    let prop_file = mtx_file.replace(".mtx.gz", ".prop.parquet");
    let batch_memb_file = mtx_file.replace(".mtx.gz", ".batch.gz");
    let ln_batch_file = mtx_file.replace(".mtx.gz", ".ln_batch.parquet");

    remove_all_files(&vec![
        backend_file.clone().into_boxed_str(),
        mtx_file.clone().into_boxed_str(),
        dict_file.clone().into_boxed_str(),
        prop_file.clone().into_boxed_str(),
        batch_memb_file.clone().into_boxed_str(),
        ln_batch_file.clone().into_boxed_str(),
    ])
    .expect("failed to clean up existing output files");

    let sim_args = simulate::SimArgs {
        rows: cmd_args.rows,
        cols: cmd_args.cols,
        depth: cmd_args.depth,
        factors: cmd_args.factors,
        batches: cmd_args.batches,
        overdisp: cmd_args.overdisp,
        pve_topic: cmd_args.pve_topic,
        pve_batch: cmd_args.pve_batch,
        rseed: cmd_args.rseed,
    };

    let sim = simulate::generate_factored_poisson_gamma_data(&sim_args)?;
    info!("successfully generated factored Poisson-Gamma data");

    let batch_out: Vec<Box<str>> = sim
        .batch_membership
        .iter()
        .map(|&x| Box::from(x.to_string()))
        .collect();

    write_lines(&batch_out, &batch_memb_file)?;
    info!("batch membership: {:?}", &batch_memb_file);

    let mtx_shape = (sim_args.rows, sim_args.cols, sim.triplets.len());

    let rows: Vec<Box<str>> = (0..cmd_args.rows)
        .map(|i| i.to_string().into_boxed_str())
        .collect();

    let cols: Vec<Box<str>> = (0..cmd_args.cols)
        .map(|i| i.to_string().into_boxed_str())
        .collect();

    sim.ln_delta_db
        .to_parquet(Some(&rows), None, &ln_batch_file)?;
    sim.theta_kn
        .transpose()
        .to_parquet(Some(&cols), None, &prop_file)?;
    sim.beta_dk.to_parquet(Some(&rows), None, &dict_file)?;

    info!(
        "wrote parameter files:\n{:?},\n{:?},\n{:?}",
        &ln_batch_file, &dict_file, &prop_file
    );

    if cmd_args.save_mtx {
        let mut triplets = sim.triplets.clone();
        triplets.par_sort_by_key(|&(row, _, _)| row);
        triplets.par_sort_by_key(|&(_, col, _)| col);

        mtx_io::write_mtx_triplets(&triplets, sim_args.rows, sim_args.cols, &mtx_file)?;
        write_lines(&rows, &row_file)?;
        write_lines(&cols, &col_file)?;

        info!(
            "save mtx, row, and column files:\n{}\n{}\n{}",
            mtx_file, row_file, col_file
        );
    }

    info!("registering triplets ...");

    let mut data = create_sparse_from_triplets(
        &sim.triplets,
        mtx_shape,
        Some(&backend_file),
        Some(&backend),
    )?;

    info!("created sparse matrix: {}", backend_file);

    data.register_row_names_vec(&rows);
    data.register_column_names_vec(&cols);

    info!("done");
    Ok(())
}

// {
//     let group = zarrs::group::Group::open_opt(
//         store.clone(),
//         "/",
//         &zarrs::config::MetadataRetrieveVersion::Default,
//     )?;
//     let group = group.to_v3();
//     group.store_metadata()?;
//     group.erase_metadata_opt(zarrs::config::MetadataEraseVersion::V2)?;
//     let node = zarrs::node::Node::open_opt(
//         &store,
//         "/",
//         &zarrs::config::MetadataRetrieveVersion::Default,
//     )
//     .unwrap();
//     let tree = node.hierarchy_tree();
//     println!("hierarchy_tree:\n{}", tree);
// }
