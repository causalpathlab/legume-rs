mod misc;
mod simulate;
mod sparse_data_visitors;
mod sparse_io;
mod sparse_io_vector;
mod sparse_matrix_hdf5;
mod sparse_matrix_zarr;
mod statistics;

use crate::misc::*;
use crate::sparse_data_visitors::*;
use crate::sparse_io::*;
use crate::sparse_io_vector::*;
use crate::statistics::RunningStatistics;

use clap::{ArgAction, Args, Parser, Subcommand, ValueEnum};
use indicatif::ParallelProgressIterator;
use indicatif::{ProgressBar, ProgressDrawTarget};
use log::info;
use matrix_util::common_io::basename;
use matrix_util::traits::IoOps;
use matrix_util::*;
use rayon::prelude::*;

use std::collections::HashMap;
use std::sync::{Arc, Mutex};

fn main() -> anyhow::Result<()> {
    env_logger::init();

    let cli = Cli::parse();

    match &cli.commands {
        Commands::FromMtx(args) => {
            run_build_from_mtx(args)?;
        }
        Commands::ListH5(args) => {
            list_h5(args)?;
        }
        Commands::FromH5ad(args) => {
            run_build_from_h5_triplets(args)?;
        }
        Commands::FromH5(args) => {
            run_build_from_h5_triplets(args)?;
        }
        Commands::Simulate(args) => {
            run_simulate(args)?;
        }
        Commands::Stat(args) => {
            run_stat(args)?;
        }
        Commands::Info(args) => {
            show_info(args)?;
        }
        Commands::Statistics(args) => {
            run_stat(args)?;
        }
        Commands::Squeeze(args) => {
            run_squeeze(args)?;
        }
        Commands::Columns(args) => {
            take_columns(args)?;
        }
        Commands::ColumnNames(args) => {
            take_column_names(args)?;
        }
        Commands::RowNames(args) => {
            take_row_names(args)?;
        }
        Commands::SubsetColumns(args) => {
            subset_columns(args)?;
        }
        Commands::SortRows(args) => {
            reorder_rows(args)?;
        }
        Commands::MergeMtx(args) => {
            run_merge_mtx(args)?;
        }
    }

    Ok(())
}

/// Basic utility functions for processing a sparse matrix.
///
/// We assume non-negative sparse matrices were generated by
/// single-cell transcriptomics technology (feature x cell). It will
/// create a data structure like the following for faster accesses:
///
/// ```text
/// (root)
///     ├── nrow
///     ├── ncell
///     ├── by_column
///     │   ├── data
///     │   ├── indices (row indices)
///     │   └── indptr (column pointers)
///     └── by_row
///         ├── data
///         ├── indices (column indices)
///         └── indptr (row pointers)
/// ```
///
/// - build: build from .mtx fileset to another faster format
/// - stat: compute basic statistics of a sparse matrix
/// - squeeze: filter out rows and columns that are (nearly) empty
/// - simulate: simulate a sparse matrix
///
#[derive(Parser, Debug)]
#[command(version, about, long_about, term_width = 80)]
struct Cli {
    #[command(subcommand)]
    commands: Commands,
}

#[derive(Subcommand, Debug)]
enum Commands {
    /// Build faster backend data from `mtx`
    FromMtx(FromMtxArgs),

    /// Build a backend from triplets in `h5`
    FromH5ad(FromH5Args),

    /// Build a backend from triplets in `h5`
    FromH5(FromH5Args),

    /// List what are included in `h5` file
    ListH5(ListH5Args),

    /// Sort rows according to the order of row names specified in a
    /// row name file
    SortRows(SortRowsArgs),

    /// Take columns from the sparse matrix and save it to an `output`
    /// file as a dense matrix for a quick examination
    Columns(TakeColumnsArgs),

    /// List column names
    ColumnNames(TakeColumnNamesArgs),

    /// List row names
    RowNames(TakeRowNamesArgs),

    /// Take columns from the sparse matrix and create a new sparse matrix backend.
    SubsetColumns(SubsetColumnsArgs),

    /// Merge multiple 10x `.mtx` files into one fileset
    MergeMtx(MergeMtxArgs),

    /// Squeeze out rows and columns with too few non-zeros. It will
    /// overwrite the original (be careful) and save the indices kept.
    Squeeze(RunSqueezeArgs),

    /// Show basic information of a sparse matrix. If output header is
    /// provided, row and column names will be saved.
    Info(InfoArgs),

    /// Take basic statistics from a sparse matrix
    Statistics(RunStatArgs),

    /// Take basic statistics from a sparse matrix (same as `statistics`)
    Stat(RunStatArgs),

    /// Simulate Poisson factorization data with batch effects:
    /// `Y(i,j) ~ δ(i,B(j)) sum_k β(i,k) * θ(j,k)` where the β and θ
    /// parameters are sampled from Gamma, and the batch effect
    /// `ln(δ)` has a batch assignment `B(j)` and random noise.
    Simulate(RunSimulateArgs),
}

#[derive(Args, Debug)]
pub struct SortRowsArgs {
    /// Data file -- either `.zarr` or `.h5`
    data_file: Box<str>,

    /// Row/feature name file (name per each line; `.tsv.gz` or `.tsv`)
    #[arg(short, long, required = true)]
    row_file: Box<str>,
}

#[derive(Args, Debug)]
pub struct TakeColumnsArgs {
    /// data file -- either `.zarr` or `.h5`
    data_file: Box<str>,

    /// column indices to take: e.g., `0,1,2,3`
    #[arg(short = 'i', long, value_delimiter = ',')]
    column_indices: Option<Vec<usize>>,

    /// column name file where each line is a column name
    #[arg(short = 'f', long)]
    name_file: Option<Box<str>>,

    /// output `parquet` file
    #[arg(short, long, default_value = "stdout")]
    output: Box<str>,
}

#[derive(Args, Debug)]
pub struct TakeColumnNamesArgs {
    /// data file -- either `.zarr` or `.h5`
    data_file: Box<str>,

    /// output file
    #[arg(short, long, default_value = "stdout")]
    output: Box<str>,
}

#[derive(Args, Debug)]
pub struct TakeRowNamesArgs {
    /// data file -- either `.zarr` or `.h5`
    data_file: Box<str>,

    /// output file
    #[arg(short, long, default_value = "stdout")]
    output: Box<str>,
}

#[derive(Args, Debug)]
pub struct SubsetColumnsArgs {
    /// data file -- either `.zarr` or `.h5`
    data_file: Box<str>,

    /// column indices to take: e.g., `0,1,2,3`
    #[arg(short = 'i', long, value_delimiter = ',')]
    column_indices: Option<Vec<usize>>,

    /// column name file where each line is a column name
    #[arg(short = 'f', long)]
    name_file: Option<Box<str>>,

    /// squeeze
    #[arg(long, default_value_t = false)]
    do_squeeze: bool,

    /// output file
    #[arg(short, long, required = true)]
    output: Box<str>,
}

#[derive(Args, Debug)]
pub struct FromMtxArgs {
    /// matrix market-formatted data file (`.mtx.gz` or `.mtx`)
    mtx: Box<str>,

    /// row/feature name file (name per each line; `.tsv.gz` or `.tsv`)
    #[arg(short, long)]
    row: Option<Box<str>>,

    /// column/cell/barcode file (name per each line; `.tsv.gz` or `.tsv`)
    #[arg(short, long)]
    col: Option<Box<str>>,

    /// backend for the output file
    #[arg(long, value_enum, default_value = "zarr")]
    backend: SparseIoBackend,

    /// output file header: {output}.{backend}
    #[arg(short, long)]
    output: Option<Box<str>>,

    /// squeeze
    #[arg(long, default_value_t = false)]
    do_squeeze: bool,

    /// verbose mode
    #[arg(short, long, action = ArgAction::Count)]
    verbose: u8,
}

#[derive(Args, Debug)]
pub struct ListH5Args {
    h5_file: Box<str>,
}

#[derive(Args, Debug)]
pub struct FromH5Args {
    /// `h5` file where triplets of sparse matrix data were stored
    /// (10X genomics or H5AD).
    h5_file: Box<str>,

    /// backend for the output file
    #[arg(long, value_enum, default_value = "zarr")]
    backend: SparseIoBackend,

    /// output file header: {output}.{backend}
    #[arg(short, long)]
    output: Option<Box<str>>,

    /// group name for sparse data triplets (check out them by `list-h5` subcommand)
    #[arg(short = 'x', long, default_value = "matrix")]
    data_group_name: Box<str>,

    /// triplet values, X(i,j) value itself
    #[arg(short = 'd', long, default_value = "data")]
    data_field: Box<str>,

    /// indices: row indices for CSC, column indices for CSR
    #[arg(short = 'i', long, default_value = "indices")]
    indices_field: Box<str>,

    /// indptr: column pointers for CSC, row pointers for CSR
    #[arg(short = 'p', long, default_value = "indptr")]
    indptr_field: Box<str>,

    /// group/dataset name for rows/genes/features
    #[arg(short = 'r', long, default_value = "matrix/features/id")]
    row_name_field: Box<str>,

    /// group/dataset name for columns/cells
    #[arg(short = 'c', long, default_value = "matrix/barcodes")]
    column_name_field: Box<str>,

    /// squeeze
    #[arg(long, default_value_t = false)]
    do_squeeze: bool,

    /// verbose mode
    #[arg(short, long, action = ArgAction::Count)]
    verbose: u8,
}

/// Merge multiple .mtx file sets into one sparse backend file.
#[derive(Args, Debug)]
pub struct MergeMtxArgs {
    /// Within each directory and sub-directories, it will search
    /// `mtx_file_name`, `feature_file_name`, and `barcode_file_name`
    /// to merge into one `backend` file.
    #[arg(value_delimiter = ',', required = true)]
    data_directories: Vec<Box<str>>,

    /// backend
    #[arg(long, value_enum, default_value = "zarr")]
    backend: SparseIoBackend,

    /// output file header: {output}.{backend} and {output}.batch.gz
    ///
    /// The backend will contain everything.  But the batch assignment
    /// information will be saved in a separate file and needed for
    /// embedding steps later.
    #[arg(short, long, required = true)]
    output: Box<str>,

    /// matrix file name (10x default is `matrix.mtx`).
    #[arg(short, long, default_value = "matrix.mtx")]
    mtx_file_name: Box<str>,

    /// feature/row file name
    #[arg(short, long, default_value = "genes.tsv.gz")]
    feature_file_name: Box<str>,

    /// number of words to use for feature names
    #[arg(long, default_value_t = 2)]
    num_feature_name_words: usize,

    /// barcode/column file name
    #[arg(short, long, default_value = "barcodes.tsv.gz")]
    barcode_file_name: Box<str>,

    /// number of words to use for barcode names
    #[arg(long, default_value_t = 5)]
    num_barcode_name_words: usize,

    /// squeeze
    #[arg(long, default_value_t = false)]
    do_squeeze: bool,

    /// verbose mode
    #[arg(short, long, action = ArgAction::Count)]
    verbose: u8,
}

/// Squeeze out rows and columns that have fewer non-zeros. This will
/// overwrite the original file (be careful) and save the row/column
/// indices that are kept in the output file.
#[derive(Args, Debug)]
#[command(about)]
pub struct RunSqueezeArgs {
    /// data file -- either `.zarr` or `.h5`
    data_file: Box<str>,

    /// number of non-zero cutoff for rows
    #[arg(short, long, default_value = "0")]
    row_nnz_cutoff: usize,

    /// number of non-zero cutoff for columns
    #[arg(short, long, default_value = "0")]
    column_nnz_cutoff: usize,

    /// block_size for parallel processing
    #[arg(long, default_value = "100")]
    block_size: usize,
}

#[derive(ValueEnum, Clone, Debug, PartialEq)]
#[clap(rename_all = "lowercase")]
enum StatDim {
    Row,
    Column,
}

/// Generate row-wise and column-wise basic statistics.
#[derive(Args, Debug)]
pub struct RunStatArgs {
    /// Data files of either `.zarr` or `.h5` format. All the formats
    /// in the given list should be identical. We can convert `.mtx`
    /// to `.zarr` or `.h5` using `asap-data build` command.
    #[arg(required = true)]
    data_files: Vec<Box<str>>,

    /// statistics over `row` or `column`
    #[arg(short, long, value_enum)]
    stat_dim: StatDim,

    /// block_size
    #[arg(long, value_enum, default_value = "100")]
    block_size: usize,

    /// output statistics file
    #[arg(short, long, default_value = "stdout")]
    output: Box<str>,
}

/// A quick information of the underlying matrix of a backend file.
#[derive(Args, Debug)]
pub struct InfoArgs {
    /// data file -- .zarr or .h5 file
    data_file: Box<str>,

    /// file header for {output}.{rows.gz,columns.gz}
    #[arg(short, long, default_value = "")]
    output: Box<str>,
}

#[derive(Args, Debug)]
pub struct RunSimulateArgs {
    /// number of rows/genes/features
    #[arg(short, long)]
    rows: usize,

    /// number of columns/cells
    #[arg(short, long)]
    cols: usize,

    /// depth per column--how many non-zero genes expected per cell?
    #[arg(short, long, default_value_t = 1000)]
    depth: usize,

    /// number of factors--number of cell types/topics/states, etc.
    #[arg(short, long, default_value_t = 1)]
    factors: usize,

    /// number of batches
    #[arg(short, long, default_value_t = 1)]
    batches: usize,

    /// proportion of variance explained by batch effects. This
    /// parameter will take effect when there are two or more batches.
    #[arg(long, default_value_t = 1.0)]
    pve_batch: f32,

    /// output file header: {output}.{backend}
    #[arg(short, long)]
    output: Box<str>,

    /// Set overdispersion `φ` parameter for `β ~ Gamma(1/φ, K*φ)` and
    /// `θ ~ Gamma(1/φ, K*φ)`. Here, the mean of both `β` and `θ` will
    /// be K. A high value, e.g., `φ>10`, would cause strong
    /// influences from `βθ` factorization. On the other hand, a small
    /// value, e.g., `φ<5`, would significantly weaken the
    /// factorization. So, `φ=10` seems right.
    #[arg(long, default_value_t = 10.0)]
    overdisp: f32,

    /// random seed
    #[arg(long, default_value_t = 42)]
    rseed: u64,

    /// save mtx
    #[arg(long, default_value_t = false)]
    save_mtx: bool,

    /// backend
    #[arg(long, value_enum, default_value = "zarr")]
    backend: SparseIoBackend,
}

/////////////////////
// implementations //
/////////////////////

fn read_row_names(row_file: Box<str>, max_row_name_idx: usize) -> anyhow::Result<Vec<Box<str>>> {
    let (_names, _) = common_io::read_lines_of_words(&row_file, -1)?;
    Ok(_names
        .into_iter()
        .map(|x| {
            let s = (0..x.len().min(max_row_name_idx))
                .filter_map(|i| x.get(i))
                .map(|x| x.to_string())
                .collect::<Vec<_>>()
                .join(ROW_SEP)
                .parse::<String>()
                .expect("invalid row name");
            s.into_boxed_str()
        })
        .collect())
}

fn read_col_names(col_file: Box<str>, max_column_name_idx: usize) -> anyhow::Result<Vec<Box<str>>> {
    let (_names, _) = common_io::read_lines_of_words(&col_file, -1)?;
    Ok(_names
        .into_iter()
        .map(|x| {
            let s = (0..x.len().min(max_column_name_idx))
                .filter_map(|i| x.get(i))
                .map(|x| x.to_string())
                .collect::<Vec<_>>()
                .join(COLUMN_SEP)
                .parse::<String>()
                .expect("invalid col name");
            s.into_boxed_str()
        })
        .collect())
}

fn reorder_rows(args: &SortRowsArgs) -> anyhow::Result<()> {
    let data_file = args.data_file.clone();
    let row_names_order: Vec<Box<str>> = read_row_names(args.row_file.clone(), MAX_ROW_NAME_IDX)?;

    let backend = match common_io::extension(&data_file)?.as_ref() {
        "zarr" => SparseIoBackend::Zarr,
        "h5" => SparseIoBackend::HDF5,
        _ => return Err(anyhow::anyhow!("Unknown file format: {}", data_file)),
    };

    let mut data = open_sparse_matrix(&data_file, &backend.clone())?;
    data.reorder_rows(&row_names_order)?;

    Ok(())
}

fn subset_columns(args: &SubsetColumnsArgs) -> anyhow::Result<()> {
    let data_file = args.data_file.clone();

    let columns_indices = args.column_indices.clone();
    let column_name_file = args.name_file.clone();

    let backend = match common_io::extension(&data_file)?.as_ref() {
        "zarr" => SparseIoBackend::Zarr,
        "h5" => SparseIoBackend::HDF5,
        _ => return Err(anyhow::anyhow!("Unknown file format: {}", data_file)),
    };

    let output = args.output.clone();

    let data = open_sparse_matrix(&data_file, &backend.clone())?;

    let row_names = data.row_names()?;
    let cols = data.column_names()?;

    let ((nrow, ncol, triplets), col_names) = if let Some(idx) = columns_indices {
        if idx.is_empty() {
            return Err(anyhow::anyhow!("Empty index"));
        }

        let col_names = idx.iter().map(|&x| cols[x].clone()).collect::<Vec<_>>();
        (data.read_triplets_by_columns(idx.clone())?, col_names)
    } else if let Some(column_file) = column_name_file {
        let col_names = read_col_names(column_file, MAX_COLUMN_NAME_IDX)?;

        if col_names.is_empty() {
            return Err(anyhow::anyhow!("Empty column file"));
        }

        let col_names_map = data
            .column_names()
            .expect("column names not found in data file")
            .iter()
            .enumerate()
            .map(|(i, x)| (x.clone(), i))
            .collect::<HashMap<_, _>>();

        let col_names_order = col_names
            .into_iter()
            .filter_map(|x| col_names_map.get(&x))
            .collect::<Vec<_>>();

        let columns: Vec<usize> = col_names_order.iter().map(|&x| *x).collect();
        let col_names = columns.iter().map(|&x| cols[x].clone()).collect::<Vec<_>>();

        if columns.is_empty() {
            return Err(anyhow::anyhow!("Found empty columns"));
        }

        (data.read_triplets_by_columns(columns)?, col_names)
    } else {
        return Err(anyhow::anyhow!(
            "either `column-indices` or `name-file` must be provided"
        ));
    };

    let backend_file = match backend {
        SparseIoBackend::HDF5 => format!("{}.h5", &output),
        SparseIoBackend::Zarr => format!("{}.zarr", &output),
    };

    let mtx_shape = (nrow, ncol, triplets.len());

    if std::path::Path::new(&backend_file).exists() {
        info!(
            "This existing backend file '{}' will be deleted",
            &backend_file
        );
        common_io::remove_file(&backend_file)?;
    }

    let mut data =
        create_sparse_from_triplets(triplets, mtx_shape, Some(&backend_file), Some(&backend))?;

    data.register_row_names_vec(&row_names);
    data.register_column_names_vec(&col_names);

    info!(
        "Successfully created a sparse backend file: {}",
        &backend_file
    );

    if args.do_squeeze {
        info!("Squeeze the backend data {}", &backend_file);
        let squeeze_args = RunSqueezeArgs {
            data_file: backend_file.into_boxed_str(),
            row_nnz_cutoff: 0,
            column_nnz_cutoff: 0,
            block_size: 100,
        };

        run_squeeze(&squeeze_args)?;
    }

    Ok(())
}

fn take_columns(args: &TakeColumnsArgs) -> anyhow::Result<()> {
    let data_file = args.data_file.clone();

    let columns = args.column_indices.clone();
    let column_name_file = args.name_file.clone();

    let backend = match common_io::extension(&data_file)?.as_ref() {
        "zarr" => SparseIoBackend::Zarr,
        "h5" => SparseIoBackend::HDF5,
        _ => return Err(anyhow::anyhow!("Unknown file format: {}", data_file)),
    };

    let output = args.output.clone();

    let data = open_sparse_matrix(&data_file, &backend.clone())?;
    let row_names = data.row_names()?;

    let (data, column_names) = if let Some(columns) = columns {
        let n_columns = data.num_columns().unwrap_or(0);
        let columns: Vec<usize> = columns.into_iter().filter(|&i| i < n_columns).collect();

        if columns.is_empty() {
            return Err(anyhow::anyhow!("invalid indexes"));
        }

        let _names = data.column_names()?;
        let column_names: Vec<Box<str>> = columns.iter().map(|&i| _names[i].clone()).collect();

        (data.read_columns_ndarray(columns)?, column_names)
    } else if let Some(column_file) = column_name_file {
        let col_names = read_col_names(column_file, MAX_COLUMN_NAME_IDX)?;
        let col_names_map = data
            .column_names()?
            .iter()
            .enumerate()
            .map(|(i, x)| (x.clone(), i))
            .collect::<HashMap<_, _>>();

        let col_names_order = col_names
            .into_iter()
            .filter_map(|x| col_names_map.get(&x))
            .collect::<Vec<_>>();

        let columns: Vec<usize> = col_names_order.iter().map(|&x| *x).collect();

        let _names = data.column_names()?;
        let column_names: Vec<Box<str>> = columns.iter().map(|&i| _names[i].clone()).collect();

        (data.read_columns_ndarray(columns)?, column_names)
    } else {
        return Err(anyhow::anyhow!(
            "either `column-indices` or `name-file` must be provided"
        ));
    };

    if let Ok(ext) = common_io::extension(&output) {
        if ext.as_ref() == "parquet" {
            data.to_parquet(Some(&row_names), Some(&column_names), &output)?;
            return Ok(());
        }
    }

    data.to_tsv(&output)?;

    Ok(())
}

fn run_merge_mtx(args: &MergeMtxArgs) -> anyhow::Result<()> {
    if args.verbose > 0 {
        std::env::set_var("RUST_LOG", "info");
    }

    let directories = args.data_directories.clone();

    let mut mtx_files = vec![];
    let mut row_files = vec![];
    let mut col_files = vec![];
    let mut batch_names = vec![];

    for dir in directories.iter() {
        let dir = dir.clone().into_string();

        if let Some(base) = std::path::Path::new(&dir).file_stem() {
            let base = base.to_str().expect("invalid base name").to_string();
            info!("Searching relevant files within: {}", &base);
            let batch_name = Some(base);

            if let Ok(this_dir) = std::fs::read_dir(&dir) {
                let mut mtx: Option<Box<str>> = None;
                let mut row: Option<Box<str>> = None;
                let mut col: Option<Box<str>> = None;

                for x in this_dir {
                    if let Some(_path) = x?.path().to_str() {
                        let _path = _path.to_string();

                        if _path.ends_with(args.mtx_file_name.as_ref()) {
                            mtx = Some(_path.into_boxed_str());
                        } else if _path.ends_with(args.feature_file_name.as_ref()) {
                            row = Some(_path.into_boxed_str());
                        } else if _path.ends_with(args.barcode_file_name.as_ref()) {
                            col = Some(_path.into_boxed_str());
                        }
                    }
                }

                if let (Some(m), Some(r), Some(c), Some(b)) = (mtx, row, col, batch_name) {
                    info!("Build {} from {}, {}, {} ", &b, &m, &r, &c);
                    mtx_files.push(m);
                    row_files.push(r);
                    col_files.push(c);
                    batch_names.push(b);
                }
            }
        }

        info!("Searching subdir within: {}", &dir);

        let mut sub_dir_vec = std::fs::read_dir(&dir)?
            .filter_map(Result::ok)
            .collect::<Vec<_>>();

        sub_dir_vec.sort_by_key(|entry| entry.file_name());

        for sub in sub_dir_vec {
            if let Some(sub_dir) = sub.path().to_str() {
                let mut mtx: Option<Box<str>> = None;
                let mut row: Option<Box<str>> = None;
                let mut col: Option<Box<str>> = None;

                if let Some(base) = std::path::Path::new(&sub_dir).file_stem() {
                    let base = base.to_str().expect("invalid base name").to_string();
                    info!("searching {} ...", &base);

                    let batch_name = Some(base);

                    if let Ok(sub_dir) = std::fs::read_dir(sub_dir) {
                        for x in sub_dir {
                            if let Some(_path) = x?.path().to_str() {
                                let _path = _path.to_string();

                                info!("Found: {}", &_path);

                                if _path.ends_with(args.mtx_file_name.as_ref()) {
                                    mtx = Some(_path.into_boxed_str());
                                } else if _path.ends_with(args.feature_file_name.as_ref()) {
                                    row = Some(_path.into_boxed_str());
                                } else if _path.ends_with(args.barcode_file_name.as_ref()) {
                                    col = Some(_path.into_boxed_str());
                                }
                            }
                        }
                    }

                    if let (Some(m), Some(r), Some(c), Some(b)) = (mtx, row, col, batch_name) {
                        info!("Build {} from {}, {}, {} ", &b, &m, &r, &c);
                        mtx_files.push(m);
                        row_files.push(r);
                        col_files.push(c);
                        batch_names.push(b);
                    }
                }
            }
        }
    }

    let num_batches = batch_names.len();

    info!("merging over {} batches ...", num_batches);

    debug_assert_eq!(num_batches, mtx_files.len());
    debug_assert_eq!(num_batches, row_files.len());
    debug_assert_eq!(num_batches, col_files.len());

    if num_batches == 0 {
        return Err(anyhow::anyhow!("No relevant files found"));
    }

    info!("Finding common rows/features ...");

    let mut row_hash: HashMap<Box<str>, usize> = HashMap::new();

    for row_file in row_files.iter() {
        let row_names = read_row_names(row_file.clone(), args.num_feature_name_words)?;
        for name in row_names.iter() {
            let n = row_hash.entry(name.clone()).or_insert(0);
            *n += 1;
        }
    }

    let mut common_rows: Vec<Box<str>> = row_hash
        .into_iter()
        .filter_map(|(k, v)| {
            if v == num_batches {
                Some(k.clone())
            } else {
                None
            }
        })
        .collect();

    common_rows.sort_by_key(|x| x.to_string());

    let row_pos: HashMap<Box<str>, usize> = common_rows
        .iter()
        .enumerate()
        .map(|(i, v)| (v.clone(), i))
        .collect();

    info!(
        "Found {} common row/feature names across {} file sets",
        row_pos.len(),
        num_batches
    );

    info!("Elongating column/barcode names ...");

    let mut column_names = vec![];
    let mut column_batch_names = vec![];

    for (col_file, batch_name) in col_files.iter().zip(batch_names.iter()) {
        let _names = read_col_names(col_file.clone(), args.num_barcode_name_words)?;
        let nn = _names.len();
        column_names.extend(
            _names
                .into_iter()
                .map(|x| format!("{}{}{}", x, COLUMN_SEP, batch_name).into_boxed_str())
                .collect::<Vec<_>>(),
        );

        column_batch_names.extend(vec![batch_name.clone().into_boxed_str(); nn]);
    }

    info!("Found {} columns/barcodes ...", column_names.len());

    info!("Renaming triplets...");

    let mut renamed_triplets = vec![];
    let mut offset = 0;
    let mut nnz_tot = 0;

    let pb = ProgressBar::new(num_batches as u64);

    if args.verbose > 0 {
        pb.set_draw_target(ProgressDrawTarget::hidden());
    }

    for b in 0..num_batches {
        let row_names = read_row_names(row_files[b].clone(), args.num_feature_name_words)?;

        let (triplets, shape) = mtx_io::read_mtx_triplets(&mtx_files[b].clone())?;

        if let Some((nrow, ncol, nnz)) = shape {
            info!(
                "{}: {} rows, {} columns, {} non-zeros",
                &mtx_files[b], nrow, ncol, nnz
            );

            let triplets = triplets
                .into_iter()
                .filter_map(|(batch_i, batch_j, x_ij)| {
                    if let Some(i) = row_pos.get(&row_names[batch_i as usize]) {
                        let i = *i as u64;
                        let j = batch_j + offset;
                        Some((i, j, x_ij))
                    } else {
                        None
                    }
                })
                .collect::<Vec<_>>();

            nnz_tot += triplets.len();
            renamed_triplets.extend(triplets);

            let batch_col_names =
                read_col_names(col_files[b].clone(), args.num_barcode_name_words)?;
            let batch_name = batch_names[b].clone();

            (0..ncol).for_each(|batch_j| {
                let loc = format!("{}{}{}", batch_col_names[batch_j], COLUMN_SEP, batch_name)
                    .into_boxed_str();

                let glob_j = offset as usize + batch_j;
                let glob = column_names[glob_j].clone();
                debug_assert_eq!(glob, loc);
            });

            offset += ncol as u64;
            pb.inc(1);
        }
    }
    pb.finish_and_clear();

    info!("Done with creating triplets from {} mtx files", num_batches);

    let backend = args.backend.clone();
    let output = args.output.clone();
    let batch_memb_file = (output.to_string() + ".batch.gz").into_boxed_str();

    let backend_file = match backend {
        SparseIoBackend::HDF5 => format!("{}.h5", &output),
        SparseIoBackend::Zarr => format!("{}.zarr", &output),
    };

    if std::path::Path::new(&backend_file).exists() {
        info!(
            "This existing backend file '{}' will be deleted",
            &backend_file
        );
        common_io::remove_file(&backend_file)?;
    }

    common_io::write_lines(&column_batch_names, &batch_memb_file)?;
    info!("Wrote batch membership file: {}", &batch_memb_file);

    let mut data = create_sparse_from_triplets(
        renamed_triplets,
        (row_pos.len(), offset as usize, nnz_tot),
        Some(&backend_file),
        Some(&backend),
    )?;

    data.register_row_names_vec(&common_rows);
    data.register_column_names_vec(&column_names);

    info!(
        "Successfully created a sparse backend file: {}",
        &backend_file
    );

    if args.do_squeeze {
        info!("Squeeze the backend data {}", &backend_file);
        let squeeze_args = RunSqueezeArgs {
            data_file: backend_file.into_boxed_str(),
            row_nnz_cutoff: 0,
            column_nnz_cutoff: 0,
            block_size: 100,
        };

        run_squeeze(&squeeze_args)?;
    }

    Ok(())
}

fn run_build_from_mtx(args: &FromMtxArgs) -> anyhow::Result<()> {
    if args.verbose > 0 {
        std::env::set_var("RUST_LOG", "info");
    }

    let mtx_file = args.mtx.as_ref();
    let row_file = args.row.as_ref();
    let col_file = args.col.as_ref();

    let backend = args.backend.clone();

    let output = match args.output.clone() {
        Some(output) => output,
        None => {
            let (dir, mut base, ext) = common_io::dir_base_ext(mtx_file)?;

            if base.ends_with(".mtx") && ext.ends_with("gz") {
                base = base
                    .into_string()
                    .trim_end_matches(".mtx")
                    .to_string()
                    .into_boxed_str();
            }

            match (dir.len(), base.len()) {
                (0, 0) => "./".to_string().into_boxed_str(),
                (0, _) => format!("./{}", base).into_boxed_str(),
                _ => format!("{}/{}", dir, base).into_boxed_str(),
            }
        }
    };

    let backend_file = match backend {
        SparseIoBackend::HDF5 => format!("{}.h5", &output),
        SparseIoBackend::Zarr => format!("{}.zarr", &output),
    };

    if std::path::Path::new(&backend_file).exists() {
        info!(
            "This existing backend file '{}' will be deleted",
            &backend_file
        );
        common_io::remove_file(&backend_file)?;
    }

    let mut data = create_sparse_from_mtx_file(mtx_file, Some(&backend_file), Some(&backend))?;

    if let Some(row_file) = row_file {
        data.register_row_names_file(row_file);
    } else if let Some(nrow) = data.num_rows() {
        let row_names: Vec<Box<str>> = (1..(nrow + 1)).map(|i| format!("{}", i).into()).collect();
        data.register_row_names_vec(&row_names);
    }

    if let Some(col_file) = col_file {
        data.register_column_names_file(col_file);
    } else if let Some(ncol) = data.num_columns() {
        let col_names: Vec<Box<str>> = (1..(ncol + 1)).map(|i| format!("{}", i).into()).collect();
        data.register_column_names_vec(&col_names);
    }

    if args.do_squeeze {
        let squeeze_args = RunSqueezeArgs {
            data_file: backend_file.into_boxed_str(),
            row_nnz_cutoff: 0,
            column_nnz_cutoff: 0,
            block_size: 100,
        };

        run_squeeze(&squeeze_args)?;
    }

    Ok(())
}

fn list_h5(cmd_args: &ListH5Args) -> anyhow::Result<()> {
    let data_file = cmd_args.h5_file.clone();
    let file = hdf5::File::open(data_file.to_string())?;
    info!("Opened {}", data_file.clone());

    fn list_group(group: &hdf5::Group, indent: usize) -> hdf5::Result<()> {
        for member in group.member_names()? {
            println!("{:indent$}{}", "", member, indent = indent);

            if let Ok(obj) = group.group(&member) {
                if let Ok(subgroup) = obj.as_group() {
                    list_group(&subgroup, indent + 2)?;
                }
            }
        }
        Ok(())
    }

    list_group(&file, 0)?;

    Ok(())
}

fn read_triplets_from_h5(
    root: &hdf5::Group,
    data_name: &str,
    indices_name: &str,
    indptr_name: &str,
) -> anyhow::Result<(Vec<(u64, u64, f32)>, (usize, usize, usize))> {
    if let (Ok(values), Ok(indices), Ok(indptr)) = (
        root.dataset(data_name),
        root.dataset(indices_name),
        root.dataset(indptr_name),
    ) {
        use rayon::prelude::*;
        let indptr = indptr.read_1d::<usize>()?.to_vec();
        let nvectors = indptr.len() - 1;

        info!("Collecting triplets over {} data points...", nvectors);

        let mut triplets = vec![];
        let arc_triplets = Arc::new(Mutex::new(&mut triplets));

        let full_values = values.read_1d::<f32>()?.to_vec();
        let full_indices = indices.read_1d::<u64>()?.to_vec();

        (0..nvectors)
            .into_par_iter()
            .progress_count(nvectors as u64)
            .for_each(|_idx| {
                let j = _idx as u64;
                let start = indptr[_idx];
                let end = indptr[_idx + 1];
                let values_slice = &full_values[start..end];
                let indices_slice = &full_indices[start..end];

                // let values_slice = values.read_slice_1d::<f32, _>(start..end).expect("values");
                // let indices_slice = indices
                //     .read_slice_1d::<u64, _>(start..end)
                //     .expect("indices");

                // Note: h5ad treats cells as rows, but we treat cells as columns
                let triplets_slice: Vec<(u64, u64, f32)> = indices_slice
                    .iter()
                    .zip(values_slice.iter())
                    .map(|(&i, &x_ij)| (i, j, x_ij))
                    .collect();

                arc_triplets
                    .lock()
                    .expect("failed to lock triplets")
                    .extend(triplets_slice);
            });
        let nfeatures = triplets.iter().map(|&(i, _, _)| i).max().unwrap_or(0_u64) as usize + 1;
        let nnz = triplets.len();
        Ok((triplets, (nfeatures, nvectors, nnz)))
    } else {
        Err(anyhow::anyhow!("cannot read triplets"))
    }
}

fn run_build_from_h5_triplets(cmd_args: &FromH5Args) -> anyhow::Result<()> {
    if cmd_args.verbose > 0 {
        std::env::set_var("RUST_LOG", "info");
    }

    let data_file = cmd_args.h5_file.clone();
    let backend = cmd_args.backend.clone();
    let output = match cmd_args.output.clone() {
        Some(output) => output,
        None => {
            let (dir, base, _ext) = common_io::dir_base_ext(&data_file)?;

            match (dir.len(), base.len()) {
                (0, 0) => "./".to_string().into_boxed_str(),
                (0, _) => format!("./{}", base).into_boxed_str(),
                _ => format!("{}/{}", dir, base).into_boxed_str(),
            }
        }
    };

    let backend_file = match backend {
        SparseIoBackend::HDF5 => format!("{}.h5", &output),
        SparseIoBackend::Zarr => format!("{}.zarr", &output),
    };

    if std::path::Path::new(&backend_file).exists() {
        info!(
            "This existing backend file '{}' will be deleted",
            &backend_file
        );
        common_io::remove_file(&backend_file)?;
    }

    let file = hdf5::File::open(data_file.to_string())?;
    info!("Opened {}", data_file.clone());

    let group_name = &cmd_args.data_group_name.to_string();
    let data_name = &cmd_args.data_field.to_string();
    let indices_name = &cmd_args.indices_field.to_string();
    let indptr_name = &cmd_args.indptr_field.to_string();
    let row_name = &cmd_args.row_name_field.to_string();
    let column_name = &cmd_args.column_name_field.to_string();

    if let Ok(data) = file.group(group_name) {
        let (triplets, mtx_shape) =
            read_triplets_from_h5(&data, data_name, indices_name, indptr_name)?;

        let (nrows, ncols, nnz) = mtx_shape;
        info!("Read {} non-zero elements in {} x {}", nnz, nrows, ncols);

        let row_names: Vec<Box<str>> = match file.dataset(row_name) {
            Ok(rows) => read_hdf5_strings(rows)?,
            _ => {
                info!("row (feature) names not found");
                (0..nrows).map(|x| x.to_string().into_boxed_str()).collect()
            }
        };
        info!("Read {} row names", row_names.len());
        assert_eq!(nrows, row_names.len());

        let column_names: Vec<Box<str>> = match file.dataset(column_name) {
            Ok(columns) => read_hdf5_strings(columns)?,
            _ => {
                info!("column (cell) names not found");
                (0..ncols).map(|x| x.to_string().into_boxed_str()).collect()
            }
        };
        info!("Read {} column names", column_names.len());
        assert_eq!(ncols, column_names.len());

        let mut out = create_sparse_from_triplets(
            triplets,
            (nrows, ncols, nnz),
            Some(&backend_file),
            Some(&backend),
        )?;
        info!("created sparse matrix: {}", backend_file);
        out.register_row_names_vec(&row_names);
        out.register_column_names_vec(&column_names);
        info!("done");
    } else {
        return Err(anyhow::anyhow!("data group `{}` is missing", group_name));
    }

    if cmd_args.do_squeeze {
        info!("Squeeze the backend data {}", &backend_file);
        let squeeze_args = RunSqueezeArgs {
            data_file: backend_file.into_boxed_str(),
            row_nnz_cutoff: 0,
            column_nnz_cutoff: 0,
            block_size: 100,
        };

        run_squeeze(&squeeze_args)?;
    }

    Ok(())
}

fn take_column_names(cmd_args: &TakeColumnNamesArgs) -> anyhow::Result<()> {
    use common_io::write_lines;

    let output = cmd_args.output.clone();
    let input = cmd_args.data_file.clone();

    let backend = match common_io::extension(&input)?.as_ref() {
        "zarr" => SparseIoBackend::Zarr,
        "h5" => SparseIoBackend::HDF5,
        _ => return Err(anyhow::anyhow!("Unknown file format: {}", input)),
    };

    let data = open_sparse_matrix(&input, &backend.clone())?;
    let col_names = data.column_names()?;
    write_lines(&col_names, &output)?;

    Ok(())
}

fn take_row_names(cmd_args: &TakeRowNamesArgs) -> anyhow::Result<()> {
    use common_io::write_lines;

    let output = cmd_args.output.clone();
    let input = cmd_args.data_file.clone();

    let backend = match common_io::extension(&input)?.as_ref() {
        "zarr" => SparseIoBackend::Zarr,
        "h5" => SparseIoBackend::HDF5,
        _ => return Err(anyhow::anyhow!("Unknown file format: {}", input)),
    };

    let data = open_sparse_matrix(&input, &backend.clone())?;

    let row_names = data.row_names()?;
    write_lines(&row_names, &output)?;

    Ok(())
}

fn show_info(cmd_args: &InfoArgs) -> anyhow::Result<()> {
    let output = cmd_args.output.clone();
    let input = cmd_args.data_file.clone();

    let backend = match common_io::extension(&input)?.as_ref() {
        "zarr" => SparseIoBackend::Zarr,
        "h5" => SparseIoBackend::HDF5,
        _ => return Err(anyhow::anyhow!("Unknown file format: {}", input)),
    };

    let data = open_sparse_matrix(&input, &backend.clone())?;

    if let (Some(nrow), Some(ncol), Some(nnz)) =
        (data.num_rows(), data.num_columns(), data.num_non_zeros())
    {
        println!("number_of_rows:\t{}", nrow);
        println!("number_of_columns:\t{}", ncol);
        println!("number_of_nonzeros:\t{}", nnz);
    }

    if output.len() > 0 {
        use common_io::{mkdir, write_lines};
        mkdir(&output)?;
        let row_names = data.row_names()?;
        let col_names = data.column_names()?;
        write_lines(&row_names, &(output.to_string() + ".rows.gz"))?;
        write_lines(&col_names, &(output.to_string() + ".columns.gz"))?;
    }

    Ok(())
}

fn run_stat(cmd_args: &RunStatArgs) -> anyhow::Result<()> {
    let output = cmd_args.output.clone();
    common_io::mkdir(&output)?;

    let file = cmd_args.data_files[0].as_ref();
    let backend = match common_io::extension(file)?.to_string().as_str() {
        "h5" => SparseIoBackend::HDF5,
        "zarr" => SparseIoBackend::Zarr,
        _ => return Err(anyhow::anyhow!("Unknown file format: {}", file)),
    };

    let mut data = SparseIoVec::new();
    for data_file in cmd_args.data_files.iter() {
        match common_io::extension(data_file)?.as_ref() {
            "zarr" => {
                assert_eq!(backend, SparseIoBackend::Zarr);
            }
            "h5" => {
                assert_eq!(backend, SparseIoBackend::HDF5);
            }
            _ => return Err(anyhow::anyhow!("Unknown file format: {}", data_file)),
        };

        let this_data = open_sparse_matrix(data_file, &backend)?;
        let data_name = basename(data_file)?;
        data.push(Arc::from(this_data), Some(data_name))?;
    }

    match cmd_args.stat_dim {
        StatDim::Row => {
            let mut row_stat = RunningStatistics::new(Ix1(data.num_rows()?));
            data.visit_columns_by_block(
                &row_stat_visitor,
                &EmptyArgs {},
                &mut row_stat,
                Some(cmd_args.block_size),
            )?;

            row_stat.save(&cmd_args.output, &data.row_names()?, "\t")?;
        }
        StatDim::Column => {
            let mut col_stat = RunningStatistics::new(Ix1(data.num_columns()?));
            data.visit_columns_by_block(
                &col_stat_visitor,
                &EmptyArgs {},
                &mut col_stat,
                Some(cmd_args.block_size),
            )?;
            col_stat.save(&cmd_args.output, &data.column_names()?, "\t")?;
        }
    };

    Ok(())
}

fn collect_row_stat(
    data: &SparseIoVec,
    block_size: usize,
) -> anyhow::Result<RunningStatistics<Ix1>> {
    let mut row_stat = RunningStatistics::new(Ix1(data.num_rows()?));
    data.visit_columns_by_block(
        &row_stat_visitor,
        &EmptyArgs {},
        &mut row_stat,
        Some(block_size),
    )?;
    Ok(row_stat)
}

fn collect_column_stat(
    data: &SparseIoVec,
    block_size: usize,
) -> anyhow::Result<RunningStatistics<Ix1>> {
    let mut col_stat = RunningStatistics::new(Ix1(data.num_columns()?));
    data.visit_columns_by_block(
        &col_stat_visitor,
        &EmptyArgs {},
        &mut col_stat,
        Some(block_size),
    )?;
    Ok(col_stat)
}

fn run_squeeze(cmd_args: &RunSqueezeArgs) -> anyhow::Result<()> {
    let data_file = cmd_args.data_file.clone();
    let row_nnz_cutoff = cmd_args.row_nnz_cutoff;
    let col_nnz_cutoff = cmd_args.column_nnz_cutoff;

    use common_io::extension as file_ext;

    let backend = match file_ext(&data_file)?.as_ref() {
        "zarr" => SparseIoBackend::Zarr,
        "h5" => SparseIoBackend::HDF5,
        _ => return Err(anyhow::anyhow!("Unknown file format: {}", data_file)),
    };

    match file_ext(&data_file)?.as_ref() {
        "zarr" => {
            assert_eq!(backend, SparseIoBackend::Zarr);
        }
        "h5" => {
            assert_eq!(backend, SparseIoBackend::HDF5);
        }
        _ => return Err(anyhow::anyhow!("Unknown file format: {}", data_file)),
    }

    let data = open_sparse_matrix(&data_file, &backend)?;

    info!(
        "before squeeze -- data: {} rows x {} columns",
        data.num_rows().unwrap(),
        data.num_columns().unwrap()
    );

    let mut data_vec = SparseIoVec::new();
    let data_name = basename(&data_file)?;
    data_vec.push(Arc::from(data), Some(data_name))?;

    let row_stat = collect_row_stat(&data_vec, cmd_args.block_size)?;
    let col_stat = collect_column_stat(&data_vec, cmd_args.block_size)?;

    let mut data = open_sparse_matrix(&data_file, &backend)?;
    data.preload_columns()?;

    fn nnz_index(nnz: &[f32], cutoff: usize) -> Vec<usize> {
        nnz.iter()
            .enumerate()
            .filter_map(|(i, &x)| if (x as usize) > cutoff { Some(i) } else { None })
            .collect()
    }

    let row_nnz_vec = row_stat.count_positives().to_vec();
    let row_idx = nnz_index(&row_nnz_vec, row_nnz_cutoff);
    let col_nnz_vec = col_stat.count_positives().to_vec();
    let col_idx = nnz_index(&col_nnz_vec.to_vec(), col_nnz_cutoff);

    data.subset_columns_rows(Some(&col_idx), Some(&row_idx))?;

    info!(
        "after squeeze -- data: {} rows x {} columns",
        data.num_rows().unwrap(),
        data.num_columns().unwrap()
    );

    let hdr = data_file.replace(file_ext(&data_file)?.as_ref(), "");
    let row_idx_file = format!("{}row.idx.gz", hdr);
    let col_idx_file = format!("{}col.idx.gz", hdr);

    fn to_str_vec(v: &[usize]) -> Vec<Box<str>> {
        v.iter()
            .map(|x| format!("{}", x).into_boxed_str())
            .collect()
    }

    let col_idx = to_str_vec(&col_idx);
    let row_idx = to_str_vec(&row_idx);
    common_io::write_lines(&col_idx, &col_idx_file)?;
    common_io::write_lines(&row_idx, &row_idx_file)?;

    Ok(())
}

fn run_simulate(cmd_args: &RunSimulateArgs) -> anyhow::Result<()> {
    let output = cmd_args.output.clone();
    common_io::mkdir(&output)?;

    let backend = cmd_args.backend.clone();

    let backend_file = match backend {
        SparseIoBackend::HDF5 => output.to_string() + ".h5",
        SparseIoBackend::Zarr => output.to_string() + ".zarr",
    };

    let mtx_file = output.to_string() + ".mtx.gz";
    let row_file = output.to_string() + ".rows.gz";
    let col_file = output.to_string() + ".cols.gz";

    let dict_file = mtx_file.replace(".mtx.gz", ".dict.parquet");
    let prop_file = mtx_file.replace(".mtx.gz", ".prop.parquet");
    let batch_memb_file = mtx_file.replace(".mtx.gz", ".batch.gz");
    let ln_batch_file = mtx_file.replace(".mtx.gz", ".ln_batch.parquet");

    common_io::remove_all_files(&vec![
        backend_file.clone().into_boxed_str(),
        mtx_file.clone().into_boxed_str(),
        dict_file.clone().into_boxed_str(),
        prop_file.clone().into_boxed_str(),
        batch_memb_file.clone().into_boxed_str(),
        ln_batch_file.clone().into_boxed_str(),
    ])
    .expect("failed to clean up existing output files");

    let sim_args = simulate::SimArgs {
        rows: cmd_args.rows,
        cols: cmd_args.cols,
        depth: cmd_args.depth,
        factors: cmd_args.factors,
        batches: cmd_args.batches,
        overdisp: cmd_args.overdisp,
        pve_batch: cmd_args.pve_batch,
        rseed: cmd_args.rseed,
    };

    let sim = simulate::generate_factored_poisson_gamma_data(&sim_args);
    info!("successfully generated factored Poisson-Gamma data");

    let batch_out: Vec<Box<str>> = sim
        .batch_membership
        .iter()
        .map(|&x| Box::from(x.to_string()))
        .collect();

    common_io::write_lines(&batch_out, &batch_memb_file)?;
    info!("batch membership: {:?}", &batch_memb_file);

    let mtx_shape = (sim_args.rows, sim_args.cols, sim.triplets.len());

    let rows: Vec<Box<str>> = (0..cmd_args.rows)
        .map(|i| i.to_string().into_boxed_str())
        .collect();

    let cols: Vec<Box<str>> = (0..cmd_args.cols)
        .map(|i| i.to_string().into_boxed_str())
        .collect();

    sim.ln_delta_db
        .to_parquet(Some(&rows), None, &ln_batch_file)?;
    sim.theta_kn
        .transpose()
        .to_parquet(Some(&cols), None, &prop_file)?;
    sim.beta_dk.to_parquet(Some(&rows), None, &dict_file)?;

    info!(
        "wrote parameter files:\n{:?},\n{:?},\n{:?}",
        &ln_batch_file, &dict_file, &prop_file
    );

    if cmd_args.save_mtx {
        let mut triplets = sim.triplets.clone();
        triplets.par_sort_by_key(|&(row, _, _)| row);
        triplets.par_sort_by_key(|&(_, col, _)| col);

        mtx_io::write_mtx_triplets(&triplets, sim_args.rows, sim_args.cols, &mtx_file)?;
        common_io::write_lines(&rows, &row_file)?;
        common_io::write_lines(&cols, &col_file)?;

        info!(
            "save mtx, row, and column files:\n{}\n{}\n{}",
            mtx_file, row_file, col_file
        );
    }

    info!("registering triplets ...");

    let mut data =
        create_sparse_from_triplets(sim.triplets, mtx_shape, Some(&backend_file), Some(&backend))?;

    info!("created sparse matrix: {}", backend_file);

    data.register_row_names_vec(&rows);
    data.register_column_names_vec(&cols);

    info!("done");
    Ok(())
}

struct EmptyArgs {}

fn row_stat_visitor(
    job: (usize, usize),
    data: &SparseIoVec,
    _: &EmptyArgs,
    arc_stat: Arc<Mutex<&mut RunningStatistics<Ix1>>>,
) -> anyhow::Result<()> {
    let (lb, ub) = job;
    let xx = data.read_columns_ndarray(lb..ub)?;

    let mut stat = arc_stat.lock().expect("failed to lock row_stat");
    for x in xx.axis_iter(Axis(1)) {
        stat.add(&x);
    }
    Ok(())
}

fn col_stat_visitor(
    job: (usize, usize),
    data: &SparseIoVec,
    _: &EmptyArgs,
    arc_stat: Arc<Mutex<&mut RunningStatistics<Ix1>>>,
) -> anyhow::Result<()> {
    let (lb, ub) = job;
    let xx = data.read_columns_ndarray(lb..ub)?;
    let mut stat = arc_stat.lock().expect("failed to lock col_stat");

    for x in xx.axis_iter(Axis(0)) {
        for j in lb..ub {
            let i = j - lb;
            stat.add_element(&[j], x[i]);
        }
    }

    Ok(())
}
