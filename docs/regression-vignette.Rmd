---
title: "Using candle-util Regression Utilities"
author: "Your Name"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    theme: cosmo
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)

# Create temporary directory for all outputs
tmpdir <- tempfile("candle_util_vignette_")
dir.create(tmpdir)
knitr::opts_knit$set(root.dir = tmpdir)
```

## Introduction

This vignette demonstrates how to use the regression utilities provided by `candle-util`. We assume you have already installed `candle-util` (or `legume` in general) and can call it from the command line.

The regression command uses Stochastic Gradient Variational Bayes (SGVB) for inference, supporting multiple likelihood models and variational priors.

## Getting Help

To see available commands:

```{bash}
candle-util --help
```

For regression-specific help:

```{bash}
candle-util regression --help
```

## Command Line Arguments

### Required Arguments

| Argument | Short | Description |
|----------|-------|-------------|
| `--x` | `-x` | Design matrix file (CSV, TSV, or Parquet) |
| `--y` | `-y` | Response variable file (CSV, TSV, or Parquet) |

### Optional Arguments

| Argument | Short | Default | Description |
|----------|-------|---------|-------------|
| `--model` | `-m` | `gaussian` | Likelihood model: `gaussian`, `poisson`, `negbin` |
| `--prior` | `-p` | `gaussian` | Variational prior: `gaussian`, `susie` |
| `--output` | `-o` | None | Output file base name (generates `.mean.parquet` and `.var.parquet`) |
| `--x_var` | | None | Variance predictor matrix (for heteroskedastic models) |
| `--susie-layers` | | `5` | Number of Susie layers (L parameter) |
| `--iters` | | `500` | Number of training iterations |
| `--lr` | | `0.01` | Learning rate for AdamW optimizer |
| `--samples` | | `50` | Number of Monte Carlo samples per iteration |
| `--verbose` | `-v` | `false` | Enable verbose logging output |

## Supported Models

### Likelihood Models

- **gaussian**: $y \sim N(X\beta, \exp(X_{var}\gamma))$
- **poisson**: $y \sim \text{Poisson}(\exp(X\beta))$
- **negbin**: $y \sim \text{NB}(\exp(X\beta), \exp(X_{var}\gamma))$ (Negative Binomial)

### Variational Priors

- **gaussian**: Standard Gaussian variational approximation
- **susie**: Sparse variational approximation (Sum of Single Effects)

## Input Data Formats

Supported input formats:

- CSV (`.csv`)
- TSV (`.tsv`)
- Parquet (`.parquet`, `.pq`)
- Gzipped variants (`.csv.gz`, `.tsv.gz`, etc.)

## Basic Usage Examples

### Gaussian Regression with Gaussian Prior

The simplest case - standard linear regression:

```{bash, eval=FALSE}
candle-util regression \
  -x data_X.parquet \
  -y data_Y.parquet \
  -o coefficients
```

### Gaussian Regression with Susie Prior

Sparse regression using Sum of Single Effects:

```{bash, eval=FALSE}
candle-util regression \
  -x data_X.parquet \
  -y data_Y.parquet \
  --model gaussian \
  --prior susie \
  --susie-layers 10 \
  -o sparse_coefficients \
  -v
```

### Poisson Regression

For count data with Poisson likelihood:

```{bash, eval=FALSE}
candle-util regression \
  -x predictors.parquet \
  -y counts.parquet \
  --model poisson \
  --prior gaussian \
  --iters 1000 \
  --lr 0.001 \
  -o poisson_results
```

### Poisson Regression with Susie Prior

Sparse Poisson regression:

```{bash, eval=FALSE}
candle-util regression \
  -x predictors.parquet \
  -y counts.parquet \
  --model poisson \
  --prior susie \
  --susie-layers 5 \
  -o sparse_poisson \
  -v
```

### Negative Binomial Regression

For overdispersed count data:

```{bash, eval=FALSE}
candle-util regression \
  -x data.parquet \
  -y response.parquet \
  --model negbin \
  -o negbin_results
```

## Example Workflow in R

Here's how you might integrate `candle-util` into an R workflow:

### Preparing Data

```{r}
library(arrow)

# Simulate some data
set.seed(42)
n <- 1000
p <- 500

# Create design matrix
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
colnames(X) <- paste0("feature_", 1:p)

# True coefficients (sparse)
beta_true <- rep(0, p)
beta_true[c(10, 50, 100)] <- c(2, -1.5, 1)

# Generate response
y <- X %*% beta_true + rnorm(n, sd = 0.1)

# Write to parquet
write_parquet(as.data.frame(X), "X.parquet")
write_parquet(data.frame(y = as.vector(y)), "y.parquet")
```

### Running Regression

```{bash}
candle-util regression \
  -x X.parquet \
  -y y.parquet \
  --model gaussian \
  --prior susie \
  --susie-layers 3 \
  --iters 1500 \
  -o results \
  -v
```

### Reading Results

```{r}
# Read coefficient estimates
results <- read_parquet("results.mean.parquet")

print(head(results))

# Compare with true coefficients
causal_idx <- which(beta_true != 0)
comparison <- data.frame(
  feature = colnames(X)[causal_idx],
  true_beta = beta_true[causal_idx],
  estimated_beta = results$mean[causal_idx],
  pip = results$pip[causal_idx]
)
print(comparison)
```

### Visualizing Results

```{r}
# Posterior inclusion probabilities
plot(results$pip, pch = 16, col = "steelblue",
     main = "Posterior Inclusion Probabilities",
     xlab = "Feature", ylab = "PIP")
abline(h = 0.5, lty = 2, col = "gray")
points(causal_idx, results$pip[causal_idx], pch = 16, col = "red", cex = 1.5)
legend("topright", legend = c("All features", "True causal"),
       pch = 16, col = c("steelblue", "red"))
```

```{r}
# Top features by PIP
top_k <- 10
top_idx <- order(results$pip, decreasing = TRUE)[1:top_k]
top_pip <- results$pip[top_idx]
top_names <- colnames(X)[top_idx]
is_causal <- top_idx %in% causal_idx

par(mar = c(5, 10, 4, 2))
barplot(rev(top_pip), names.arg = rev(top_names), horiz = TRUE, las = 1,
        col = ifelse(rev(is_causal), "red", "steelblue"),
        main = paste("Top", top_k, "Features by PIP"),
        xlab = "Posterior Inclusion Probability")
legend("bottomright", legend = c("Non-causal", "True causal"),
       fill = c("steelblue", "red"))
par(mar = c(5, 4, 4, 2))
```

```{r}
# Estimated vs true coefficients
plot(beta_true, results$mean, pch = 16, col = rgb(0, 0, 0, 0.5),
     main = "Estimated vs True Coefficients",
     xlab = "True Coefficient", ylab = "Estimated Coefficient")
abline(0, 1, lty = 2, col = "red")
```

### Count Data Example (Poisson)

```{r}
# Simulate Poisson count data
lambda <- exp(X %*% (beta_true * 0.5))  # scale down for reasonable counts
y_counts <- rpois(n, lambda)

write_parquet(data.frame(y = y_counts), "y_counts.parquet")
```

```{bash}
candle-util regression \
  -x X.parquet \
  -y y_counts.parquet \
  --model poisson \
  --prior susie \
  --susie-layers 3 \
  --iters 1500 \
  -o poisson_results \
  -v
```

```{r}
poisson_results <- read_parquet("poisson_results.mean.parquet")

print(head(poisson_results))

# Compare with true coefficients
print(data.frame(
  feature = colnames(X)[causal_idx],
  true_beta = beta_true[causal_idx] * 0.5,  # scaled coefficients used in simulation
  estimated_beta = poisson_results$mean[causal_idx],
  pip = poisson_results$pip[causal_idx]
))
```

```{r}
# Posterior inclusion probabilities for Poisson model
plot(poisson_results$pip, pch = 16, col = "steelblue",
     main = "Poisson Model: Posterior Inclusion Probabilities",
     xlab = "Feature", ylab = "PIP")
abline(h = 0.5, lty = 2, col = "gray")
points(causal_idx, poisson_results$pip[causal_idx], pch = 16, col = "red", cex = 1.5)
legend("topright", legend = c("All features", "True causal"),
       pch = 16, col = c("steelblue", "red"))
```

```{r}
# Top features by PIP for Poisson model
top_idx_pois <- order(poisson_results$pip, decreasing = TRUE)[1:top_k]
top_pip_pois <- poisson_results$pip[top_idx_pois]
top_names_pois <- colnames(X)[top_idx_pois]
is_causal_pois <- top_idx_pois %in% causal_idx

par(mar = c(5, 10, 4, 2))
barplot(rev(top_pip_pois), names.arg = rev(top_names_pois), horiz = TRUE, las = 1,
        col = ifelse(rev(is_causal_pois), "red", "steelblue"),
        main = paste("Poisson Model: Top", top_k, "Features by PIP"),
        xlab = "Posterior Inclusion Probability")
legend("bottomright", legend = c("Non-causal", "True causal"),
       fill = c("steelblue", "red"))
par(mar = c(5, 4, 4, 2))
```

## Output Format

The output generates two Parquet files based on the `-o` base name:

- **Mean file** (`<output>.mean.parquet`):
  - For Susie prior: contains `mean` (coefficient estimate) and `pip` (posterior inclusion probability)
  - For Gaussian prior: contains dense coefficient estimates

- **Variance file** (`<output>.var.parquet`):
  - Contains variance/dispersion parameter estimates

## Tuning Parameters

### Learning Rate (`--lr`)

- Default: 0.01
- Lower values (e.g., 0.001) for more stable but slower convergence
- Higher values for faster initial progress but potential instability

### Iterations (`--iters`)

- Default: 500
- Increase for complex models or if convergence is not reached
- Monitor the ELBO/loss with verbose mode to check convergence

### Monte Carlo Samples (`--samples`)

- Default: 50
- Higher values give more accurate gradient estimates but slower iterations
- Lower values for faster but noisier updates

### Susie Layers (`--susie-layers`)

- Default: 5
- Represents the expected number of causal effects
- Increase if you expect more non-zero coefficients

## Session Info

```{r}
sessionInfo()

# Temporary files in tmpdir will be cleaned up by the OS
```
